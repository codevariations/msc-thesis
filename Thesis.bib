Automatically generated by Mendeley Desktop 1.19
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ba2015,
abstract = {One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo- attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end us- ing the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.},
archivePrefix = {arXiv},
arxivId = {1506.00511},
author = {Ba, Jimmy Lei and Swersky, Kevin and Fidler, Sanja and Salakhutdinov, Ruslan},
doi = {10.1109/ICCV.2015.483},
eprint = {1506.00511},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BaZSL.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {4247--4255},
title = {{Predicting deep zero-shot convolutional neural networks using textual descriptions}},
volume = {2015 Inter},
year = {2015}
}
@inproceedings{Netzer2011,
author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
booktitle = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning},
title = {{Reading Digits in Natural Images with Unsupervised Feature Learning}},
year = {2011}
}
@article{Yeh2017,
abstract = {Multi-label classification is a practical yet challenging task in machine learning related fields, since it requires the prediction of more than one label category for each input instance. We propose a novel deep neural networks (DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this task. Aiming at better relating feature and label domain data for improved classification, we uniquely perform joint feature and label embedding by deriving a deep latent space, followed by the introduction of label-correlation sensitive loss function for recovering the predicted label outputs. Our C2AE is achieved by integrating the DNN architectures of canonical correlation analysis and autoencoder, which allows end-to-end learning and prediction with the ability to exploit label dependency. Moreover, our C2AE can be easily extended to address the learning problem with missing labels. Our experiments on multiple datasets with different scales confirm the effectiveness and robustness of our proposed method, which is shown to perform favorably against state-of-the-art methods for multi-label classification.},
archivePrefix = {arXiv},
arxivId = {1707.00418},
author = {Yeh, Chih-Kuan and Wu, Wei-Chieh and Ko, Wei-Jen and Wang, Yu-Chiang Frank},
eprint = {1707.00418},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YehLatentmulti.pdf:pdf},
keywords = {Machine Learning Methods},
pages = {2838--2844},
title = {{Learning Deep Latent Spaces for Multi-Label Classification}},
url = {http://arxiv.org/abs/1707.00418},
year = {2017}
}
@misc{Hinton1986a,
abstract = {Concepts can be represented by distributed patterns of activity in networks of neuron-like units. One advantage of this kind of representation is that it leads to automatic generalization. When the weighjts in the network are changed to incorporate new knowledgwe about one concept, the changes affect the knowledge associated with other concepts that are represented by similar activity patterns. There have been numerous demonstrations of sensible generalization which have depended on the experimenter choosing appropriately similar patterns for diferent concepts. This paper shows how the network can be made to choose the patterns itself when shown a set of propositions that use the concepts. It chooses patterns which make explicit the underlying features that are only implicit in the propositions it is shown.},
archivePrefix = {arXiv},
arxivId = {1601.01280},
author = {Hinton, Geoffrey},
booktitle = {Css},
doi = {10.1109/69.917563},
eprint = {1601.01280},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Hinton1986.pdf:pdf},
isbn = {0-262-68053-X},
issn = {10414347},
pages = {1--12},
pmid = {21943171},
title = {{Learning distributed representations of concepts}},
url = {http://www.cogsci.ucsd.edu/{~}ajyu/Teaching/Cogs202{\_}sp13/Readings/hinton86.pdf},
year = {1986}
}
@article{Szegedy2015a,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SzegedyInception.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{JurgenSchmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in
pattern recognition and machine learning. This historical survey compactly summarizes relevant work,
much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth
of their credit assignment paths, which are chains of possibly learnable, causal links between actions
and effects. I review deep supervised learning (also recapitulating the history of backpropagation),
unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short
programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828},
author = {{J{\"{u}}rgen Schmidhuber}},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SchmidhuberHistory.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
pages = {85--117},
title = {{Deep learning in neural networks: An overview}},
url = {https://ac.els-cdn.com/S0893608014002135/1-s2.0-S0893608014002135-main.pdf?{\_}tid=d25460aa-c556-11e7-a97c-00000aacb35f{\&}acdnat=1510236415{\_}e1693fdedfe07935906af07f4120d5af},
volume = {61},
year = {2015}
}
@article{Guillaumin2010,
author = {Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Schmid, Cordelia and Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Discrim-, Cordelia Schmid Tagprop and Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Schmid, Cordelia and Kuntzmann, Laboratoire Jean},
doi = {10.1109/ICCV.2009.5459266>},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/tagprop.pdf:pdf},
title = {{TagProp : Discriminative metric learning in nearest neighbor models for image auto-annotation To cite this version : TagProp : Discriminative Metric Learning in Nearest Neighbor Models for Image Auto-Annotation}},
year = {2010}
}
@article{Mikolov,
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MikoloW2V.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
pages = {1--9},
pmid = {903},
title = {{5021-Distributed-Representations-of-Words-and-Phrases-and-Their-Compositionality}}
}
@article{Wu2014,
abstract = {We introduce Deep Semantic Embedding (DSE), a super- vised learning algorithm which computes semantic repre- sentation for text documents by respecting their similarity to a given query. Unlike other methods that use single- layer learning machines, DSE maps word inputs into a low- dimensional semantic space with deep neural network, and achieves a highly nonlinear embedding to model the human perception of text semantics. Through discriminative fine- tuning of the deep neural network, DSE is able to encode the relative similarity between relevant/irrelevant document pairs in training data, and hence learn a reliable ranking score for a query-document pair. We present test results on datasets including scientific publications and user-generated knowledge base.},
author = {Wu, Hao and Min, Martin Renqiang and Bai, Bing},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BaiDeepsemantic.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Deep Learning,Nonlinear Embedding,Ranking,Semantic Indexing},
pages = {46--52},
title = {{Deep semantic embedding}},
volume = {1204},
year = {2014}
}
@article{Harris1954,
abstract = {Harris maintains that it is possible to define a linguistic structure solely in terms of the "distributions" (= patterns of co-occurrences) of its elements. There is no parallel meaning-structure which can aid in describing formal structure. Meaning is partly a function of distribution.},
author = {Harris, Zellig S.},
doi = {10.1080/00437956.1954.11659520},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HarrisLanguage.pdf:pdf},
isbn = {978-90-277-1267-7},
issn = {0043-7956},
journal = {{\textless}i{\textgreater}WORD{\textless}/i{\textgreater}},
number = {2-3},
pages = {146--162},
title = {{Distributional Structure}},
url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
volume = {10},
year = {1954}
}
@article{Zhang2015a,
abstract = {Zero-shot recognition (ZSR) deals with the problem of predicting class labels for target domain instances based on source domain side information (e.g. attributes) of unseen classes. We formulate ZSR as a binary prediction problem. Our resulting classifier is class-independent. It takes an arbitrary pair of source and target domain instances as input and predicts whether or not they come from the same class, i.e. whether there is a match. We model the posterior probability of a match since it is a sufficient statistic and propose a latent probabilistic model in this context. We develop a joint discriminative learning framework based on dictionary learning to jointly learn the parameters of our model for both domains, which ultimately leads to our class-independent classifier. Many of the existing embedding methods can be viewed as special cases of our probabilistic model. On ZSR our method shows 4.90$\backslash${\%} improvement over the state-of-the-art in accuracy averaged across four benchmark datasets. We also adapt ZSR method for zero-shot retrieval and show 22.45$\backslash${\%} improvement accordingly in mean average precision (mAP).},
archivePrefix = {arXiv},
arxivId = {1511.04512},
author = {Zhang, Ziming and Saligrama, Venkatesh},
doi = {10.1109/CVPR.2016.649},
eprint = {1511.04512},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Zhang16.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {978-1-4673-8391-2},
title = {{Zero-Shot Learning via Joint Latent Similarity Embedding}},
url = {http://arxiv.org/abs/1511.04512},
year = {2015}
}
@article{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
eprint = {1502.03167},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/glorotDeepSparse.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {AISTATS '11: Proceedings of the 14th International Conference on Artificial Intelligence and Statistics},
pages = {315--323},
pmid = {28788938},
title = {{Deep sparse rectifier neural networks}},
volume = {15},
year = {2011}
}
@book{Trindade2013,
abstract = {Sentiment analysis is an area of research that has gained considerable attention in recent years due to the increasing availability of opinionated information online. The majority of the work in sentiment analysis considers the polarity of word terms rather than the polarity of specific senses of the word but different senses of a word can have different opinion-related properties. In order to address this issue we consider novel semantic features of words in the context of a sentence. We take a sentence as a sequence of words augmented with features based on word sense disambiguation and sentiment lexicons with sense specific opinion-related properties. We then use a factored version of the sequence kernel in a support vector machine, and apply it to sentiment classification of sentences. We evaluate this sentiment analysis methodology on three publicly available corpuses. We also evaluate the effectiveness of several publicly available sense specific polarity lexicons and combinations. Experiments show that our factored approach offers improvements over the surface words baseline and other state-of-the-art kernels. {\textcopyright} 2013 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0445v2},
author = {Trindade, Luis and Wang, Hui and Blackburn, William and Rooney, Niall},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39593-2},
eprint = {arXiv:1305.0445v2},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BengioRepresentations.pdf:pdf},
isbn = {978-3-642-39592-5},
issn = {03029743},
keywords = {Information Retrieval,Kernel Methods,Opinion Mining,Polarity Classification,Sentiment Analysis,Social Media,Word Sense Disambiguation},
number = {July},
pages = {284--296},
title = {{Statistical Language and Speech Processing}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84883190443{\&}partnerID=tZOtx3y1},
volume = {7978},
year = {2013}
}
@misc{Princeton2010,
author = {Princeton},
title = {{WordNet - Princeton University}},
url = {wordnet.princeton.edu},
year = {2010}
}
@article{Chamberlain2017,
abstract = {Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.},
archivePrefix = {arXiv},
arxivId = {1705.10359},
author = {Chamberlain, Benjamin Paul and Clough, James and Deisenroth, Marc Peter},
eprint = {1705.10359},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Neural Embeddings.pdf:pdf},
keywords = {complex networks,geometry,graph embeddings,neural networks},
title = {{Neural Embeddings of Graphs in Hyperbolic Space}},
url = {http://arxiv.org/abs/1705.10359},
year = {2017}
}
@article{Read2014,
abstract = {In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature},
archivePrefix = {arXiv},
arxivId = {1502.05988},
author = {Read, Jesse and Perez-Cruz, Fernando},
eprint = {1502.05988},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ReadDLmultilabel.pdf:pdf},
pages = {1--8},
title = {{Deep Learning for Multi-label Classification}},
url = {http://arxiv.org/abs/1502.05988},
year = {2014}
}
@book{Prince2012,
author = {Prince, Simon J. D.},
isbn = {9781107011793},
title = {{Computer Vision: Models, Learning, and Inference}},
year = {2012}
}
@inproceedings{Zeiler2014,
author = {Zeiler, Matthew and Fergus, Rob},
booktitle = {European Conference on Computer Vision},
pages = {818--833},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2014}
}
@article{JiaDeng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\&}{\#}x201C;ImageNet{\&}{\#}x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
doi = {10.1109/CVPRW.2009.5206848},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DengIMGNET.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {248--255},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
year = {2009}
}
@article{Couprie2013,
abstract = {Scene labeling consists in labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape and contextual information.We report results using multiple post-processing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, e.g. they can be taken from a segmentation tree, or from any family of over-segmentations. The system yields record accuracies on the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) and near-record accuracy on Stanford Background Dataset (8 classes), while being an order of magnitude faster than competing approaches, producing a 320Ã—240 image labeling in less than a second, including feature extraction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Couprie, Camille and Najman, Laurent and Lecun, Yann},
doi = {10.1109/TPAMI.2012.231},
eprint = {arXiv:1011.1669v3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FarabetScenelabel.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {Convolutional networks,deep learning,image classification,image segmentation,scene parsing},
number = {8},
pages = {1915--1929},
pmid = {23787344},
title = {{Learning Hierarchical Features for scence labeling}},
volume = {35},
year = {2013}
}
@article{Akata2015,
abstract = {Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with fine-grained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results.},
archivePrefix = {arXiv},
arxivId = {1409.8403},
author = {Akata, Zeynep and Reed, Scott and Walter, Daniel and Lee, Honglak and Schiele, Bernt},
doi = {10.1109/CVPR.2015.7298911},
eprint = {1409.8403},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Akata2016.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {1},
pages = {2927--2936},
title = {{Evaluation of output embeddings for fine-grained image classification}},
volume = {07-12-June},
year = {2015}
}
@article{Yang2018,
abstract = {Multi-label classification (MLC) is an important learning problem that expects the learning algorithm to take the hidden correlation of the labels into account. Extracting the hidden correlation is generally a challenging task. In this work, we propose a novel deep learning framework to better extract the hidden correlation with the help of the memory structure within recurrent neural networks. The memory stores the temporary guesses on the labels and effectively allows the framework to rethink about the goodness and correlation of the guesses before making the final prediction. Furthermore, the rethinking process makes it easy to adapt to different evaluation criterion to match real-world application needs. Experimental results across many real-world data sets justify that the rethinking process indeed improves MLC performance across different evaluation criteria and leads to superior performance over state-of-the-art MLC algorithms.},
archivePrefix = {arXiv},
arxivId = {1802.01697},
author = {Yang, Yao-Yuan and Lin, Yi-An and Chu, Hong-Min and Lin, Hsuan-Tien},
eprint = {1802.01697},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YangRethinking.pdf:pdf},
title = {{Deep Learning with a Rethinking Structure for Multi-label Classification}},
url = {http://arxiv.org/abs/1802.01697},
year = {2018}
}
@article{Karpathy2015,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Li, Fei Fei},
doi = {10.1109/CVPR.2015.7298932},
eprint = {1412.2306},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/KarpathyDeepvisual.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {4},
pages = {3128--3137},
pmid = {16873662},
title = {{Deep visual-semantic alignments for generating image descriptions}},
volume = {39},
year = {2015}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1112.6209},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
eprint = {arXiv:1112.6209},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Rosenblatt1958.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471(Electronic);0033-295X(Print)},
journal = {Psychological Review},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in {\ldots}}},
url = {http://psycnet.apa.org/journals/rev/65/6/386.pdf{\%}5Cnpapers://c53d1644-cd41-40df-912d-ee195b4a4c2b/Paper/p15420},
volume = {65},
year = {1958}
}
@article{Huang2012,
abstract = {Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one represen- tation per word. This is problematic because words are often polysemous and global con- text can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the se- mantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning mul- tiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper- forms competitive baselines and other neural language models.},
archivePrefix = {arXiv},
arxivId = {1602.07019},
author = {Huang, Eric H and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
doi = {10.1002/9781118510001.ch8},
eprint = {1602.07019},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HuangWordemb.pdf:pdf},
isbn = {9781937284244},
issn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {873--882},
pmid = {1710995},
title = {{ImprovingWord Representations via Global Context and MultipleWord Prototypes}},
year = {2012}
}
@article{Li2015,
abstract = {Given the difficulty of acquiring labeled examples for many fine-grained visual classes, there is an increasing interest in zero-shot image tagging, aiming to tag images with novel labels that have no training examples present. Using a se-mantic space trained by a neural language model, the cur-rent state-of-the-art embeds both images and labels into the space, wherein cross-media similarity is computed. How-ever, for labels of relatively low occurrence, its similarity to images and other labels can be unreliable. This paper pro-poses Hierarchical Semantic Embedding (HierSE), a simple model that exploits the WordNet hierarchy to improve label embedding and consequently image embedding. Moreover, we identify two good tricks, namely training the neural lan-guage model using Flickr tags instead of web documents, and using partial match instead of full match for vectorizing a WordNet node. All this lets us outperform the state-of-the-art. On a test set of over 1,500 visual object classes and 1.3 million images, the proposed model beats the current best results (18.3{\%} versus 9.4{\%} in hit@1).},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Li, Xirong and Liao, Shuai and Lan, Weiyu and Du, Xiaoyong and Yang, Gang},
doi = {10.1145/2766462.2767773},
eprint = {arXiv:1011.1669v3},
file = {:home/hege/Documents/msc-thesis/Papers/liHIERSE.pdf:pdf},
isbn = {9781450336215},
issn = {14678330},
journal = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '15},
keywords = {image tagging,semantic embedding,zero shot learning},
pages = {879--882},
pmid = {15664853},
title = {{Zero-shot Image Tagging by Hierarchical Semantic Embedding}},
url = {http://dl.acm.org/citation.cfm?doid=2766462.2767773},
year = {2015}
}
@inproceedings{Paszke2017,
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
booktitle = {NIPS},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@article{Hubeld1962,
author = {Hubel, D. H. and Wiesel, T.},
journal = {Journal of Physiology},
number = {160},
pages = {106 -- 154},
title = {{Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex}},
year = {1962}
}
@article{Wang2016,
abstract = {While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model},
archivePrefix = {arXiv},
arxivId = {1604.04573},
author = {Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
doi = {10.1109/CVPR.2016.251},
eprint = {1604.04573},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/cnnrnn.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2285--2294},
pmid = {10463930},
title = {{CNN-RNN: A Unified Framework for Multi-label Image Classification}},
url = {http://ieeexplore.ieee.org/document/7780620/},
year = {2016}
}
@article{Hillel2007,
author = {Hillel, Aharon Bar and Weinshall, Daphna},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HillelCategorization.pdf:pdf},
journal = {Nips},
pages = {73--80},
title = {{Subordinate class recognition using relational object models}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=Tbn1l9P1220C{\&}oi=fnd{\&}pg=PA73{\&}dq=Subordinate+class+recognition+using+relational+object+models{\&}ots=V2m9Ilrr20{\&}sig=PN0UIGfPs7FGucveI3J0xO7{\_}qI8},
volume = {19},
year = {2007}
}
@article{Kodirov2017,
abstract = {Existing zero-shot learning (ZSL) models typically learn a projection function from a feature space to a semantic embedding space (e.g.{\~{}}attribute space). However, such a projection function is only concerned with predicting the training seen class semantic representation (e.g.{\~{}}attribute prediction) or classification. When applied to test data, which in the context of ZSL contains different (unseen) classes without training data, a ZSL model typically suffers from the project domain shift problem. In this work, we present a novel solution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing ZSL models. However, the decoder exerts an additional constraint, that is, the projection/code must be able to reconstruct the original visual feature. We show that with this additional reconstruction constraint, the learned projection function from the seen classes is able to generalise better to the new unseen classes. Importantly, the encoder and decoder are linear and symmetric which enable us to develop an extremely efficient learning algorithm. Extensive experiments on six benchmark datasets demonstrate that the proposed SAE outperforms significantly the existing ZSL models with the additional benefit of lower computational cost. Furthermore, when the SAE is applied to supervised clustering problem, it also beats the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1704.08345},
author = {Kodirov, Elyor and Xiang, Tao and Gong, Shaogang},
doi = {10.1109/CVPR.2017.473},
eprint = {1704.08345},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/KodirovSAE.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {4447--4456},
title = {{Semantic autoencoder for zero-shot learning}},
volume = {2017-Janua},
year = {2017}
}
@article{Zhou2005,
abstract = {Page 1. - with Training Examples Zhi-Hua Zhou 1 De-Chuan Zhan 1 Qiang Yang 2 1 National Key},
author = {Zhou, Zhi-Hua and Zhan, De-Chuan and Yang, Qiang},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZhouSemisuperv.pdf:pdf},
isbn = {978-1-57735-323-2},
journal = {Artificial Intelligence},
keywords = {Machine Learning,Technical Papers},
number = {1},
pages = {675--680},
title = {{Semi-Supervised Learning with Very Few Labeled Training Examples}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-107.pdf},
volume = {22},
year = {2005}
}
@article{Biederman1989,
abstract = {Reports that an article by I. Biederman published in Psychological Review, 1987(Apr), Vol 94(2), 115-117) was inadvertently a duplicate publication. A large portion of this article had previously appeared as unedited conference proceedings in another journal and in an edited book. (The following abstract of this article originally appeared in PA, Vol 74:20898.) The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons, can be derived from contrasts of five readily detectable properties of edges in a two-dimensional image. The detection of these properties is generally invariant over viewing position and image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. (PsycLIT Database Copyright 1993 American Psychological Assn, all rights reserved)},
author = {Biederman, Irving},
doi = {10.1037/0033-295X.96.1.2},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/biederman1987.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471},
journal = {Psychological Review},
number = {1},
pages = {2--2},
title = {{"Recognition-by-components: A theory of human image understanding": Clarification.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.96.1.2},
volume = {96},
year = {1989}
}
@article{Barnard2001,
abstract = {We present a statistical model for organizing image collections which integrates semantic information provided by associated text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text andlor image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition. 1.},
author = {Barnard, K. and Forsyth, D.},
doi = {10.1109/ICCV.2001.937654},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BarnardSemantics.pdf:pdf},
isbn = {0-7695-1143-0},
issn = {0-7695-1143-0},
journal = {Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},
pages = {408--415},
title = {{Learning the semantics of words and pictures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=937654},
volume = {2},
year = {2001}
}
@article{You2016,
abstract = {Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.},
archivePrefix = {arXiv},
arxivId = {1603.03925},
author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
doi = {10.1109/CVPR.2016.503},
eprint = {1603.03925},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YouCaptionatt.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
title = {{Image Captioning with Semantic Attention}},
url = {http://arxiv.org/abs/1603.03925},
year = {2016}
}
@article{Baltruschat2018,
abstract = {The increased availability of X-ray image archives (e.g. the ChestX-ray14 dataset from the NIH Clinical Center) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolutions of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we evaluate the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. We observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-50, integrating non-image data yields the best overall results.},
archivePrefix = {arXiv},
arxivId = {1803.02315},
author = {Baltruschat, Ivo M. and Nickisch, Hannes and Grass, Michael and Knopp, Tobias and Saalbach, Axel},
eprint = {1803.02315},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/XRAY.pdf:pdf},
keywords = {chest x-ray,chestx-ray14,convolutional neural net-,deep learning,transfer learning,works},
number = {c},
title = {{Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification}},
url = {http://arxiv.org/abs/1803.02315},
volume = {14},
year = {2018}
}
@article{Marszaek2007,
abstract = {In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection. We evaluate how our approach influences the classification accuracy and speed on the PASCAL VOC challenge 2006 dataset, a set of challenging real-world images. We also demonstrate additional features that become available to object recognition due to the extension with semantic inference tools - we can classify high-level categories, such as animals, and we can train part detectors, for example a window detector, by pure inference in the semantic network.},
author = {Marsza{\l}ek, Marcin and Schmid, Cordelia},
doi = {10.1109/CVPR.2007.383272},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MarszalekHierarchies.pdf:pdf},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
title = {{Semantic hierarchies for visual object Recognition}},
year = {2007}
}
@article{Sharma2015,
abstract = {This paper proposes a learning-based approach to scene parsing inspired by the deep Recursive Context Propagation Network (RCPN). RCPN is a deep feed-forward neural network that utilizes the contextual information from the entire image, through bottom-up followed by top-down context propagation via random binary parse trees. This improves the feature representation of every super-pixel in the image for better classification into semantic categories. We analyze RCPN and propose two novel contributions to further improve the model. We first analyze the learning of RCPN parameters and discover the presence of bypass error paths in the computation graph of RCPN that can hinder contextual propagation. We propose to tackle this problem by including the classification loss of the internal nodes of the random parse trees in the original RCPN loss function. Secondly, we use an MRF on the parse tree nodes to model the hierarchical dependency present in the output. Both modifications provide performance boosts over the original RCPN and the new system achieves state-of-the-art performance on Stanford Background, SIFT-Flow and Daimler urban datasets.},
archivePrefix = {arXiv},
arxivId = {1503.02725},
author = {Sharma, Abhishek and Tuzel, Oncel and Jacobs, David W.},
doi = {10.1109/CVPR.2015.7298651},
eprint = {1503.02725},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SharmaDeephierarchies.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {530--538},
title = {{Deep hierarchical parsing for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PenningtonGlove.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1532--1543},
pmid = {1710995},
title = {{Glove: Global Vectors for Word Representation}},
url = {http://aclweb.org/anthology/D14-1162},
year = {2014}
}
@article{Pathak2014,
abstract = {Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.},
archivePrefix = {arXiv},
arxivId = {1412.7144},
author = {Pathak, Deepak and Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1103/PhysRevA.92.013626},
eprint = {1412.7144},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PathakMIL.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {1050-2947},
number = {1},
pages = {1--4},
pmid = {823878},
title = {{Fully Convolutional Multi-Class Multiple Instance Learning}},
url = {http://arxiv.org/abs/1412.7144},
year = {2014}
}
@article{Kobatake1994,
author = {Kobatake, E. and Tanaka, K.},
journal = {Journal of Neurophysiology},
pages = {856--867},
title = {{Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex}},
volume = {71},
year = {1994}
}
@article{Aslandogan1997,
abstract = {Image retrievaf based on semantic contents involves extrac-tion, modelling and indexing of content information. While extraction of abstract contents is a hard problem, it is only part of the bigger picture. In this paper we use knowledge about the semantic contents of images to improve retrieval effectiveness. In particular we use Word Net, an electronic Iexicaf system for query and database expansion. Our con-tent model facilitates novel uses of WordNet. We also pro-pose a new normalization formula, an object significance scheme and evaluate their effectiveness with real user ex-periments. We describe the experiment setup and provide quantitative evaluation of each technique.},
author = {Aslandogan, Y Alp and Thier, Chuck and Yu, Clement T. and Zou, Jon and Rishe, Naphtali},
doi = {10.1145/278459.258591},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/AslandoganSemantics.pdf:pdf},
isbn = {0-89791-836-3},
issn = {01635840},
journal = {ACM SIGIR Forum},
keywords = {cplj94,gzcs94,hck90,lw93,rs9s,sq96,textuaf description or captions,user defined attributes},
number = {SI},
pages = {286--295},
title = {{Using semantic contents and WordNet in image retrieval}},
url = {http://delivery.acm.org/10.1145/260000/258591/p286-aslandogan.pdf?ip=130.85.58.228{\&}id=258591{\&}acc=ACTIVE SERVICE{\&}key=5F8E7AA76238C9EB.E2B546BDBAFC5578.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=921946910{\&}CFTOKEN=99821475{\&}{\_}{\_}acm{\_}{\_}=1491753365{\_}6aa72903a3c6962ac840},
volume = {31},
year = {1997}
}
@article{Rumelhart1985,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
journal = {Institute for Cognitive Science Report},
number = {8506},
title = {{Learning Internal Representations by Error Propagation}},
year = {1985}
}
@article{Ji2017,
abstract = {Zero-Shot Learning (ZSL) aims at classifying previously unseen class samples and has gained its popularity in applications where samples of some categories are scarce for training. The basic idea to address this issue is transferring knowledge from the seen classes to the unseen classes through mapping the visual feature to an embedding space spanned by class semantic information. The class semantic information can be obtained from human-labeled attributes or text corpus in an unsupervised fashion. Therefore, the embedding function from visual space to the embedding space is extremely important. However, the existing embedding approaches to ZSL mainly focus on aligning pairwise semantic consistency from heterogeneous spaces but ignore the intrinsic structure of the locally homogeneous isomorph. In order to preserve the locally visual structure in the embedding process, this paper proposes a Manifold regularized Cross-Modal Embedding (MCME) approach for ZSL by formulating the manifold constraint for intrinsic structure of the visual features as well as aligning pairwise consistency. The linear, closed-form solution makes MCME efficient to compute. Furthermore, rather than applying the embedding function learned from the seen classes directly, we also propose a new domain adaptation strategy to overcome the domain-shift problem during the knowledge transfer process. The MCME with the domain adaptation method is called MCME-DA. Extensive experiments on the benchmark datasets of AwA and CUB validate the superiority and promise of MCME and MCME-DA.},
author = {Ji, Zhong and Yu, Yunlong and Pang, Yanwei and Guo, Jichang and Zhang, Zhongfei},
doi = {10.1016/j.ins.2016.10.025},
file = {:home/hege/Downloads/JiXMODAL.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Cross-modal embedding,Domain adaptation,Image classification,Manifold,Zero-shot learning},
pages = {48--58},
publisher = {Elsevier Inc.},
title = {{Manifold regularized cross-modal embedding for zero-shot learning}},
volume = {378},
year = {2017}
}
@article{Girshick2012,
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GirschickSemseg.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
pages = {2--9},
pmid = {26656583},
title = {{Girshick{\_}Rich{\_}Feature{\_}Hierarchies{\_}2014{\_}CVPR{\_}paper}},
year = {2012}
}
@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E},
doi = {10.1145/584091.584093},
eprint = {9411012},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Shannon.pdf:pdf},
isbn = {0252725484},
issn = {07246811},
journal = {The Bell System Technical Journal},
number = {July 1928},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A mathematical theory of communication}},
url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
volume = {27},
year = {1948}
}
@article{Xian2016,
abstract = {We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.},
archivePrefix = {arXiv},
arxivId = {1603.08895},
author = {Xian, Yongqin and Akata, Zeynep and Sharma, Gaurav and Nguyen, Quynh and Hein, Matthias and Schiele, Bernt},
doi = {10.1109/CVPR.2016.15},
eprint = {1603.08895},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Xian2016.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
title = {{Latent Embeddings for Zero-shot Classification}},
url = {http://arxiv.org/abs/1603.08895},
year = {2016}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HeResNet.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Srivastava2015,
abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1507.06228},
author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/CVPR.2016.90},
eprint = {1507.06228},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SchmidhuberDepth.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10495258},
pages = {1--11},
pmid = {23554596},
title = {{Training Very Deep Networks}},
url = {http://arxiv.org/abs/1507.06228},
year = {2015}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Shelhamer2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor and Shelhamer, Evan and Darrell, Trevor and Long, Jonathan and Darrell, Trevor and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/LongSegmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2015}
}
@article{Tsai2017,
abstract = {Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.},
archivePrefix = {arXiv},
arxivId = {1703.05908},
author = {Tsai, Yao Hung Hubert and Huang, Liang Kang and Salakhutdinov, Ruslan},
doi = {10.1109/ICCV.2017.386},
eprint = {1703.05908},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/TsaiSEM.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3591--3600},
title = {{Learning Robust Visual-Semantic Embeddings}},
volume = {2017-Octob},
year = {2017}
}
@article{Hinton1986,
abstract = {(From the chapter) first section of this chapter stresses some of the virtues of distributed representations / second section considers the efficiency of distributed representations, and shows clearly why distributed representations can be better than local ones for certain classes of problems / final section discusses some difficult issues which are often avoided by advocates of distributed representations, such as the representation of constituent structure and the sequential focusing of processing effort on different aspects of a structure object (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Hinton, G E and McClelland, J L and Rumelhart, D E},
doi = {10.1146/annurev-psych-120710-100344},
eprint = {15334406},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HintonDistributed.pdf:pdf},
isbn = {0-262-68053-X},
issn = {1534-7362},
journal = {Parallel Distributed Processing},
pages = {77--109},
pmid = {21943171},
title = {{Distributed representations}},
year = {1986}
}
@article{Rosch1976,
abstract = {Categorizations which humans make of the concrete world are not arbitrary but highly determined. In taxonomies of concrete objects, there is one level of abstraction at which the most basic category cuts are made. Basic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. The four experiments of Part I define basic objects by demonstrating that in taxonomies of common concrete nouns in English based on class inclusion, basic objects are the most inclusive categories whose members: (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. The eight experiments of Part II explore implications of the structure of categories. Basic objects are shown to be the most inclusive categories for which a concrete image of the category as a whole can be formed, to be the first categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language. {\textcopyright} 1976.},
author = {Rosch, Eleanor and Mervis, Carolyn B. and Gray, Wayne D. and Johnson, David M. and Boyes-Braem, Penny},
doi = {10.1016/0010-0285(76)90013-X},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RoschCategories.pdf:pdf},
isbn = {0010-0285},
issn = {00100285},
journal = {Cognitive Psychology},
number = {3},
pages = {382--439},
title = {{Basic objects in natural categories}},
volume = {8},
year = {1976}
}
@article{Srikanth2005,
abstract = {Automatic image annotation is the task of automatically assigning words to an image that describe the content of the image. Machine learning approaches have been explored to model the association between words and images from an annotated set of images and generate annotations for a test image. The paper proposes methods to use a hierarchy defined on the annotation words derived from a text ontology to improve automatic image annotation and retrieval. Specifically, the hierarchy is used in the context of generating a visual vocabulary for representing images and as a framework for the proposed hierarchical classification approach for automatic image annotation. The effect of using the hierarchy in generating the visual vocabulary is demonstrated by improvements in the annotation performance of translation models. In addition to performance improvements, hierarchical classification approaches yield well to constructing multimedia ontologies. [ABSTRACT FROM AUTHOR]},
author = {Srikanth, Munirathnam and Varner, Joshua and Bowden, Mitchell and Moldovan, Dan},
doi = {10.1145/1076034.1076128},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SrikanthOntologies.pdf:pdf},
isbn = {1595930345},
issn = {01635840},
journal = {SIGIR Forum},
keywords = {ANNOTATIONS,Automatic image annotation,LEXICOLOGY,MACHINE learning,MACHINE theory,MULTIMEDIA systems,hierarchical classification models,image retrieval,ontologies,translation models},
pages = {552--558},
title = {{Exploiting Ontologies for Automatic Image Annotation.}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=lxh{\&}AN=19054808{\&}site=ehost-live},
year = {2005}
}
@article{Griffin2013,
author = {Griffin, Greg},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GriffinTaxonomies.pdf:pdf},
isbn = {9781424422432},
title = {{Learning and Using Taxonomies for Visual and Olfactory Classification Thesis by}},
volume = {2013},
year = {2013}
}
@book{Scholkopf2002,
author = {Sch{\"{o}}lkopf, B. and Bach, F.},
publisher = {MIT Press},
title = {{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
year = {2002}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/AlexNet.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Monay2004,
abstract = {We address the problem of unsupervised image auto-annotation with probabilistic latent space models. Unlike most previous works, which build latent space representations assuming equal relevance for the text and visual modalities, we propose a new way of modeling multi-modal co-occurrences, constraining the definition of the latent space to ensure its consistency in semantic terms (words), while retaining the ability to jointly model visual information. The concept is implemented by a linked pair of Probabilistic Latent Semantic Analysis (PLSA) models. On a 16000-image collection, we show with extensive experiments that our approach significantly outperforms previous joint models.},
author = {Monay, Florent and Gatica-perez, Daniel},
doi = {10.1145/1027527.1027608},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MonayPLSA.pdf:pdf},
isbn = {1581138938},
journal = {Proceedings of the 12th annual ACM international conference on Multimedia},
keywords = {automatic annotation of images,plsa,semantic indexing},
pages = {348--351},
title = {{PLSA-based Image Auto-Annotation : Constraining the Latent Space}},
year = {2004}
}
@article{Deselaers2011,
abstract = {Many computer vision approaches take for granted positive answers to questions such as {\&}{\#}x201C;Are semantic categories visually separable?{\&}{\#}x201D; and {\&}{\#}x201C;Is visual similarity correlated to semantic similarity?{\&}{\#}x201D;. In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances.},
author = {Deselaers, Thomas and Ferrari, Vittorio},
doi = {10.1109/CVPR.2011.5995474},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DeselaersSimilarity.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
journal = {Cvpr},
pages = {1777--1784},
title = {{Visual and semantic similarity in ImageNet}},
year = {2011}
}
@article{Guo2016,
abstract = {Deep learning algorithms are a subset of the machine learning algorithms, which aim at discovering multiple levels of distributed representations. Recently, numerous deep learning algorithms have been proposed to solve traditional artificial intelligence problems. This work aims to review the state-of-the-art in deep learning algorithms in computer vision by highlighting the contributions and challenges from over 210 recent research papers. It first gives an overview of various deep learning approaches and their recent developments, and then briefly describes their applications in diverse vision tasks, such as image classification, object detection, image retrieval, semantic segmentation and human pose estimation. Finally, the paper summarizes the future trends and challenges in designing and training deep neural networks.},
author = {Guo, Yanming and Liu, Yu and Oerlemans, Ard and Lao, Songyang and Wu, Song and Lew, Michael S.},
doi = {10.1016/j.neucom.2015.09.116},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GuoVisualunderstand.pdf:pdf},
isbn = {0925-2312},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Applications,Challenges,Computer vision,Deep learning,Developments,Trends},
pages = {27--48},
title = {{Deep learning for visual understanding: A review}},
volume = {187},
year = {2016}
}
@article{Gong2013,
abstract = {Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with approximate top-{\$}k{\$} ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10{\%}, obtaining the best reported performance in the literature.},
archivePrefix = {arXiv},
arxivId = {1312.4894},
author = {Gong, Yunchao and Jia, Yangqing and Leung, Thomas and Toshev, Alexander and Ioffe, Sergey},
eprint = {1312.4894},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GongDeepmultirank.pdf:pdf},
pages = {1--9},
pmid = {2094121},
title = {{Deep Convolutional Ranking for Multilabel Image Annotation}},
url = {http://arxiv.org/abs/1312.4894},
year = {2013}
}
@article{Lecun2015,
author = {Lecun, Yann and Hinton, Geoffrey E. and Bengio, Yoshua},
journal = {Nature},
pages = {436--444},
title = {{Deep Learning}},
volume = {521},
year = {2015}
}
@article{Cadieu2014,
abstract = {The primate visual system achieves remarkable visual object recognition performance even in brief presentations and under changes to object exemplar, geometric transformations, and background variation (a.k.a. core visual object recognition). This remarkable performance is mediated by the representation formed in inferior temporal (IT) cortex. In parallel, recent advances in machine learning have led to ever higher performing models of object recognition using artificial deep neural networks (DNNs). It remains unclear, however, whether the representational performance of DNNs rivals that of the brain. To accurately produce such a comparison, a major difficulty has been a unifying metric that accounts for experimental limitations such as the amount of noise, the number of neural recording sites, and the number trials, and computational limitations such as the complexity of the decoding classifier and the number of classifier training examples. In this work we perform a direct comparison that corrects for these experimental limitations and computational considerations. As part of our methodology, we propose an extension of "kernel analysis" that measures the generalization accuracy as a function of representational complexity. Our evaluations show that, unlike previous bio-inspired models, the latest DNNs rival the representational performance of IT cortex on this visual object recognition task. Furthermore, we show that models that perform well on measures of representational performance also perform well on measures of representational similarity to IT and on measures of predicting individual IT multi-unit responses. Whether these DNNs rely on computational mechanisms similar to the primate visual system is yet to be determined, but, unlike all previous bio-inspired models, that possibility cannot be ruled out merely on representational performance grounds.},
archivePrefix = {arXiv},
arxivId = {1406.3284},
author = {Cadieu, Charles F. and Hong, Ha and Yamins, Daniel L K and Pinto, Nicolas and Ardila, Diego and Solomon, Ethan A. and Majaj, Najib J. and DiCarlo, James J.},
doi = {10.1371/journal.pcbi.1003963},
eprint = {1406.3284},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/cadieu2014.pdf:pdf},
isbn = {1553-7358},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {12},
pmid = {25521294},
title = {{Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition}},
volume = {10},
year = {2014}
}
@article{Makadia2008,
abstract = {Automatically assigning keywords to images is of great interest as it allows one to retrieve, index, organize and understand large collections of image data. Many techniques have been proposed for image annotation in the last decade that give reasonable performance on standard datasets. However, most of these works fail to compare their methods with simple baseline techniques to justify the need for complex models and subsequent training. In this work, we introduce a new and simple baseline technique for image annotation that treats annotation as a retrieval problem. The proposed technique utilizes global low-level image features and a simple combination of basic distance measures to find nearest neighbors of a given image. The keywords are then assigned using a greedy label transfer mechanism. The proposed baseline method outperforms the current state-of-the-art methods on two standard and one large Web dataset. We believe that such a baseline measure will provide a strong platform to compare and better understand future annotation techniques.},
author = {Makadia, Ameesh and Pavlovic, Vladimir and Kumar, Sanjiv},
doi = {10.1007/978-3-540-88690-7-24},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MakadiaBaseline.pdf:pdf},
isbn = {3540886893},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {a baseline measure will,a greedy label transfer,a strong platform to,and one large web,are then assigned using,compare and better understand,current state-of-the-art methods on,dataset,future annotation techniques,mechanism,posed baseline outperforms the,provide,the pro-,two standard,we believe that such},
number = {PART 3},
pages = {316--329},
pmid = {16546397},
title = {{A new baseline for image annotation}},
volume = {5304 LNCS},
year = {2008}
}
@article{Robbins1951,
author = {Robbins, Herbert and Monro, Suttton},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {400--407},
title = {{A Stochastic Approximation Method}},
volume = {22},
year = {1951}
}
@article{Redmon,
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RedmonYOLO9000.pdf:pdf},
pages = {7263--7271},
title = {{Better , Faster , Stronger}}
}
@article{Zweig2007,
abstract = {We investigated the computational properties of natural object hierarchy in the context of constellation object class models, and its utility for object class recognition. We first observed an interesting computational property of the object hierarchy: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels (e.g., closed-frame vehicles or vehicles) exhibit higher recall but lower precision when compared with the class specific level (e.g., bus). These inherent differences suggest that combining object classifiers from different hierarchical levels into a single classifier may improve classification, as it appears like these models capture different aspects of the object. We describe a method to combine these classifiers, and analyze the conditions under which improvement can be guaranteed. When given a small sample of a new object class, we describe a method to transfer knowledge across the tree hierarchy, between related objects. Finally, we describe extensive experiments using object hierarchies obtained from publicly available datasets, and show that the combined classifiers significantly improve recognition results.},
author = {Zweig, Alon and Weinshall, Daphna},
doi = {10.1109/ICCV.2007.4409064},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZweigHierarchies.pdf:pdf},
isbn = {978-1-4244-1630-1},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {October},
title = {{Exploiting object hierarchy: Combining models from different category levels}},
year = {2007}
}
@article{Joliceur1984,
author = {Joliceur, P and Gluck, M A and Kosslyn, S M},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/JolicoeurPictures.pdf:pdf},
journal = {Cognitive Psychology},
pages = {243--275},
title = {{Pictures and Naming: Making the Connection}},
volume = {16},
year = {1984}
}
@article{Hodgkin1990,
abstract = {This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkinet al., 1952,J. Physiol.116, 424-448; Hodgkin and Huxley 1952,J. Physiol.116, 449-566). Its general object is to discuss the results of the preceding papers (Section 1), to put them into mathematical form (Section 2) and to whow that they will account for conduction and excitation in quantitative terms (Sections 3-6).},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hodgkin, A. L. and Huxley, A. F.},
doi = {10.1007/BF02459568},
eprint = {NIHMS150003},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HodgingHuxley.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$n1097-6256 (Linking)},
issn = {00928240},
journal = {Bulletin of Mathematical Biology},
number = {1-2},
pages = {25--71},
pmid = {2185861},
title = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
volume = {52},
year = {1990}
}
@article{Lei2018,
author = {Lei, Jie and Guo, Zhenyu and Wang, Yang},
doi = {10.1109/CRV.2017.21},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/LeiGuoCoarse.pdf:pdf},
isbn = {9781538628188},
journal = {Proceedings - 2017 14th Conference on Computer and Robot Vision, CRV 2017},
keywords = {convolutional neural networks,image classification,weakly supervised},
pages = {240--247},
title = {{Weakly Supervised Image Classification with Coarse and Fine Labels}},
volume = {2018-Janua},
year = {2018}
}
@article{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources â€“ such as text data â€“ both to train visual models and to constrain their pre- dictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18{\%} across thou- sands of novel labels never seen by the visual model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5650v3},
author = {Frome, Andrea and Corrado, Gs and Shlens, Jonathon},
eprint = {arXiv:1312.5650v3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FromeDevise.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural {\ldots}},
pages = {1--11},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@article{Weston2010,
abstract = {Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced sibling precision metric, where our method also obtains excellent results.},
author = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
doi = {10.1007/s10994-010-5198-3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/WestonWordimg.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Embedding,Image annotation,Large scale,Learning to rank},
number = {1},
pages = {21--35},
pmid = {2895801},
title = {{Large scale image annotation: Learning to rank with joint word-image embeddings}},
volume = {81},
year = {2010}
}
@article{DiCarlo2012,
author = {DiCarlo, James J. and Yoccolan, Davide and Rust, Nicole C.},
doi = {10.1016/j.neuron.2012.01.010.How},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DiCarlo2013.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
number = {3},
pages = {415--434},
pmid = {22325196},
title = {{How does the brain solve visual object recognition?}},
volume = {73},
year = {2012}
}
@article{Socher,
abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training im-ages and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what ob-jects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or un-seen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.},
archivePrefix = {arXiv},
arxivId = {1301.3666},
author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
eprint = {1301.3666},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SocherXmodal.pdf:pdf},
issn = {10495258},
title = {{Zero-Shot Learning Through Cross-Modal Transfer}}
}
@article{Peterson2018,
abstract = {Modern convolutional neural networks (CNNs) are able to achieve human-level object classification accuracy on specific tasks, and currently outperform competing models in explaining complex human visual representations. However, the categorization problem is posed differently for these networks than for humans: the accuracy of these networks is evaluated by their ability to identify single labels assigned to each image. These labels often cut arbitrarily across natural psychological taxonomies (e.g., dogs are separated into breeds, but never jointly categorized as "dogs"), and bias the resulting representations. By contrast, it is common for children to hear both "dog" and "Dalmatian" to describe the same stimulus, helping to group perceptually disparate objects (e.g., breeds) into a common mental class. In this work, we train CNN classifiers with multiple labels for each image that correspond to different levels of abstraction, and use this framework to reproduce classic patterns that appear in human generalization behavior.},
archivePrefix = {arXiv},
arxivId = {1805.07647},
author = {Peterson, Joshua C. and Soulos, Paul and Nematzadeh, Aida and Griffiths, Thomas L.},
eprint = {1805.07647},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Peterson.pdf:pdf},
title = {{Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels}},
url = {http://arxiv.org/abs/1805.07647},
year = {2018}
}
@misc{Le,
author = {Lecun, Y. and Boser, B. and Denker, J.S. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/lecun89.pdf:pdf},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
year = {1989}
}
@article{Palatucci2009,
abstract = {We consider the problem of zero-shot learning, where the goal is to$\backslash$nlearn a clas- sifier f : X ï¿½?Y that must predict novel values of$\backslash$nY that were omitted from the training set. To achieve this, we define$\backslash$nthe notion of a semantic output code classifier (SOC) which utilizes$\backslash$na knowledge base of semantic properties of Y to extrapolate to novel$\backslash$nclasses. We provide a formalism for this type of classifier and study$\backslash$nits theoretical properties in a PAC framework, showing conditions$\backslash$nun- der which the classifier can accurately predict novel classes.$\backslash$nAs a case study, we build a SOC classifier for a neural decoding$\backslash$ntask and show that it can often predict words that people are thinking$\backslash$nabout from functional magnetic resonance images (fMRI) of their neural$\backslash$nactivity, even without training examples for those words.},
author = {Palatucci, Mark and Hinton, Geoffrey E and Pomerleau, Dean and Mitchell, Tom M},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PalatucciZSL.pdf:pdf},
isbn = {9781615679119},
issn = {{\textless}null{\textgreater}},
journal = {Neural Information Processing Systems},
keywords = {Machine learning,zero-shot learning},
pages = {1--9},
title = {{Zero-Shot Learning with Semantic Output Codes}},
year = {2009}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/mikolovSkp.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@misc{Abadi2015,
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
title = {{TensorFlow: Large-scale machine learning on heterogeneous systems}},
url = {tensorflow.org},
year = {2015}
}
@article{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, D.G.},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SIFT.pdf:pdf},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
pages = {1150--1157 vol.2},
pmid = {15806121},
primaryClass = {cs},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/document/790410/},
year = {1999}
}
@article{Sun2015,
abstract = {People believe that depth plays an important role in success of deep neural networks (DNN). However, this belief lacks solid theoretical justifications as far as we know. We investigate role of depth from perspective of margin bound. In margin bound, expected error is upper bounded by empirical margin error plus Rademacher Average (RA) based capacity term. First, we derive an upper bound for RA of DNN, and show that it increases with increasing depth. This indicates negative impact of depth on test performance. Second, we show that deeper networks tend to have larger representation power (measured by Betti numbers based complexity) than shallower networks in multi-class setting, and thus can lead to smaller empirical margin error. This implies positive impact of depth. The combination of these two results shows that for DNN with restricted number of hidden units, increasing depth is not always good since there is a tradeoff between positive and negative impacts. These results inspire us to seek alternative ways to achieve positive impact of depth, e.g., imposing margin-based penalty terms to cross entropy loss so as to reduce empirical margin error without increasing depth. Our experiments show that in this way, we achieve significantly better test performance.},
archivePrefix = {arXiv},
arxivId = {1506.05232},
author = {Sun, Shizhao and Chen, Wei and Wang, Liwei and Liu, Xiaoguang and Liu, Tie-Yan},
eprint = {1506.05232},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/CnnDeth.pdf:pdf},
isbn = {9781577357605},
title = {{On the Depth of Deep Neural Networks: A Theoretical View}},
url = {http://arxiv.org/abs/1506.05232},
year = {2015}
}
@article{Zhang2015,
abstract = {In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information ($\backslash$eg attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.},
archivePrefix = {arXiv},
arxivId = {1509.04767},
author = {Zhang, Ziming and Saligrama, Venkatesh},
doi = {10.1109/ICCV.2015.474},
eprint = {1509.04767},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/saligram2015.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
title = {{Zero-Shot Learning via Semantic Similarity Embedding}},
url = {http://arxiv.org/abs/1509.04767},
year = {2015}
}
@article{Razavian2014,
abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the $\backslash$overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the $\backslash$overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the $\backslash$overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or {\$}L2{\$} distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
doi = {10.1109/CVPRW.2014.131},
eprint = {1403.6382},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RazavianCNN.pdf:pdf},
isbn = {9781479943098},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {512--519},
pmid = {87882338},
title = {{CNN features off-the-shelf: An astounding baseline for recognition}},
year = {2014}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/inceptionV3.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RussakovskyIMGNET.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Author,
author = {Author, Anonymous and Author, Anonymous},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BaiSemantic.pdf:pdf},
number = {1},
pages = {1--9},
title = {{Polynomial Semantic Indexing}}
}
@article{Linnainmaa1976,
abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Linnainmaa1976.pdf:pdf},
issn = {00063835},
journal = {Bit},
number = {2},
pages = {146--160},
title = {{Taylor expansion of the accumulated rounding error}},
volume = {16},
year = {1976}
}
@article{Wang2015,
abstract = {Recent advances in deep learning have led to significant progress in the computer vision field, especially for visual object recognition tasks. The features useful for object classification are learned by feed-forward deep convolutional neural networks (CNNs) automatically, and they are shown to be able to predict and decode neural representations in the ventral visual pathway of humans and monkeys. However, despite the huge amount of work on optimizing CNNs, there has not been much research focused on linking CNNs with guiding principles from the human visual cortex. In this work, we propose a network optimization strategy inspired by both of the developmental trajectory of children's visual object recognition capabilities, and Bar (2003), who hypothesized that basic level information is carried in the fast magnocellular pathway through the prefrontal cortex (PFC) and then projected back to inferior temporal cortex (IT), where subordinate level categorization is achieved. We instantiate this idea by training a deep CNN to perform basic level object categorization first, and then train it on subordinate level categorization. We apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC 2012 dataset and show that the top-5 accuracy increases from 80.13{\%} to 82.14{\%}, demonstrating the effectiveness of the method. We also show that subsequent transfer learning on smaller datasets gives superior results.},
archivePrefix = {arXiv},
arxivId = {1511.04103},
author = {Wang, Panqu and Cottrell, Garrison W.},
doi = {10.1162/jocn_a_00409},
eprint = {1511.04103},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/WangHierarchy.pdf:pdf},
isbn = {9780192880512},
issn = {0898-929X},
pages = {1--13},
pmid = {23647519},
title = {{Basic Level Categorization Facilitates Visual Object Recognition}},
url = {http://arxiv.org/abs/1511.04103},
year = {2015}
}
@article{Grosky2002,
abstract = {We present the results of our work that seek to negotiate the gap between low-level features and high-level concepts in the domain of web document retrieval. This work concerns a technique, called the latent semantic indexing (LSI), which has been used for textual information retrieval for many years. In this environment, LSI determines clusters of co-occurring keywords so that a query which uses a particular keyword can then retrieve documents perhaps not containing this keyword, but containing other keywords from the same cluster. In this paper, we examine the use of this technique for content-based web document retrieval, using both keywords and image features to represent the documents. Two different approaches to image feature representation, namely, color histograms and color anglograms, are adopted and evaluated. Experimental results show that LSI, together with both textual and visual features, is able to extract the underlying semantic structure of web documents, thus helping to improve the retrieval performance significantly, even when querying is done using only keywords.},
author = {Grosky, W.I.},
doi = {10.1109/TMM.2002.1017733},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZhaoSemGap.pdf:pdf},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {but containing other keywords,can then retrieve documents,content-based,from the same cluster,image features,in,keyword,of this technique for,perhaps not containing this,this paper,using both keywords and,we examine the use,web document retrieval},
number = {2},
pages = {189--200},
title = {{Narrowing the semantic gap - improved text-based web document retrieval using visual features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1017733},
volume = {4},
year = {2002}
}
@article{Norouzi2013,
abstract = {Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional $\backslash$nway{\{}{\}} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing $\backslash$nway{\{}{\}} image classifier and a semantic word embedding model, which contains the {\$}\backslashn{\$} class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.},
archivePrefix = {arXiv},
arxivId = {1312.5650},
author = {Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S. and Dean, Jeffrey},
eprint = {1312.5650},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/NorouziCONSE.pdf:pdf},
issn = {10495258},
pages = {1--9},
title = {{Zero-Shot Learning by Convex Combination of Semantic Embeddings}},
url = {http://arxiv.org/abs/1312.5650},
year = {2013}
}
@article{Zhao2001,
abstract = {In this paper, we present the results of a project that seeks to transform low-level features to a higher level of meaning. This project concerns a technique, latent semantic indexing (LSI), in conjunction with normalization and term weighting, which have been used for full-text retrieval for many years. In this environment, LSI determines clusters of co-occurring keywords, sometimes, called concepts, so that a query which uses a particular keyword can then retrieve documents perhaps not containing this keyword, but containing other keywords from the same cluster. In this paper, we examine the use of this technique for content-based image retrieval, using two different approaches to image feature representation. We also study the integration of visual features and textual keywords and the results show that it can help improve the retrieval performance significantly. {\textcopyright} 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Zhao, Rong and Grosky, W. I.},
doi = {10.1016/S0031-3203(01)00062-0},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZhaoGrosskyPatternRecognition.pdf:pdf},
isbn = {9783540429128},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Anglogram,Content-based,Feature maps,Image retrieval,Latent semantic indexing,Semantic gap},
number = {3},
pages = {593--600},
title = {{Negotiating the semantic gap: From feature maps to semantic landscapes}},
volume = {35},
year = {2001}
}
@article{Xu2000,
abstract = {The authors present a Bayesian framework for understanding how adults and children learn the meanings of words. The theory explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents, by making rational inductive inferences that integrate prior knowledge about plausible word meanings with the statistical structure of the observed examples. The theory addresses shortcomings of the two best known approaches to modeling word learning, based on deductive hypothesis elimination and associative learning. Three experiments with adults and children test the Bayesian account's predictions in the context of learning words for object categories at multiple levels of a taxonomic hierarchy. Results provide strong support for the Bayesian account over competing accounts, in terms of both quantitative model fits and the ability to explain important qualitative phenomena. Several extensions of the basic theory are discussed, illustrating the broader potential for Bayesian models of word learning.},
author = {Xu, Fei and Tenenbaum, Joshua B},
doi = {10.1037/0033-295X.114.2.245},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/XuTennebaum.pdf:pdf},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Proceedings of the 22nd Annual Meeting of the Cognitive Science Society},
keywords = {Bayes Theorem,Child,Humans,Models,Preschool,Psychological,Verbal Learning,Vocabulary},
pages = {517--522},
pmid = {17500627},
title = {{Word learning as Bayesian inference.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17500627},
year = {2000}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Dropout.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
