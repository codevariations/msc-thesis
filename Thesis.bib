Automatically generated by Mendeley Desktop 1.19
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1112.6209},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
eprint = {arXiv:1112.6209},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Rosenblatt1958.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471(Electronic);0033-295X(Print)},
journal = {Psychological Review},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in {\ldots}}},
url = {http://psycnet.apa.org/journals/rev/65/6/386.pdf{\%}5Cnpapers://c53d1644-cd41-40df-912d-ee195b4a4c2b/Paper/p15420},
volume = {65},
year = {1958}
}
@book{Prince2012,
author = {Prince, Simon J. D.},
isbn = {9781107011793},
title = {{Computer Vision: Models, Learning, and Inference}},
year = {2012}
}
@article{Guillaumin2010,
author = {Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Schmid, Cordelia and Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Discrim-, Cordelia Schmid Tagprop and Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Schmid, Cordelia and Kuntzmann, Laboratoire Jean},
doi = {10.1109/ICCV.2009.5459266>},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/tagprop.pdf:pdf},
title = {{TagProp : Discriminative metric learning in nearest neighbor models for image auto-annotation To cite this version : TagProp : Discriminative Metric Learning in Nearest Neighbor Models for Image Auto-Annotation}},
year = {2010}
}
@article{Taigman2014,
abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35{\%} on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27{\%}, closely approaching human-level performance.},
archivePrefix = {arXiv},
arxivId = {1501.05703},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
doi = {10.1109/CVPR.2014.220},
eprint = {1501.05703},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/TaigmanDEEPFACE.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1701--1708},
pmid = {21646680},
title = {{DeepFace: Closing the gap to human-level performance in face verification}},
year = {2014}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Reed2016,
author = {Reed, Scott and Akata, Zeynep and Lee, Honglak and Schiele, Bernt},
doi = {10.1109/CVPR.2016.13},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/scottreed.pdf:pdf},
title = {{Learning Deep Representations of Fine-Grained Visual Descriptions}},
year = {2016}
}
@book{Greenberg1994,
author = {Greenberg, Marvin Jay},
edition = {3},
isbn = {0-7167-2446-4},
publisher = {W. H. Freeman and Company},
title = {{Euclidean and Non-Euclidean Geometries - Development and History}},
year = {1994}
}
@article{Deselaers2011,
abstract = {Many computer vision approaches take for granted positive answers to questions such as {\&}{\#}x201C;Are semantic categories visually separable?{\&}{\#}x201D; and {\&}{\#}x201C;Is visual similarity correlated to semantic similarity?{\&}{\#}x201D;. In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances.},
author = {Deselaers, Thomas and Ferrari, Vittorio},
doi = {10.1109/CVPR.2011.5995474},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DeselaersSimilarity.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
journal = {Cvpr},
pages = {1777--1784},
title = {{Visual and semantic similarity in ImageNet}},
year = {2011}
}
@article{Xu2000,
abstract = {The authors present a Bayesian framework for understanding how adults and children learn the meanings of words. The theory explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents, by making rational inductive inferences that integrate prior knowledge about plausible word meanings with the statistical structure of the observed examples. The theory addresses shortcomings of the two best known approaches to modeling word learning, based on deductive hypothesis elimination and associative learning. Three experiments with adults and children test the Bayesian account's predictions in the context of learning words for object categories at multiple levels of a taxonomic hierarchy. Results provide strong support for the Bayesian account over competing accounts, in terms of both quantitative model fits and the ability to explain important qualitative phenomena. Several extensions of the basic theory are discussed, illustrating the broader potential for Bayesian models of word learning.},
author = {Xu, Fei and Tenenbaum, Joshua B},
doi = {10.1037/0033-295X.114.2.245},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/XuTennebaum.pdf:pdf},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Proceedings of the 22nd Annual Meeting of the Cognitive Science Society},
keywords = {Bayes Theorem,Child,Humans,Models,Preschool,Psychological,Verbal Learning,Vocabulary},
pages = {517--522},
pmid = {17500627},
title = {{Word learning as Bayesian inference.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17500627},
year = {2000}
}
@article{Fu2016,
abstract = {Despite significant progress in object categorization, in recent years, a number of important challenges remain, mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of semi-supervised vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot and open set recognition using a unified framework. Specifically, we propose a maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms, ensuring that labeled samples are projected closest to their correct prototypes, in the embedding space, than to others. We show that resulting model shows improvements in supervised, zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.},
archivePrefix = {arXiv},
arxivId = {1604.07093},
author = {Fu, Yanwei and Sigal, Leonid},
doi = {10.1109/CVPR.2016.576},
eprint = {1604.07093},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FuVOCAB.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
title = {{Semi-supervised Vocabulary-informed Learning}},
url = {http://arxiv.org/abs/1604.07093},
year = {2016}
}
@article{Farhadi2010,
abstract = {We propose an approach to find and describe objects within broad domains. We introduce a new dataset that provides annotation for sharing models of appearance and correlation across categories. We use it to learn part and category detectors. These serve as the visual basis for an integrated model of objects. We describe objects by the spatial arrangement of their attributes and the interactions between them. Using this model, our system can find animals and vehicles that it has not seen and infer attributes, such as function and pose. Our experiments demonstrate that we can more reliably locate and describe both familiar and unfamiliar objects, compared to a baseline that relies purely on basic category detectors.},
author = {Farhadi, Ali and Endres, Ian and Hoiem, Derek},
doi = {10.1109/CVPR.2010.5539924},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FarhadiAttribute.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2352--2359},
pmid = {13269725317367411122},
title = {{Attribute-centric recognition for cross-category generalization}},
year = {2010}
}
@article{Rosch1976,
abstract = {Categorizations which humans make of the concrete world are not arbitrary but highly determined. In taxonomies of concrete objects, there is one level of abstraction at which the most basic category cuts are made. Basic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. The four experiments of Part I define basic objects by demonstrating that in taxonomies of common concrete nouns in English based on class inclusion, basic objects are the most inclusive categories whose members: (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. The eight experiments of Part II explore implications of the structure of categories. Basic objects are shown to be the most inclusive categories for which a concrete image of the category as a whole can be formed, to be the first categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language. {\textcopyright} 1976.},
author = {Rosch, Eleanor and Mervis, Carolyn B. and Gray, Wayne D. and Johnson, David M. and Boyes-Braem, Penny},
doi = {10.1016/0010-0285(76)90013-X},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RoschCategories.pdf:pdf},
isbn = {0010-0285},
issn = {00100285},
journal = {Cognitive Psychology},
number = {3},
pages = {382--439},
title = {{Basic objects in natural categories}},
volume = {8},
year = {1976}
}
@article{Norouzi2013,
abstract = {Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional $\backslash$nway{\{}{\}} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing $\backslash$nway{\{}{\}} image classifier and a semantic word embedding model, which contains the {\$}\backslashn{\$} class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.},
archivePrefix = {arXiv},
arxivId = {1312.5650},
author = {Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S. and Dean, Jeffrey},
eprint = {1312.5650},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/NorouziCONSE.pdf:pdf},
issn = {10495258},
pages = {1--9},
title = {{Zero-Shot Learning by Convex Combination of Semantic Embeddings}},
url = {http://arxiv.org/abs/1312.5650},
year = {2013}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Dropout.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Nickel2018,
abstract = {We are concerned with the discovery of hierarchical relationships from large-scale unstructured similarity scores. For this purpose, we study different models of hyperbolic space and find that learning embeddings in the Lorentz model is substantially more efficient than in the Poincar$\backslash$'e-ball model. We show that the proposed approach allows us to learn high-quality embeddings of large taxonomies which yield improvements over Poincar$\backslash$'e embeddings, especially in low dimensions. Lastly, we apply our model to discover hierarchies in two real-world datasets: we show that an embedding in hyperbolic space can reveal important aspects of a company's organizational structure as well as reveal historical relationships between language families.},
archivePrefix = {arXiv},
arxivId = {1806.03417},
author = {Nickel, Maximilian and Kiela, Douwe},
eprint = {1806.03417},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/NickelLorenz.pdf:pdf},
issn = {1938-7228},
title = {{Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry}},
url = {http://arxiv.org/abs/1806.03417},
year = {2018}
}
@misc{Le,
author = {Lecun, Y. and Boser, B. and Denker, J.S. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/lecun89.pdf:pdf},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
year = {1989}
}
@article{Mikolov,
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MikoloW2V.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
pages = {1--9},
pmid = {903},
title = {{5021-Distributed-Representations-of-Words-and-Phrases-and-Their-Compositionality}}
}
@article{Rumelhart1985,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
journal = {Institute for Cognitive Science Report},
number = {8506},
title = {{Learning Internal Representations by Error Propagation}},
year = {1985}
}
@article{Aslandogan1997,
abstract = {Image retrievaf based on semantic contents involves extrac-tion, modelling and indexing of content information. While extraction of abstract contents is a hard problem, it is only part of the bigger picture. In this paper we use knowledge about the semantic contents of images to improve retrieval effectiveness. In particular we use Word Net, an electronic Iexicaf system for query and database expansion. Our con-tent model facilitates novel uses of WordNet. We also pro-pose a new normalization formula, an object significance scheme and evaluate their effectiveness with real user ex-periments. We describe the experiment setup and provide quantitative evaluation of each technique.},
author = {Aslandogan, Y Alp and Thier, Chuck and Yu, Clement T. and Zou, Jon and Rishe, Naphtali},
doi = {10.1145/278459.258591},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/AslandoganSemantics.pdf:pdf},
isbn = {0-89791-836-3},
issn = {01635840},
journal = {ACM SIGIR Forum},
keywords = {cplj94,gzcs94,hck90,lw93,rs9s,sq96,textuaf description or captions,user defined attributes},
number = {SI},
pages = {286--295},
title = {{Using semantic contents and WordNet in image retrieval}},
url = {http://delivery.acm.org/10.1145/260000/258591/p286-aslandogan.pdf?ip=130.85.58.228{\&}id=258591{\&}acc=ACTIVE SERVICE{\&}key=5F8E7AA76238C9EB.E2B546BDBAFC5578.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=921946910{\&}CFTOKEN=99821475{\&}{\_}{\_}acm{\_}{\_}=1491753365{\_}6aa72903a3c6962ac840},
volume = {31},
year = {1997}
}
@article{Sun2015,
abstract = {People believe that depth plays an important role in success of deep neural networks (DNN). However, this belief lacks solid theoretical justifications as far as we know. We investigate role of depth from perspective of margin bound. In margin bound, expected error is upper bounded by empirical margin error plus Rademacher Average (RA) based capacity term. First, we derive an upper bound for RA of DNN, and show that it increases with increasing depth. This indicates negative impact of depth on test performance. Second, we show that deeper networks tend to have larger representation power (measured by Betti numbers based complexity) than shallower networks in multi-class setting, and thus can lead to smaller empirical margin error. This implies positive impact of depth. The combination of these two results shows that for DNN with restricted number of hidden units, increasing depth is not always good since there is a tradeoff between positive and negative impacts. These results inspire us to seek alternative ways to achieve positive impact of depth, e.g., imposing margin-based penalty terms to cross entropy loss so as to reduce empirical margin error without increasing depth. Our experiments show that in this way, we achieve significantly better test performance.},
archivePrefix = {arXiv},
arxivId = {1506.05232},
author = {Sun, Shizhao and Chen, Wei and Wang, Liwei and Liu, Xiaoguang and Liu, Tie-Yan},
eprint = {1506.05232},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/CnnDeth.pdf:pdf},
isbn = {9781577357605},
title = {{On the Depth of Deep Neural Networks: A Theoretical View}},
url = {http://arxiv.org/abs/1506.05232},
year = {2015}
}
@misc{Princeton2010,
author = {Princeton},
title = {{WordNet - Princeton University}},
url = {wordnet.princeton.edu},
year = {2010}
}
@inproceedings{Netzer2011,
author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
booktitle = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning},
title = {{Reading Digits in Natural Images with Unsupervised Feature Learning}},
year = {2011}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/inceptionV3.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Socher,
abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training im-ages and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what ob-jects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or un-seen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.},
archivePrefix = {arXiv},
arxivId = {1301.3666},
author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
eprint = {1301.3666},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SocherXmodal.pdf:pdf},
issn = {10495258},
title = {{Zero-Shot Learning Through Cross-Modal Transfer}}
}
@article{Makadia2008,
abstract = {Automatically assigning keywords to images is of great interest as it allows one to retrieve, index, organize and understand large collections of image data. Many techniques have been proposed for image annotation in the last decade that give reasonable performance on standard datasets. However, most of these works fail to compare their methods with simple baseline techniques to justify the need for complex models and subsequent training. In this work, we introduce a new and simple baseline technique for image annotation that treats annotation as a retrieval problem. The proposed technique utilizes global low-level image features and a simple combination of basic distance measures to find nearest neighbors of a given image. The keywords are then assigned using a greedy label transfer mechanism. The proposed baseline method outperforms the current state-of-the-art methods on two standard and one large Web dataset. We believe that such a baseline measure will provide a strong platform to compare and better understand future annotation techniques.},
author = {Makadia, Ameesh and Pavlovic, Vladimir and Kumar, Sanjiv},
doi = {10.1007/978-3-540-88690-7-24},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MakadiaBaseline.pdf:pdf},
isbn = {3540886893},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {a baseline measure will,a greedy label transfer,a strong platform to,and one large web,are then assigned using,compare and better understand,current state-of-the-art methods on,dataset,future annotation techniques,mechanism,posed baseline outperforms the,provide,the pro-,two standard,we believe that such},
number = {PART 3},
pages = {316--329},
pmid = {16546397},
title = {{A new baseline for image annotation}},
volume = {5304 LNCS},
year = {2008}
}
@techreport{Shaoul2010,
author = {Shaoul, Cyrus and Westbury, Chris},
title = {{The westbury lab wikipedia corpus.}},
year = {2010}
}
@article{Karpathy2015,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Li, Fei Fei},
doi = {10.1109/CVPR.2015.7298932},
eprint = {1412.2306},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/KarpathyDeepvisual.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {4},
pages = {3128--3137},
pmid = {16873662},
title = {{Deep visual-semantic alignments for generating image descriptions}},
volume = {39},
year = {2015}
}
@book{Trindade2013,
abstract = {Sentiment analysis is an area of research that has gained considerable attention in recent years due to the increasing availability of opinionated information online. The majority of the work in sentiment analysis considers the polarity of word terms rather than the polarity of specific senses of the word but different senses of a word can have different opinion-related properties. In order to address this issue we consider novel semantic features of words in the context of a sentence. We take a sentence as a sequence of words augmented with features based on word sense disambiguation and sentiment lexicons with sense specific opinion-related properties. We then use a factored version of the sequence kernel in a support vector machine, and apply it to sentiment classification of sentences. We evaluate this sentiment analysis methodology on three publicly available corpuses. We also evaluate the effectiveness of several publicly available sense specific polarity lexicons and combinations. Experiments show that our factored approach offers improvements over the surface words baseline and other state-of-the-art kernels. {\textcopyright} 2013 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0445v2},
author = {Trindade, Luis and Wang, Hui and Blackburn, William and Rooney, Niall},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39593-2},
eprint = {arXiv:1305.0445v2},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BengioRepresentations.pdf:pdf},
isbn = {978-3-642-39592-5},
issn = {03029743},
keywords = {Information Retrieval,Kernel Methods,Opinion Mining,Polarity Classification,Sentiment Analysis,Social Media,Word Sense Disambiguation},
number = {July},
pages = {284--296},
title = {{Statistical Language and Speech Processing}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84883190443{\&}partnerID=tZOtx3y1},
volume = {7978},
year = {2013}
}
@article{Changpinyo2017,
abstract = {Leveraging class semantic descriptions and examples of known objects, zero-shot learning makes it possible to train a recognition model for an object class whose examples are not available. In this paper, we propose a novel zero-shot learning model that takes advantage of clustering structures in the semantic embedding space. The key idea is to impose the structural constraint that semantic representations must be predictive of the locations of their corresponding visual exemplars. To this end, this reduces to training multiple kernel-based regressors from semantic representation-exemplar pairs from labeled data of the seen object categories. Despite its simplicity, our approach significantly outperforms existing zero-shot learning methods on standard benchmark datasets, including the ImageNet dataset with more than 20,000 unseen categories.},
archivePrefix = {arXiv},
arxivId = {1605.08151},
author = {Changpinyo, Soravit and Chao, Wei Lun and Sha, Fei},
doi = {10.1109/ICCV.2017.376},
eprint = {1605.08151},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Changpinyo2017.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3496--3505},
title = {{Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning}},
volume = {2017-Octob},
year = {2017}
}
@book{Cannon1997,
author = {Cannon, James W and Floyd, William J and Kenyon, Richard and Parry, Walter R},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/CannonHYPER.pdf:pdf},
pages = {59--115},
title = {{Hyperbolic Geometry}},
volume = {31},
year = {1997}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/AlexNet.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Zhang2015,
abstract = {In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information ($\backslash$eg attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.},
archivePrefix = {arXiv},
arxivId = {1509.04767},
author = {Zhang, Ziming and Saligrama, Venkatesh},
doi = {10.1109/ICCV.2015.474},
eprint = {1509.04767},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/saligram2015.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
title = {{Zero-Shot Learning via Semantic Similarity Embedding}},
url = {http://arxiv.org/abs/1509.04767},
year = {2015}
}
@article{Wang2018a,
abstract = {We consider the problem of zero-shot recognition: learning a visual classifier for a category with zero training examples, just using the word embedding of the category and its relationship to other categories, which visual data are provided. The key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class. In this paper, we build upon the recently introduced Graph Convolutional Network (GCN) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers. Given a learned knowledge graph (KG), our approach takes as input semantic embeddings for each node (representing visual category). After a series of graph convolutions, we predict the visual classifier for each category. During training, the visual classifiers for a few categories are given to learn the GCN parameters. At test time, these filters are used to predict the visual classifiers of unseen categories. We show that our approach is robust to noise in the KG. More importantly, our approach provides significant improvement in performance compared to the current state-of-the-art results (from 2 {\~{}} 3{\%} on some metrics to whopping 20{\%} on a few).},
archivePrefix = {arXiv},
arxivId = {1803.08035},
author = {Wang, Xiaolong and Ye, Yufei and Gupta, Abhinav},
doi = {10.1109/CVPR.2018.00717},
eprint = {1803.08035},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SOTA.pdf:pdf},
title = {{Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs}},
url = {http://arxiv.org/abs/1803.08035},
year = {2018}
}
@book{Iversen1992,
author = {Iversen, Birger},
publisher = {Cambridge University Press},
title = {{Hyperbolic geometry, volume 25}},
year = {1992}
}
@article{Shigeto2015,
abstract = {This paper discusses the effect of hubness in zero-shot learning, when ridge regression is used to find a mapping between the example space to the label space. Contrary to the existing approach, which attempts to find a mapping from the example space to the label space, we show that mapping labels into the example space is desirable to suppress the emergence of hubs in the subsequent nearest neighbor search step. Assuming a simple data model, we prove that the proposed approach indeed reduces hubness. This was verified empirically on the tasks of bilingual lexicon extraction and image labeling: hubness was reduced with both of these tasks and the accuracy was improved accordingly.},
archivePrefix = {arXiv},
arxivId = {1507.00825},
author = {Shigeto, Yutaro and Suzuki, Ikumi and Hara, Kazuo and Shimbo, Masashi and Matsumoto, Yuji},
doi = {10.1007/978-3-319-23528-8_9},
eprint = {1507.00825},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Shigato2015.pdf:pdf},
isbn = {9783319235271},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {135--151},
pmid = {22183238},
title = {{Ridge regression, hubness, and zero-shot learning}},
volume = {9284},
year = {2015}
}
@article{Monay2004,
abstract = {We address the problem of unsupervised image auto-annotation with probabilistic latent space models. Unlike most previous works, which build latent space representations assuming equal relevance for the text and visual modalities, we propose a new way of modeling multi-modal co-occurrences, constraining the definition of the latent space to ensure its consistency in semantic terms (words), while retaining the ability to jointly model visual information. The concept is implemented by a linked pair of Probabilistic Latent Semantic Analysis (PLSA) models. On a 16000-image collection, we show with extensive experiments that our approach significantly outperforms previous joint models.},
author = {Monay, Florent and Gatica-perez, Daniel},
doi = {10.1145/1027527.1027608},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MonayPLSA.pdf:pdf},
isbn = {1581138938},
journal = {Proceedings of the 12th annual ACM international conference on Multimedia},
keywords = {automatic annotation of images,plsa,semantic indexing},
pages = {348--351},
title = {{PLSA-based Image Auto-Annotation : Constraining the Latent Space}},
year = {2004}
}
@article{Palatucci2009,
abstract = {We consider the problem of zero-shot learning, where the goal is to$\backslash$nlearn a clas- sifier f : X �?Y that must predict novel values of$\backslash$nY that were omitted from the training set. To achieve this, we define$\backslash$nthe notion of a semantic output code classifier (SOC) which utilizes$\backslash$na knowledge base of semantic properties of Y to extrapolate to novel$\backslash$nclasses. We provide a formalism for this type of classifier and study$\backslash$nits theoretical properties in a PAC framework, showing conditions$\backslash$nun- der which the classifier can accurately predict novel classes.$\backslash$nAs a case study, we build a SOC classifier for a neural decoding$\backslash$ntask and show that it can often predict words that people are thinking$\backslash$nabout from functional magnetic resonance images (fMRI) of their neural$\backslash$nactivity, even without training examples for those words.},
author = {Palatucci, Mark and Hinton, Geoffrey E and Pomerleau, Dean and Mitchell, Tom M},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PalatucciZSL.pdf:pdf},
isbn = {9781615679119},
issn = {{\textless}null{\textgreater}},
journal = {Neural Information Processing Systems},
keywords = {Machine learning,zero-shot learning},
pages = {1--9},
title = {{Zero-Shot Learning with Semantic Output Codes}},
year = {2009}
}
@book{Ungar2005,
author = {Ungar, Abraham Albert},
publisher = {World Scientific},
title = {{Analytic hyperbolic geometry: Mathematical foundations and applications}},
year = {2005}
}
@article{Huang2012,
abstract = {Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one represen- tation per word. This is problematic because words are often polysemous and global con- text can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the se- mantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning mul- tiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper- forms competitive baselines and other neural language models.},
archivePrefix = {arXiv},
arxivId = {1602.07019},
author = {Huang, Eric H and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
doi = {10.1002/9781118510001.ch8},
eprint = {1602.07019},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HuangWordemb.pdf:pdf},
isbn = {9781937284244},
issn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {873--882},
pmid = {1710995},
title = {{ImprovingWord Representations via Global Context and MultipleWord Prototypes}},
year = {2012}
}
@article{Griffin2013,
author = {Griffin, Greg},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GriffinTaxonomies.pdf:pdf},
isbn = {9781424422432},
title = {{Learning and Using Taxonomies for Visual and Olfactory Classification Thesis by}},
volume = {2013},
year = {2013}
}
@article{JiaDeng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\&}{\#}x201C;ImageNet{\&}{\#}x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
doi = {10.1109/CVPRW.2009.5206848},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DengIMGNET.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {248--255},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
year = {2009}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/mikolovSkp.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Changpinyo2016,
abstract = {Given semantic descriptions of object classes, zero-shot learning aims to accurately recognize objects of the unseen classes, from which no examples are available at the training stage, by associating them to the seen classes, from which labeled examples are provided. We propose to tackle this problem from the perspective of manifold learning. Our main idea is to align the semantic space that is derived from external information to the model space that concerns itself with recognizing visual features. To this end, we introduce a set of "phantom" object classes whose coordinates live in both the semantic space and the model space. Serving as bases in a dictionary, they can be optimized from labeled data such that the synthesized real object classifiers achieve optimal discriminative performance. We demonstrate superior accuracy of our approach over the state of the art on four benchmark datasets for zero-shot learning, including the full ImageNet Fall 2011 dataset with more than 20,000 unseen classes.},
archivePrefix = {arXiv},
arxivId = {1603.00550},
author = {Changpinyo, Soravit and Chao, Wei-Lun and Gong, Boqing and Sha, Fei},
doi = {10.1109/CVPR.2016.575},
eprint = {1603.00550},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/changpinyoSYNC.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
pmid = {28708560},
title = {{Synthesized Classifiers for Zero-Shot Learning}},
url = {http://arxiv.org/abs/1603.00550},
year = {2016}
}
@article{Gong2013,
abstract = {Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with approximate top-{\$}k{\$} ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10{\%}, obtaining the best reported performance in the literature.},
archivePrefix = {arXiv},
arxivId = {1312.4894},
author = {Gong, Yunchao and Jia, Yangqing and Leung, Thomas and Toshev, Alexander and Ioffe, Sergey},
eprint = {1312.4894},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GongDeepmultirank.pdf:pdf},
pages = {1--9},
pmid = {2094121},
title = {{Deep Convolutional Ranking for Multilabel Image Annotation}},
url = {http://arxiv.org/abs/1312.4894},
year = {2013}
}
@article{Bojarski2016,
abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
archivePrefix = {arXiv},
arxivId = {1604.07316},
author = {Bojarski, Mariusz and {Del Testa}, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
doi = {10.1109/IJCNN.2005.1556090},
eprint = {1604.07316},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BojarskiCars.pdf:pdf},
isbn = {9781450351867},
issn = {1938-7228},
pages = {1--9},
pmid = {19036266},
title = {{End to End Learning for Self-Driving Cars}},
url = {http://arxiv.org/abs/1604.07316},
year = {2016}
}
@article{Huang2016,
abstract = {Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.},
archivePrefix = {arXiv},
arxivId = {1610.08904},
author = {Huang, Chen and Loy, Chen Change and Tang, Xiaoou},
eprint = {1610.08904},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HuangDeepemb.pdf:pdf},
issn = {10495258},
number = {Nips},
pages = {1--9},
title = {{Local Similarity-Aware Deep Feature Embedding}},
url = {http://arxiv.org/abs/1610.08904},
volume = {1},
year = {2016}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RussakovskyIMGNET.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Srikanth2005,
abstract = {Automatic image annotation is the task of automatically assigning words to an image that describe the content of the image. Machine learning approaches have been explored to model the association between words and images from an annotated set of images and generate annotations for a test image. The paper proposes methods to use a hierarchy defined on the annotation words derived from a text ontology to improve automatic image annotation and retrieval. Specifically, the hierarchy is used in the context of generating a visual vocabulary for representing images and as a framework for the proposed hierarchical classification approach for automatic image annotation. The effect of using the hierarchy in generating the visual vocabulary is demonstrated by improvements in the annotation performance of translation models. In addition to performance improvements, hierarchical classification approaches yield well to constructing multimedia ontologies. [ABSTRACT FROM AUTHOR]},
author = {Srikanth, Munirathnam and Varner, Joshua and Bowden, Mitchell and Moldovan, Dan},
doi = {10.1145/1076034.1076128},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SrikanthOntologies.pdf:pdf},
isbn = {1595930345},
issn = {01635840},
journal = {SIGIR Forum},
keywords = {ANNOTATIONS,Automatic image annotation,LEXICOLOGY,MACHINE learning,MACHINE theory,MULTIMEDIA systems,hierarchical classification models,image retrieval,ontologies,translation models},
pages = {552--558},
title = {{Exploiting Ontologies for Automatic Image Annotation.}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=lxh{\&}AN=19054808{\&}site=ehost-live},
year = {2005}
}
@article{Pathak2014,
abstract = {Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.},
archivePrefix = {arXiv},
arxivId = {1412.7144},
author = {Pathak, Deepak and Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1103/PhysRevA.92.013626},
eprint = {1412.7144},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PathakMIL.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {1050-2947},
number = {1},
pages = {1--4},
pmid = {823878},
title = {{Fully Convolutional Multi-Class Multiple Instance Learning}},
url = {http://arxiv.org/abs/1412.7144},
year = {2014}
}
@article{Guo2016,
abstract = {Deep learning algorithms are a subset of the machine learning algorithms, which aim at discovering multiple levels of distributed representations. Recently, numerous deep learning algorithms have been proposed to solve traditional artificial intelligence problems. This work aims to review the state-of-the-art in deep learning algorithms in computer vision by highlighting the contributions and challenges from over 210 recent research papers. It first gives an overview of various deep learning approaches and their recent developments, and then briefly describes their applications in diverse vision tasks, such as image classification, object detection, image retrieval, semantic segmentation and human pose estimation. Finally, the paper summarizes the future trends and challenges in designing and training deep neural networks.},
author = {Guo, Yanming and Liu, Yu and Oerlemans, Ard and Lao, Songyang and Wu, Song and Lew, Michael S.},
doi = {10.1016/j.neucom.2015.09.116},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GuoVisualunderstand.pdf:pdf},
isbn = {0925-2312},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Applications,Challenges,Computer vision,Deep learning,Developments,Trends},
pages = {27--48},
title = {{Deep learning for visual understanding: A review}},
volume = {187},
year = {2016}
}
@article{Robbins1951,
author = {Robbins, Herbert and Monro, Suttton},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {400--407},
title = {{A Stochastic Approximation Method}},
volume = {22},
year = {1951}
}
@article{Akata2015,
abstract = {Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with fine-grained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results.},
archivePrefix = {arXiv},
arxivId = {1409.8403},
author = {Akata, Zeynep and Reed, Scott and Walter, Daniel and Lee, Honglak and Schiele, Bernt},
doi = {10.1109/CVPR.2015.7298911},
eprint = {1409.8403},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Akata2016.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {1},
pages = {2927--2936},
title = {{Evaluation of output embeddings for fine-grained image classification}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Li2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Li, Xirong and Liao, Shuai and Lan, Weiyu and Du, Xiaoyong and Yang, Gang},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR 15},
doi = {10.1145/2766462.2767773},
eprint = {arXiv:1011.1669v3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/LiHIERSE.pdf:pdf},
isbn = {9781450336215},
issn = {14678330},
pages = {879--882},
pmid = {15664853},
title = {{Zero-shot Image Tagging by Hierarchical Semantic Embedding}},
url = {http://dl.acm.org/citation.cfm?doid=2766462.2767773},
year = {2015}
}
@book{Ungar2009,
author = {Ungar, Abraham},
isbn = {1598298224},
publisher = {Morgan and Claypool Publishers},
title = {{A Gyrovector Space Approach to Hyperbolic Geometry (Synthesis Lectures on Mathematics and Statistics)}},
year = {2009}
}
@article{Zhang2015a,
abstract = {Zero-shot recognition (ZSR) deals with the problem of predicting class labels for target domain instances based on source domain side information (e.g. attributes) of unseen classes. We formulate ZSR as a binary prediction problem. Our resulting classifier is class-independent. It takes an arbitrary pair of source and target domain instances as input and predicts whether or not they come from the same class, i.e. whether there is a match. We model the posterior probability of a match since it is a sufficient statistic and propose a latent probabilistic model in this context. We develop a joint discriminative learning framework based on dictionary learning to jointly learn the parameters of our model for both domains, which ultimately leads to our class-independent classifier. Many of the existing embedding methods can be viewed as special cases of our probabilistic model. On ZSR our method shows 4.90$\backslash${\%} improvement over the state-of-the-art in accuracy averaged across four benchmark datasets. We also adapt ZSR method for zero-shot retrieval and show 22.45$\backslash${\%} improvement accordingly in mean average precision (mAP).},
archivePrefix = {arXiv},
arxivId = {1511.04512},
author = {Zhang, Ziming and Saligrama, Venkatesh},
doi = {10.1109/CVPR.2016.649},
eprint = {1511.04512},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Zhang16.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {978-1-4673-8391-2},
title = {{Zero-Shot Learning via Joint Latent Similarity Embedding}},
url = {http://arxiv.org/abs/1511.04512},
year = {2015}
}
@article{Grosky2002,
abstract = {We present the results of our work that seek to negotiate the gap between low-level features and high-level concepts in the domain of web document retrieval. This work concerns a technique, called the latent semantic indexing (LSI), which has been used for textual information retrieval for many years. In this environment, LSI determines clusters of co-occurring keywords so that a query which uses a particular keyword can then retrieve documents perhaps not containing this keyword, but containing other keywords from the same cluster. In this paper, we examine the use of this technique for content-based web document retrieval, using both keywords and image features to represent the documents. Two different approaches to image feature representation, namely, color histograms and color anglograms, are adopted and evaluated. Experimental results show that LSI, together with both textual and visual features, is able to extract the underlying semantic structure of web documents, thus helping to improve the retrieval performance significantly, even when querying is done using only keywords.},
author = {Grosky, W.I.},
doi = {10.1109/TMM.2002.1017733},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZhaoSemGap.pdf:pdf},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {but containing other keywords,can then retrieve documents,content-based,from the same cluster,image features,in,keyword,of this technique for,perhaps not containing this,this paper,using both keywords and,we examine the use,web document retrieval},
number = {2},
pages = {189--200},
title = {{Narrowing the semantic gap - improved text-based web document retrieval using visual features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1017733},
volume = {4},
year = {2002}
}
@article{Yang2018,
abstract = {Multi-label classification (MLC) is an important learning problem that expects the learning algorithm to take the hidden correlation of the labels into account. Extracting the hidden correlation is generally a challenging task. In this work, we propose a novel deep learning framework to better extract the hidden correlation with the help of the memory structure within recurrent neural networks. The memory stores the temporary guesses on the labels and effectively allows the framework to rethink about the goodness and correlation of the guesses before making the final prediction. Furthermore, the rethinking process makes it easy to adapt to different evaluation criterion to match real-world application needs. Experimental results across many real-world data sets justify that the rethinking process indeed improves MLC performance across different evaluation criteria and leads to superior performance over state-of-the-art MLC algorithms.},
archivePrefix = {arXiv},
arxivId = {1802.01697},
author = {Yang, Yao-Yuan and Lin, Yi-An and Chu, Hong-Min and Lin, Hsuan-Tien},
eprint = {1802.01697},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YangRethinking.pdf:pdf},
title = {{Deep Learning with a Rethinking Structure for Multi-label Classification}},
url = {http://arxiv.org/abs/1802.01697},
year = {2018}
}
@article{Wang2018,
abstract = {Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.},
archivePrefix = {arXiv},
arxivId = {1802.03601},
author = {Wang, Mei and Deng, Weihong},
doi = {10.1016/j.neucom.2018.05.083},
eprint = {1802.03601},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DeepDomainAdapt.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Computer vision applications,Deep domain adaptation,Deep networks,Transfer learning},
pages = {135--153},
title = {{Deep visual domain adaptation: A survey}},
volume = {312},
year = {2018}
}
@article{Ontrup2002,
abstract = {We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a "hyperbolic SOM" (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations.},
author = {Ontrup, J{\"{o}}rg and Ritter, Helge},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/OntrupSOM.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 14},
pages = {1417--1424},
title = {{Hyperbolic Self-Organizing Maps for Semantic Navigation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.3062{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@article{Ungar2013,
abstract = {Barycentric coordinates are commonly used in Euclidean geometry. The adaptation of barycentric coordinates for use in hyperbolic geometry gives rise to hyperbolic barycentric coordinates, known as gyrobarycentric coordinates. The aim of this article is to present the road from Einstein's velocity addition law of relativistically admissible velocities to hyperbolic barycentric coordinates along with applications.},
archivePrefix = {arXiv},
arxivId = {1304.0205},
author = {Ungar, Abraham Albert},
eprint = {1304.0205},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Gyro.pdf:pdf},
number = {1},
pages = {1--65},
title = {{An Introduction to Hyperbolic Barycentric Coordinates and their Applications}},
url = {http://arxiv.org/abs/1304.0205},
year = {2013}
}
@article{DiCarlo2012,
author = {DiCarlo, James J. and Yoccolan, Davide and Rust, Nicole C.},
doi = {10.1016/j.neuron.2012.01.010.How},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DiCarlo2013.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
number = {3},
pages = {415--434},
pmid = {22325196},
title = {{How does the brain solve visual object recognition?}},
volume = {73},
year = {2012}
}
@article{Bucher2016,
abstract = {This paper addresses the task of zero-shot image classification. The key contribution of the proposed approach is to control the semantic embedding of images -- one of the main ingredients of zero-shot learning -- by formulating it as a metric learning problem. The optimized empirical criterion associates two types of sub-task constraints: metric discriminating capacity and accurate attribute prediction. This results in a novel expression of zero-shot learning not requiring the notion of class in the training phase: only pairs of image/attributes, augmented with a consistency indicator, are given as ground truth. At test time, the learned model can predict the consistency of a test image with a given set of attributes , allowing flexible ways to produce recognition inferences. Despite its simplicity, the proposed approach gives state-of-the-art results on four challenging datasets used for zero-shot recognition evaluation.},
archivePrefix = {arXiv},
arxivId = {1607.08085},
author = {Bucher, Maxime and Herbin, St{\'{e}}phane and Jurie, Fr{\'{e}}d{\'{e}}ric},
doi = {10.1007/978-3-319-46454-1_44},
eprint = {1607.08085},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BucherZSL.pdf:pdf},
isbn = {9783319464534},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Attributes,Semantic embedding,Zero-shot learning},
pages = {730--746},
pmid = {4520227},
title = {{Improving semantic embedding consistency by metric learning for zero-shot classiffication}},
volume = {9909 LNCS},
year = {2016}
}
@article{You2016,
abstract = {Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.},
archivePrefix = {arXiv},
arxivId = {1603.03925},
author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
doi = {10.1109/CVPR.2016.503},
eprint = {1603.03925},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YouCaptionatt.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
title = {{Image Captioning with Semantic Attention}},
url = {http://arxiv.org/abs/1603.03925},
year = {2016}
}
@article{Hinton1986,
abstract = {(From the chapter) first section of this chapter stresses some of the virtues of distributed representations / second section considers the efficiency of distributed representations, and shows clearly why distributed representations can be better than local ones for certain classes of problems / final section discusses some difficult issues which are often avoided by advocates of distributed representations, such as the representation of constituent structure and the sequential focusing of processing effort on different aspects of a structure object (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Hinton, G E and McClelland, J L and Rumelhart, D E},
doi = {10.1146/annurev-psych-120710-100344},
eprint = {15334406},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HintonDistributed.pdf:pdf},
isbn = {0-262-68053-X},
issn = {1534-7362},
journal = {Parallel Distributed Processing},
pages = {77--109},
pmid = {21943171},
title = {{Distributed representations}},
year = {1986}
}
@book{Scholkopf2002,
author = {Sch{\"{o}}lkopf, B. and Bach, F.},
publisher = {MIT Press},
title = {{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
year = {2002}
}
@article{Kodirov2015,
abstract = {Zero-shot learning (ZSL) can be considered as a special case of transfer learning where the source and target do-mains have different tasks/label spaces and the target do-main is unlabelled, providing little guidance for the knowl-edge transfer. A ZSL method typically assumes that the two domains share a common semantic representation space, where a visual feature vector extracted from an image/video can be projected/embedded using a projection function. Ex-isting approaches learn the projection function from the source domain and apply it without adaptation to the target domain. They are thus based on naive knowledge transfer and the learned projections are prone to the domain shift problem. In this paper a novel ZSL method is proposed based on unsupervised domain adaptation. Specifically, we formulate a novel regularised sparse coding framework which uses the target domain class labels' projections in the semantic space to regularise the learned target domain pro-jection thus effectively overcoming the projection domain shift problem. Extensive experiments on four object and ac-tion recognition benchmark datasets show that the proposed ZSL method significantly outperforms the state-of-the-arts.},
author = {Kodirov, Elyor and Xiang, Tao and Fu, Zhenyong and Gong, Shaogang},
doi = {10.1109/ICCV.2015.282},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/KodirovUS.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {IEEE International Conference on Computer Vision (ICCV)},
pages = {2452--2460},
title = {{Unsupervised Domain Adaptation for Zero-Shot Learning}},
year = {2015}
}
@article{Szegedy2015a,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SzegedyInception.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{Cadieu2014,
abstract = {The primate visual system achieves remarkable visual object recognition performance even in brief presentations and under changes to object exemplar, geometric transformations, and background variation (a.k.a. core visual object recognition). This remarkable performance is mediated by the representation formed in inferior temporal (IT) cortex. In parallel, recent advances in machine learning have led to ever higher performing models of object recognition using artificial deep neural networks (DNNs). It remains unclear, however, whether the representational performance of DNNs rivals that of the brain. To accurately produce such a comparison, a major difficulty has been a unifying metric that accounts for experimental limitations such as the amount of noise, the number of neural recording sites, and the number trials, and computational limitations such as the complexity of the decoding classifier and the number of classifier training examples. In this work we perform a direct comparison that corrects for these experimental limitations and computational considerations. As part of our methodology, we propose an extension of "kernel analysis" that measures the generalization accuracy as a function of representational complexity. Our evaluations show that, unlike previous bio-inspired models, the latest DNNs rival the representational performance of IT cortex on this visual object recognition task. Furthermore, we show that models that perform well on measures of representational performance also perform well on measures of representational similarity to IT and on measures of predicting individual IT multi-unit responses. Whether these DNNs rely on computational mechanisms similar to the primate visual system is yet to be determined, but, unlike all previous bio-inspired models, that possibility cannot be ruled out merely on representational performance grounds.},
archivePrefix = {arXiv},
arxivId = {1406.3284},
author = {Cadieu, Charles F. and Hong, Ha and Yamins, Daniel L K and Pinto, Nicolas and Ardila, Diego and Solomon, Ethan A. and Majaj, Najib J. and DiCarlo, James J.},
doi = {10.1371/journal.pcbi.1003963},
eprint = {1406.3284},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/cadieu2014.pdf:pdf},
isbn = {1553-7358},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {12},
pmid = {25521294},
title = {{Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition}},
volume = {10},
year = {2014}
}
@article{Rohrbach2010,
abstract = {Remarkable performance has been reported to recognize single object classes. Scalability to large numbers of classes however remains an important challenge for today's recognition methods. Several authors have promoted knowledge transfer between classes as a key ingredient to address this challenge. However, in previous work the decision which knowledge to transfer has required either manual supervision or at least a few training examples limiting the scalability of these approaches. In this work we explicitly address the question of how to automatically decide which information to transfer between classes without the need of any human intervention. For this we tap into linguistic knowledge bases to provide the semantic link between sources (what) and targets (where) of knowledge transfer. We provide a rigorous experimental evaluation of different knowledge bases and state-of-the-art techniques from Natural Language Processing which goes far beyond the limited use of language in related work. We also give insights into the applicability (why) of different knowledge sources and similarity measures for knowledge transfer.},
author = {Rohrbach, Marcus and Stark, Michael and Szarvas, G. and Gurevych, I. and Schiele, B.},
doi = {10.1109/CVPR.2010.5540121},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RohrbachWHWAW.pdf:pdf},
isbn = {978-1-4244-6984-0},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
number = {c},
pages = {910--917},
pmid = {18573770},
title = {{What helps where–and why? semantic relatedness for knowledge transfer}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5540121},
volume = {1},
year = {2010}
}
@article{Nickel2017,
abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar$\backslash$'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar$\backslash$'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
archivePrefix = {arXiv},
arxivId = {1705.08039},
author = {Nickel, Maximilian and Kiela, Douwe},
eprint = {1705.08039},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PoincareEmb.pdf:pdf},
issn = {10495258},
title = {{Poincar$\backslash$'e Embeddings for Learning Hierarchical Representations}},
url = {http://arxiv.org/abs/1705.08039},
year = {2017}
}
@article{Rohrbach2013,
abstract = {Category models for objects or activities typically rely on supervised learning requiring sufficiently large training sets. Transferring knowledge from known cat-egories to novel classes with no or only a few labels is far less researched even though it is a common scenario. In this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three techniques. First, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expert-specified information, e.g., by a mid-level layer of semantic attributes. Second, we exploit the manifold structure of novel classes. More specifically we adapt a graph-based learning algorithm – so far only used for semi-supervised learning – to zero-shot and few-shot learning. Third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object-or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications, namely on Animals with Attributes and ImageNet for image classification and on MPII Composites for ac-tivity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets.},
author = {Rohrbach, Marcus and Ebert, Sandra and Schiele, Bernt},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RohrbachTransductive.pdf:pdf},
issn = {10495258},
journal = {Advances in neural information {\ldots}},
pages = {1--9},
title = {{Transfer learning in a transductive setting}},
url = {http://papers.nips.cc/paper/5209-transfer-learning-in-a-transductive-setting},
year = {2013}
}
@article{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their pre- dictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18{\%} across thou- sands of novel labels never seen by the visual model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5650v3},
author = {Frome, Andrea and Corrado, Gs and Shlens, Jonathon},
eprint = {arXiv:1312.5650v3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FromeDevise.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural {\ldots}},
pages = {1--11},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@article{Redmon,
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RedmonYOLO9000.pdf:pdf},
pages = {7263--7271},
title = {{Better , Faster , Stronger}}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HeResNet.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Krioukov2010,
abstract = {We develop a geometric framework to study the structure and function of complex networks. We assume that hyperbolic geometry underlies these networks, and we show that with this assumption, heterogeneous degree distributions and strong clustering in complex networks emerge naturally as simple reflections of the negative curvature and metric property of the underlying hyperbolic geometry. Conversely, we show that if a network has some metric structure, and if the network degree distribution is heterogeneous, then the network has an effective hyperbolic geometry underneath. We then establish a mapping between our geometric framework and statistical mechanics of complex networks. This mapping interprets edges in a network as noninteracting fermions whose energies are hyperbolic distances between nodes, while the auxiliary fields coupled to edges are linear functions of these energies or distances. The geometric network ensemble subsumes the standard configuration model and classical random graphs as two limiting cases with degenerate geometric structures. Finally, we show that targeted transport processes without global topology knowledge, made possible by our geometric framework, are maximally efficient, according to all efficiency measures, in networks with strongest heterogeneity and clustering, and that this efficiency is remarkably robust with respect to even catastrophic disturbances and damages to the network structure.},
archivePrefix = {arXiv},
arxivId = {1006.5169},
author = {Krioukov, Dmitri and Papadopoulos, Fragkiskos and Kitsak, Maksim and Vahdat, Amin and Bogu{\~{n}}{\'{a}}, Mari{\'{a}}n},
doi = {10.1103/PhysRevE.82.036106},
eprint = {1006.5169},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HBnetworks.pdf:pdf},
isbn = {0002492490000},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {3},
pages = {1--18},
pmid = {21230138},
title = {{Hyperbolic geometry of complex networks}},
volume = {82},
year = {2010}
}
@article{Xian2017,
abstract = {Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it.},
archivePrefix = {arXiv},
arxivId = {1703.04394},
author = {Xian, Yongqin and Schiele, Bernt and Akata, Zeynep},
doi = {10.1109/CVPR.2017.328},
eprint = {1703.04394},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/XianGBU.pdf:pdf},
isbn = {978-1-5386-0457-1},
title = {{Zero-Shot Learning - The Good, the Bad and the Ugly}},
url = {http://arxiv.org/abs/1703.04394},
year = {2017}
}
@article{Hodgkin1990,
abstract = {This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkinet al., 1952,J. Physiol.116, 424-448; Hodgkin and Huxley 1952,J. Physiol.116, 449-566). Its general object is to discuss the results of the preceding papers (Section 1), to put them into mathematical form (Section 2) and to whow that they will account for conduction and excitation in quantitative terms (Sections 3-6).},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hodgkin, A. L. and Huxley, A. F.},
doi = {10.1007/BF02459568},
eprint = {NIHMS150003},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HodgingHuxley.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$n1097-6256 (Linking)},
issn = {00928240},
journal = {Bulletin of Mathematical Biology},
number = {1-2},
pages = {25--71},
pmid = {2185861},
title = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
volume = {52},
year = {1990}
}
@article{Romera-Paredes2017,
abstract = {Zero-shot learning consists in learning how to recognize new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalization error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to 17{\%}17$\backslash${\%}.},
author = {Romera-Paredes, Bernardino and Torr, Philip H. S.},
doi = {10.1007/978-3-319-50077-5_2},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RomeraSimple.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
pages = {11--30},
title = {{An Embarrassingly Simple Approach to Zero-Shot Learning}},
url = {http://link.springer.com/10.1007/978-3-319-50077-5{\_}2},
volume = {37},
year = {2017}
}
@article{Gulcehre2018,
abstract = {We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact.},
archivePrefix = {arXiv},
arxivId = {1805.09786},
author = {Gulcehre, Caglar and Denil, Misha and Malinowski, Mateusz and Razavi, Ali and Pascanu, Razvan and Hermann, Karl Moritz and Battaglia, Peter and Bapst, Victor and Raposo, David and Santoro, Adam and de Freitas, Nando},
eprint = {1805.09786},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GulcehreHyper.pdf:pdf},
pages = {1--15},
title = {{Hyperbolic Attention Networks}},
url = {http://arxiv.org/abs/1805.09786},
volume = {i},
year = {2018}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PenningtonGlove.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1532--1543},
pmid = {1710995},
title = {{Glove: Global Vectors for Word Representation}},
url = {http://aclweb.org/anthology/D14-1162},
year = {2014}
}
@article{Read2014,
abstract = {In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature},
archivePrefix = {arXiv},
arxivId = {1502.05988},
author = {Read, Jesse and Perez-Cruz, Fernando},
eprint = {1502.05988},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ReadDLmultilabel.pdf:pdf},
pages = {1--8},
title = {{Deep Learning for Multi-label Classification}},
url = {http://arxiv.org/abs/1502.05988},
year = {2014}
}
@inproceedings{Zeiler2014,
author = {Zeiler, Matthew and Fergus, Rob},
booktitle = {European Conference on Computer Vision},
pages = {818--833},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2014}
}
@article{Zweig2007,
abstract = {We investigated the computational properties of natural object hierarchy in the context of constellation object class models, and its utility for object class recognition. We first observed an interesting computational property of the object hierarchy: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels (e.g., closed-frame vehicles or vehicles) exhibit higher recall but lower precision when compared with the class specific level (e.g., bus). These inherent differences suggest that combining object classifiers from different hierarchical levels into a single classifier may improve classification, as it appears like these models capture different aspects of the object. We describe a method to combine these classifiers, and analyze the conditions under which improvement can be guaranteed. When given a small sample of a new object class, we describe a method to transfer knowledge across the tree hierarchy, between related objects. Finally, we describe extensive experiments using object hierarchies obtained from publicly available datasets, and show that the combined classifiers significantly improve recognition results.},
author = {Zweig, Alon and Weinshall, Daphna},
doi = {10.1109/ICCV.2007.4409064},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZweigHierarchies.pdf:pdf},
isbn = {978-1-4244-1630-1},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {October},
title = {{Exploiting object hierarchy: Combining models from different category levels}},
year = {2007}
}
@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E},
doi = {10.1145/584091.584093},
eprint = {9411012},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Shannon.pdf:pdf},
isbn = {0252725484},
issn = {07246811},
journal = {The Bell System Technical Journal},
number = {July 1928},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A mathematical theory of communication}},
url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
volume = {27},
year = {1948}
}
@article{Dodge2017,
abstract = {Deep neural networks (DNNs) achieve excellent performance on standard classification tasks. However, under image quality distortions such as blur and noise, classification accuracy becomes poor. In this work, we compare the performance of DNNs with human subjects on distorted images. We show that, although DNNs perform better than or on par with humans on good quality images, DNN performance is still much lower than human performance on distorted images. We additionally find that there is little correlation in errors between DNNs and human subjects. This could be an indication that the internal representation of images are different between DNNs and the human visual system. These comparisons with human performance could be used to guide future development of more robust DNNs.},
archivePrefix = {arXiv},
arxivId = {1705.02498},
author = {Dodge, Samuel and Karam, Lina},
doi = {10.1109/ICCCN.2017.8038465},
eprint = {1705.02498},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DodgeHUMAN.pdf:pdf},
isbn = {978-1-5090-2991-4},
title = {{A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions}},
url = {http://arxiv.org/abs/1705.02498},
year = {2017}
}
@book{Ohl2015,
author = {Ohl, Michael},
doi = {10.1007/978-3-540-33761-4},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Ohl2014taxonomyHandbookofPaleoanthropology.pdf:pdf},
isbn = {9783540337614},
number = {July},
publisher = {Springer-Verlag Berlin},
title = {{Handbook of Paleoanthropology -Principles of Taxonomy and Classification}},
year = {2015}
}
@article{Ba2015,
abstract = {One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo- attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end us- ing the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.},
archivePrefix = {arXiv},
arxivId = {1506.00511},
author = {Ba, Jimmy Lei and Swersky, Kevin and Fidler, Sanja and Salakhutdinov, Ruslan},
doi = {10.1109/ICCV.2015.483},
eprint = {1506.00511},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BaZSL.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {4247--4255},
title = {{Predicting deep zero-shot convolutional neural networks using textual descriptions}},
volume = {2015 Inter},
year = {2015}
}
@article{Girshick2012,
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GirschickSemseg.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
pages = {2--9},
pmid = {26656583},
title = {{Girshick{\_}Rich{\_}Feature{\_}Hierarchies{\_}2014{\_}CVPR{\_}paper}},
year = {2012}
}
@article{Ji2017,
abstract = {Zero-Shot Learning (ZSL) aims at classifying previously unseen class samples and has gained its popularity in applications where samples of some categories are scarce for training. The basic idea to address this issue is transferring knowledge from the seen classes to the unseen classes through mapping the visual feature to an embedding space spanned by class semantic information. The class semantic information can be obtained from human-labeled attributes or text corpus in an unsupervised fashion. Therefore, the embedding function from visual space to the embedding space is extremely important. However, the existing embedding approaches to ZSL mainly focus on aligning pairwise semantic consistency from heterogeneous spaces but ignore the intrinsic structure of the locally homogeneous isomorph. In order to preserve the locally visual structure in the embedding process, this paper proposes a Manifold regularized Cross-Modal Embedding (MCME) approach for ZSL by formulating the manifold constraint for intrinsic structure of the visual features as well as aligning pairwise consistency. The linear, closed-form solution makes MCME efficient to compute. Furthermore, rather than applying the embedding function learned from the seen classes directly, we also propose a new domain adaptation strategy to overcome the domain-shift problem during the knowledge transfer process. The MCME with the domain adaptation method is called MCME-DA. Extensive experiments on the benchmark datasets of AwA and CUB validate the superiority and promise of MCME and MCME-DA.},
author = {Ji, Zhong and Yu, Yunlong and Pang, Yanwei and Guo, Jichang and Zhang, Zhongfei},
doi = {10.1016/j.ins.2016.10.025},
file = {:home/hege/Downloads/JiXMODAL.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Cross-modal embedding,Domain adaptation,Image classification,Manifold,Zero-shot learning},
pages = {48--58},
publisher = {Elsevier Inc.},
title = {{Manifold regularized cross-modal embedding for zero-shot learning}},
volume = {378},
year = {2017}
}
@article{Simonyan2014,
author = {Simonyan, K. and Zisserman, A.},
journal = {CoRR},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
volume = {abs/1409.1},
year = {2014}
}
@article{Joliceur1984,
author = {Joliceur, P and Gluck, M A and Kosslyn, S M},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/JolicoeurPictures.pdf:pdf},
journal = {Cognitive Psychology},
pages = {243--275},
title = {{Pictures and Naming: Making the Connection}},
volume = {16},
year = {1984}
}
@article{Fu2015,
abstract = {Most existing zero-shot learning approaches exploit transfer learning via an intermediate-level semantic representation shared between an annotated auxiliary dataset and a target dataset with different classes and no annotation. A projection from a low-level feature space to the semantic representation space is learned from the auxiliary dataset and is applied without adaptation to the target dataset. In this paper we identify two inherent limitations with these approaches. First, due to having disjoint and potentially unrelated classes, the projection functions learned from the auxiliary dataset/domain are biased when applied directly to the target dataset/domain. We call this problem the projection domain shift problem and propose a novel framework, transductive multi-view embedding, to solve it. The second limitation is the prototype sparsity problem which refers to the fact that for each target class, only a single prototype is available for zero-shot learning given a semantic representation. To overcome this problem, a novel heterogeneous multi-view hypergraph label propagation method is formulated for zero-shot learning in the transductive embedding space. It effectively exploits the complementary information offered by different semantic representations and takes advantage of the manifold structures of multiple representation spaces in a coherent manner. We demonstrate through extensive experiments that the proposed approach (1) rectifies the projection shift between the auxiliary and target domains, (2) exploits the complementarity of multiple semantic representations, (3) significantly outperforms existing methods for both zero-shot and N-shot recognition on three image and video benchmark datasets, and (4) enables novel cross-view annotation tasks.},
archivePrefix = {arXiv},
arxivId = {1501.04560},
author = {Fu, Yanwei and Hospedales, Timothy M. and Xiang, Tao and Gong, Shaogang},
doi = {10.1109/TPAMI.2015.2408354},
eprint = {1501.04560},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Fu2014.pdf:pdf},
isbn = {978-3-319-10604-5},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Transducitve learning,heterogeneous hypergraph,multi-view Learning,transfer Learning,zero-shot Learning},
number = {11},
pages = {2332--2345},
pmid = {26440271},
title = {{Transductive Multi-View Zero-Shot Learning}},
volume = {37},
year = {2015}
}
@article{Kobatake1994,
author = {Kobatake, E. and Tanaka, K.},
journal = {Journal of Neurophysiology},
pages = {856--867},
title = {{Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex}},
volume = {71},
year = {1994}
}
@article{Lecun2015,
author = {Lecun, Yann and Hinton, Geoffrey E. and Bengio, Yoshua},
journal = {Nature},
pages = {436--444},
title = {{Deep Learning}},
volume = {521},
year = {2015}
}
@article{Walter2006,
author = {Walter, Ja and Webling, D and Essig, Kai},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/WalterHY.pdf:pdf},
isbn = {1595934464},
journal = {Acm Sigkdd},
title = {{Interactive hyperbolic image browsing-towards an integrated multimedia navigator}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Interactive+Hyperbolic+Image+Browsing+?+Towards+an+Integrated+Multimedia+Navigator{\#}0},
year = {2006}
}
@article{Wang2016,
abstract = {While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model},
archivePrefix = {arXiv},
arxivId = {1604.04573},
author = {Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
doi = {10.1109/CVPR.2016.251},
eprint = {1604.04573},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/cnnrnn.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2285--2294},
pmid = {10463930},
title = {{CNN-RNN: A Unified Framework for Multi-label Image Classification}},
url = {http://ieeexplore.ieee.org/document/7780620/},
year = {2016}
}
@article{Wang2015,
abstract = {Recent advances in deep learning have led to significant progress in the computer vision field, especially for visual object recognition tasks. The features useful for object classification are learned by feed-forward deep convolutional neural networks (CNNs) automatically, and they are shown to be able to predict and decode neural representations in the ventral visual pathway of humans and monkeys. However, despite the huge amount of work on optimizing CNNs, there has not been much research focused on linking CNNs with guiding principles from the human visual cortex. In this work, we propose a network optimization strategy inspired by both of the developmental trajectory of children's visual object recognition capabilities, and Bar (2003), who hypothesized that basic level information is carried in the fast magnocellular pathway through the prefrontal cortex (PFC) and then projected back to inferior temporal cortex (IT), where subordinate level categorization is achieved. We instantiate this idea by training a deep CNN to perform basic level object categorization first, and then train it on subordinate level categorization. We apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC 2012 dataset and show that the top-5 accuracy increases from 80.13{\%} to 82.14{\%}, demonstrating the effectiveness of the method. We also show that subsequent transfer learning on smaller datasets gives superior results.},
archivePrefix = {arXiv},
arxivId = {1511.04103},
author = {Wang, Panqu and Cottrell, Garrison W.},
doi = {10.1162/jocn_a_00409},
eprint = {1511.04103},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/WangHierarchy.pdf:pdf},
isbn = {9780192880512},
issn = {0898-929X},
pages = {1--13},
pmid = {23647519},
title = {{Basic Level Categorization Facilitates Visual Object Recognition}},
url = {http://arxiv.org/abs/1511.04103},
year = {2015}
}
@article{Barnard2001,
abstract = {We present a statistical model for organizing image collections which integrates semantic information provided by associated text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text andlor image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition. 1.},
author = {Barnard, K. and Forsyth, D.},
doi = {10.1109/ICCV.2001.937654},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BarnardSemantics.pdf:pdf},
isbn = {0-7695-1143-0},
issn = {0-7695-1143-0},
journal = {Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},
pages = {408--415},
title = {{Learning the semantics of words and pictures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=937654},
volume = {2},
year = {2001}
}
@article{Marszaek2007,
abstract = {In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection. We evaluate how our approach influences the classification accuracy and speed on the PASCAL VOC challenge 2006 dataset, a set of challenging real-world images. We also demonstrate additional features that become available to object recognition due to the extension with semantic inference tools - we can classify high-level categories, such as animals, and we can train part detectors, for example a window detector, by pure inference in the semantic network.},
author = {Marsza{\l}ek, Marcin and Schmid, Cordelia},
doi = {10.1109/CVPR.2007.383272},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MarszalekHierarchies.pdf:pdf},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
title = {{Semantic hierarchies for visual object Recognition}},
year = {2007}
}
@article{Baltruschat2018,
abstract = {The increased availability of X-ray image archives (e.g. the ChestX-ray14 dataset from the NIH Clinical Center) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolutions of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we evaluate the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. We observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-50, integrating non-image data yields the best overall results.},
archivePrefix = {arXiv},
arxivId = {1803.02315},
author = {Baltruschat, Ivo M. and Nickisch, Hannes and Grass, Michael and Knopp, Tobias and Saalbach, Axel},
eprint = {1803.02315},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/XRAY.pdf:pdf},
keywords = {chest x-ray,chestx-ray14,convolutional neural net-,deep learning,transfer learning,works},
number = {c},
title = {{Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification}},
url = {http://arxiv.org/abs/1803.02315},
volume = {14},
year = {2018}
}
@article{Biederman1989,
abstract = {Reports that an article by I. Biederman published in Psychological Review, 1987(Apr), Vol 94(2), 115-117) was inadvertently a duplicate publication. A large portion of this article had previously appeared as unedited conference proceedings in another journal and in an edited book. (The following abstract of this article originally appeared in PA, Vol 74:20898.) The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons, can be derived from contrasts of five readily detectable properties of edges in a two-dimensional image. The detection of these properties is generally invariant over viewing position and image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. (PsycLIT Database Copyright 1993 American Psychological Assn, all rights reserved)},
author = {Biederman, Irving},
doi = {10.1037/0033-295X.96.1.2},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/biederman1987.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471},
journal = {Psychological Review},
number = {1},
pages = {2--2},
title = {{"Recognition-by-components: A theory of human image understanding": Clarification.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.96.1.2},
volume = {96},
year = {1989}
}
@misc{Hinton1986a,
abstract = {Concepts can be represented by distributed patterns of activity in networks of neuron-like units. One advantage of this kind of representation is that it leads to automatic generalization. When the weighjts in the network are changed to incorporate new knowledgwe about one concept, the changes affect the knowledge associated with other concepts that are represented by similar activity patterns. There have been numerous demonstrations of sensible generalization which have depended on the experimenter choosing appropriately similar patterns for diferent concepts. This paper shows how the network can be made to choose the patterns itself when shown a set of propositions that use the concepts. It chooses patterns which make explicit the underlying features that are only implicit in the propositions it is shown.},
archivePrefix = {arXiv},
arxivId = {1601.01280},
author = {Hinton, Geoffrey},
booktitle = {Css},
doi = {10.1109/69.917563},
eprint = {1601.01280},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Hinton1986.pdf:pdf},
isbn = {0-262-68053-X},
issn = {10414347},
pages = {1--12},
pmid = {21943171},
title = {{Learning distributed representations of concepts}},
url = {http://www.cogsci.ucsd.edu/{~}ajyu/Teaching/Cogs202{\_}sp13/Readings/hinton86.pdf},
year = {1986}
}
@article{Hubeld1962,
author = {Hubel, D. H. and Wiesel, T.},
journal = {Journal of Physiology},
number = {160},
pages = {106 -- 154},
title = {{Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex}},
year = {1962}
}
@article{Srivastava2015,
abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1507.06228},
author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/CVPR.2016.90},
eprint = {1507.06228},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SchmidhuberDepth.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10495258},
pages = {1--11},
pmid = {23554596},
title = {{Training Very Deep Networks}},
url = {http://arxiv.org/abs/1507.06228},
year = {2015}
}
@article{Tsai2017,
abstract = {Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.},
archivePrefix = {arXiv},
arxivId = {1703.05908},
author = {Tsai, Yao Hung Hubert and Huang, Liang Kang and Salakhutdinov, Ruslan},
doi = {10.1109/ICCV.2017.386},
eprint = {1703.05908},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/TsaiSEM.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3591--3600},
title = {{Learning Robust Visual-Semantic Embeddings}},
volume = {2017-Octob},
year = {2017}
}
@article{Miller1995,
author = {Miller, G. A.},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
title = {{Wordnet: a lexical database for english.}},
volume = {38},
year = {1995}
}
@article{Hwang2014,
abstract = {We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be represented by a supercategory + sparse combination of attributes, with an additional exclusive regularization to learn discriminative composition.},
archivePrefix = {arXiv},
arxivId = {1411.5879},
author = {Hwang, Sung Ju and Sigal, Leonid},
eprint = {1411.5879},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Hwang2014.pdf:pdf},
isbn = {9781577357070},
issn = {10495258},
keywords = {AAAI Technical Report SS-15-03},
pages = {71--74},
pmid = {1918783},
title = {{A Unified Semantic Embedding: Relating Taxonomies and Attributes}},
url = {http://arxiv.org/abs/1411.5879},
year = {2014}
}
@article{Ganegedara2017,
abstract = {Adaptability is central to autonomy. Intuitively, for high-dimensional learning problems such as navigating based on vision, internal models with higher complexity allow to accurately encode the information available. However, most learning methods rely on models with a fixed structure and complexity. In this paper, we present a self-supervised framework for robots to learn to navigate, without any prior knowledge of the environment, by incrementally building the structure of a deep network as new data becomes available. Our framework captures images from a monocular camera and self labels the images to continuously train and predict actions from a computationally efficient adaptive deep architecture based on Autoencoders (AE), in a self-supervised fashion. The deep architecture, named Reinforced Adaptive Denoising Autoencoders (RA-DAE), uses reinforcement learning to dynamically change the network structure by adding or removing neurons. Experiments were conducted in simulation and real-world indoor and outdoor environments to assess the potential of self-supervised navigation. RA-DAE demonstrates better performance than equivalent non-adaptive deep learning alternatives and can continue to expand its knowledge, trading-off past and present information.},
archivePrefix = {arXiv},
arxivId = {1712.05084},
author = {Ganegedara, Thushan and Ott, Lionel and Ramos, Fabio},
eprint = {1712.05084},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GanegedaraRobots.pdf:pdf},
isbn = {9781510860117},
issn = {14482053},
journal = {Australasian Conference on Robotics and Automation, ACRA},
pages = {114--123},
title = {{Learning to navigate by growing deep networks}},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{Paszke2017,
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
booktitle = {NIPS},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@article{Lei2018,
author = {Lei, Jie and Guo, Zhenyu and Wang, Yang},
doi = {10.1109/CRV.2017.21},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/LeiGuoCoarse.pdf:pdf},
isbn = {9781538628188},
journal = {Proceedings - 2017 14th Conference on Computer and Robot Vision, CRV 2017},
keywords = {convolutional neural networks,image classification,weakly supervised},
pages = {240--247},
title = {{Weakly Supervised Image Classification with Coarse and Fine Labels}},
volume = {2018-Janua},
year = {2018}
}
@article{Zhou2005,
abstract = {Page 1. - with Training Examples Zhi-Hua Zhou 1 De-Chuan Zhan 1 Qiang Yang 2 1 National Key},
author = {Zhou, Zhi-Hua and Zhan, De-Chuan and Yang, Qiang},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZhouSemisuperv.pdf:pdf},
isbn = {978-1-57735-323-2},
journal = {Artificial Intelligence},
keywords = {Machine Learning,Technical Papers},
number = {1},
pages = {675--680},
title = {{Semi-Supervised Learning with Very Few Labeled Training Examples}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-107.pdf},
volume = {22},
year = {2005}
}
@article{Yeh2017,
abstract = {Multi-label classification is a practical yet challenging task in machine learning related fields, since it requires the prediction of more than one label category for each input instance. We propose a novel deep neural networks (DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this task. Aiming at better relating feature and label domain data for improved classification, we uniquely perform joint feature and label embedding by deriving a deep latent space, followed by the introduction of label-correlation sensitive loss function for recovering the predicted label outputs. Our C2AE is achieved by integrating the DNN architectures of canonical correlation analysis and autoencoder, which allows end-to-end learning and prediction with the ability to exploit label dependency. Moreover, our C2AE can be easily extended to address the learning problem with missing labels. Our experiments on multiple datasets with different scales confirm the effectiveness and robustness of our proposed method, which is shown to perform favorably against state-of-the-art methods for multi-label classification.},
archivePrefix = {arXiv},
arxivId = {1707.00418},
author = {Yeh, Chih-Kuan and Wu, Wei-Chieh and Ko, Wei-Jen and Wang, Yu-Chiang Frank},
eprint = {1707.00418},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YehLatentmulti.pdf:pdf},
keywords = {Machine Learning Methods},
pages = {2838--2844},
title = {{Learning Deep Latent Spaces for Multi-Label Classification}},
url = {http://arxiv.org/abs/1707.00418},
year = {2017}
}
@article{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
eprint = {1502.03167},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/glorotDeepSparse.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {AISTATS '11: Proceedings of the 14th International Conference on Artificial Intelligence and Statistics},
pages = {315--323},
pmid = {28788938},
title = {{Deep sparse rectifier neural networks}},
volume = {15},
year = {2011}
}
@article{Chamberlain2017,
abstract = {Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.},
archivePrefix = {arXiv},
arxivId = {1705.10359},
author = {Chamberlain, Benjamin Paul and Clough, James and Deisenroth, Marc Peter},
eprint = {1705.10359},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Neural Embeddings.pdf:pdf},
keywords = {complex networks,geometry,graph embeddings,neural networks},
title = {{Neural Embeddings of Graphs in Hyperbolic Space}},
url = {http://arxiv.org/abs/1705.10359},
year = {2017}
}
@article{JurgenSchmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in
pattern recognition and machine learning. This historical survey compactly summarizes relevant work,
much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth
of their credit assignment paths, which are chains of possibly learnable, causal links between actions
and effects. I review deep supervised learning (also recapitulating the history of backpropagation),
unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short
programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828},
author = {{J{\"{u}}rgen Schmidhuber}},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SchmidhuberHistory.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
pages = {85--117},
title = {{Deep learning in neural networks: An overview}},
url = {https://ac.els-cdn.com/S0893608014002135/1-s2.0-S0893608014002135-main.pdf?{\_}tid=d25460aa-c556-11e7-a97c-00000aacb35f{\&}acdnat=1510236415{\_}e1693fdedfe07935906af07f4120d5af},
volume = {61},
year = {2015}
}
@article{Hillel2007,
author = {Hillel, Aharon Bar and Weinshall, Daphna},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HillelCategorization.pdf:pdf},
journal = {Nips},
pages = {73--80},
title = {{Subordinate class recognition using relational object models}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=Tbn1l9P1220C{\&}oi=fnd{\&}pg=PA73{\&}dq=Subordinate+class+recognition+using+relational+object+models{\&}ots=V2m9Ilrr20{\&}sig=PN0UIGfPs7FGucveI3J0xO7{\_}qI8},
volume = {19},
year = {2007}
}
@article{Shelhamer2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor and Shelhamer, Evan and Darrell, Trevor and Long, Jonathan and Darrell, Trevor and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/LongSegmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2015}
}
@misc{Abadi2015,
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
title = {{TensorFlow: Large-scale machine learning on heterogeneous systems}},
url = {tensorflow.org},
year = {2015}
}
@article{Sharma2015,
abstract = {This paper proposes a learning-based approach to scene parsing inspired by the deep Recursive Context Propagation Network (RCPN). RCPN is a deep feed-forward neural network that utilizes the contextual information from the entire image, through bottom-up followed by top-down context propagation via random binary parse trees. This improves the feature representation of every super-pixel in the image for better classification into semantic categories. We analyze RCPN and propose two novel contributions to further improve the model. We first analyze the learning of RCPN parameters and discover the presence of bypass error paths in the computation graph of RCPN that can hinder contextual propagation. We propose to tackle this problem by including the classification loss of the internal nodes of the random parse trees in the original RCPN loss function. Secondly, we use an MRF on the parse tree nodes to model the hierarchical dependency present in the output. Both modifications provide performance boosts over the original RCPN and the new system achieves state-of-the-art performance on Stanford Background, SIFT-Flow and Daimler urban datasets.},
archivePrefix = {arXiv},
arxivId = {1503.02725},
author = {Sharma, Abhishek and Tuzel, Oncel and Jacobs, David W.},
doi = {10.1109/CVPR.2015.7298651},
eprint = {1503.02725},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SharmaDeephierarchies.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {530--538},
title = {{Deep hierarchical parsing for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Harris1954,
abstract = {Harris maintains that it is possible to define a linguistic structure solely in terms of the "distributions" (= patterns of co-occurrences) of its elements. There is no parallel meaning-structure which can aid in describing formal structure. Meaning is partly a function of distribution.},
author = {Harris, Zellig S.},
doi = {10.1080/00437956.1954.11659520},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HarrisLanguage.pdf:pdf},
isbn = {978-90-277-1267-7},
issn = {0043-7956},
journal = {{\textless}i{\textgreater}WORD{\textless}/i{\textgreater}},
number = {2-3},
pages = {146--162},
title = {{Distributional Structure}},
url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
volume = {10},
year = {1954}
}
@article{Linnainmaa1976,
abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Linnainmaa1976.pdf:pdf},
issn = {00063835},
journal = {Bit},
number = {2},
pages = {146--160},
title = {{Taylor expansion of the accumulated rounding error}},
volume = {16},
year = {1976}
}
@article{Fu2015a,
abstract = {Object recognition by zero-shot learning (ZSL) aims to recognise objects without seeing any visual examples by learning knowledge transfer between seen and unseen ob- ject classes. This is typically achieved by exploring a se- mantic embedding space such as attribute space or seman- tic word vector space. In such a space, both seen and un- seen class labels, as well as image features can be embed- ded (projected), and the similarity between them can thus be measured directly. Existing works differ in what embed- ding space is used and how to project the visual data into the semantic embedding space. Yet, they all measure the similarity in the space using a conventional distance metric (e.g. cosine) that does not consider the rich intrinsic struc- ture, i.e. semantic manifold, of the semantic categories in the embedding space. In this paper we propose to model the semantic manifold in an embedding space using a semantic class label graph. The semantic manifold structure is used to redefine the distance metric in the semantic embedding space for more effective ZSL. The proposed semantic man- ifold distance is computed using a novel absorbing Markov chain process (AMP), which has a very efficient closed- form solution. The proposed new model improves upon and seamlessly unifies various existing ZSL algorithms. Exten- sive experiments on both the large scale ImageNet dataset and the widely used Animal with Attribute (AwA) dataset show that our model outperforms significantly the state-of- the-arts.},
author = {Fu, Zhenyong and Xiang, Tao A. and Kodirov, Elyor and Gong, Shaogang},
doi = {10.1109/CVPR.2015.7298879},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FuZSMANIFOLD.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2635--2644},
title = {{Zero-shot object recognition by semantic manifold distance}},
volume = {07-12-June},
year = {2015}
}
@article{Wu2014,
abstract = {We introduce Deep Semantic Embedding (DSE), a super- vised learning algorithm which computes semantic repre- sentation for text documents by respecting their similarity to a given query. Unlike other methods that use single- layer learning machines, DSE maps word inputs into a low- dimensional semantic space with deep neural network, and achieves a highly nonlinear embedding to model the human perception of text semantics. Through discriminative fine- tuning of the deep neural network, DSE is able to encode the relative similarity between relevant/irrelevant document pairs in training data, and hence learn a reliable ranking score for a query-document pair. We present test results on datasets including scientific publications and user-generated knowledge base.},
author = {Wu, Hao and Min, Martin Renqiang and Bai, Bing},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BaiDeepsemantic.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Deep Learning,Nonlinear Embedding,Ranking,Semantic Indexing},
pages = {46--52},
title = {{Deep semantic embedding}},
volume = {1204},
year = {2014}
}
@article{Zhang2017,
abstract = {Zero-shot learning (ZSL) models rely on learning a joint embedding space where both textual/semantic description of object classes and visual representation of object images can be projected to for nearest neighbour search. Despite the success of deep neural networks that learn an end-to-end model between text and images in other vision problems such as image captioning, very few deep ZSL model exists and they show little advantage over ZSL models that utilise deep feature representations but do not learn an end-to-end embedding. In this paper we argue that the key to make deep ZSL models succeed is to choose the right embedding space. Instead of embedding into a semantic space or an intermediate space, we propose to use the visual space as the embedding space. This is because that in this space, the subsequent nearest neighbour search would suffer much less from the hubness problem and thus become more effective. This model design also provides a natural mechanism for multiple semantic modalities (e.g., attributes and sentence descriptions) to be fused and optimised jointly in an end-to-end manner. Extensive experiments on four benchmarks show that our model significantly outperforms the existing models.},
archivePrefix = {arXiv},
arxivId = {1611.05088},
author = {Zhang, Li and Xiang, Tao and Gong, Shaogang},
doi = {10.1109/CVPR.2017.321},
eprint = {1611.05088},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZhangDeepEmb.pdf:pdf},
isbn = {9781538604571},
issn = {1611.05088},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {3010--3019},
title = {{Learning a deep embedding model for zero-shot learning}},
volume = {2017-Janua},
year = {2017}
}
@article{Xian2016,
abstract = {We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.},
archivePrefix = {arXiv},
arxivId = {1603.08895},
author = {Xian, Yongqin and Akata, Zeynep and Sharma, Gaurav and Nguyen, Quynh and Hein, Matthias and Schiele, Bernt},
doi = {10.1109/CVPR.2016.15},
eprint = {1603.08895},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Xian2016.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
title = {{Latent Embeddings for Zero-shot Classification}},
url = {http://arxiv.org/abs/1603.08895},
year = {2016}
}
@article{Peterson2018,
abstract = {Modern convolutional neural networks (CNNs) are able to achieve human-level object classification accuracy on specific tasks, and currently outperform competing models in explaining complex human visual representations. However, the categorization problem is posed differently for these networks than for humans: the accuracy of these networks is evaluated by their ability to identify single labels assigned to each image. These labels often cut arbitrarily across natural psychological taxonomies (e.g., dogs are separated into breeds, but never jointly categorized as "dogs"), and bias the resulting representations. By contrast, it is common for children to hear both "dog" and "Dalmatian" to describe the same stimulus, helping to group perceptually disparate objects (e.g., breeds) into a common mental class. In this work, we train CNN classifiers with multiple labels for each image that correspond to different levels of abstraction, and use this framework to reproduce classic patterns that appear in human generalization behavior.},
archivePrefix = {arXiv},
arxivId = {1805.07647},
author = {Peterson, Joshua C. and Soulos, Paul and Nematzadeh, Aida and Griffiths, Thomas L.},
eprint = {1805.07647},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Peterson.pdf:pdf},
title = {{Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels}},
url = {http://arxiv.org/abs/1805.07647},
year = {2018}
}
@article{Akata2016,
abstract = {Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as e.g. class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples.},
archivePrefix = {arXiv},
arxivId = {1503.08677},
author = {Akata, Zeynep and Perronnin, Florent and Harchaoui, Zaid and Schmid, Cordelia},
doi = {10.1109/TPAMI.2015.2487986},
eprint = {1503.08677},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/AkataALE.pdf:pdf},
isbn = {9781424439911},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Attributes,Fine Grained Image Classification,Image classification,Label Embedding,Subspace Learning},
number = {7},
pages = {1425--1438},
pmid = {26452251},
title = {{Label-Embedding for Image Classification}},
volume = {38},
year = {2016}
}
@book{Stillwell1991,
author = {Stillwell, John},
isbn = {0-8218-0529-0},
publisher = {American Mathematical Society},
title = {{Sources of hyperbolic geometry (History of mathematics v. 10)}},
year = {1991}
}
@article{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, D.G.},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SIFT.pdf:pdf},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
pages = {1150--1157 vol.2},
pmid = {15806121},
primaryClass = {cs},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/document/790410/},
year = {1999}
}
@article{Rohrbach2011,
abstract = {While knowledge transfer (KT) between object classes has been accepted as a promising route towards scalable recognition, most experimental KT studies are surprisingly limited in the number of object classes considered. To support claims of KT w.r.t. scalability we thus advocate to evaluate KT in a large-scale setting. To this end, we provide an extensive evaluation of three popular approaches to KT on a recently proposed large-scale data set, the ImageNet Large Scale Visual Recognition Competition 2010 data set. In a first setting they are directly compared to one-vs-all classification often neglected in KT papers and in a second setting we evaluate their ability to enable zero-shot learning. While none of the KT methods can improve over one-vs-all classification they prove valuable for zero-shot learning, especially hierarchical and direct similarity based KT. We also propose and describe several extensions of the evaluated approaches that are necessary for this large-scale study.},
author = {Rohrbach, Marcus and Stark, Michael and Schiele, Bernt},
doi = {10.1109/CVPR.2011.5995627},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Rohrbach2011.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {March},
pages = {1641--1648},
title = {{Evaluating knowledge transfer and zero-shot learning in a large-scale setting}},
year = {2011}
}
@article{Author,
author = {Author, Anonymous and Author, Anonymous},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BaiSemantic.pdf:pdf},
number = {1},
pages = {1--9},
title = {{Polynomial Semantic Indexing}}
}
@article{Razavian2014,
abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the $\backslash$overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the $\backslash$overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the $\backslash$overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or {\$}L2{\$} distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
doi = {10.1109/CVPRW.2014.131},
eprint = {1403.6382},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RazavianCNN.pdf:pdf},
isbn = {9781479943098},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {512--519},
pmid = {87882338},
title = {{CNN features off-the-shelf: An astounding baseline for recognition}},
year = {2014}
}
@article{Finn2015,
abstract = {Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.},
archivePrefix = {arXiv},
arxivId = {1509.06113},
author = {Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
eprint = {1509.06113},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FinnRL.pdf:pdf},
journal = {arXiv preprint arXiv:1509.06113},
title = {{Learning Visual Feature Spaces for Robotic Manipulation with Deep Spatial Autoencoders}},
url = {http://arxiv.org/abs/1509.06113},
year = {2015}
}
@article{Couprie2013,
abstract = {Scene labeling consists in labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape and contextual information.We report results using multiple post-processing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, e.g. they can be taken from a segmentation tree, or from any family of over-segmentations. The system yields record accuracies on the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) and near-record accuracy on Stanford Background Dataset (8 classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Couprie, Camille and Najman, Laurent and Lecun, Yann},
doi = {10.1109/TPAMI.2012.231},
eprint = {arXiv:1011.1669v3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FarabetScenelabel.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {Convolutional networks,deep learning,image classification,image segmentation,scene parsing},
number = {8},
pages = {1915--1929},
pmid = {23787344},
title = {{Learning Hierarchical Features for scence labeling}},
volume = {35},
year = {2013}
}
@article{Zhao2001,
abstract = {In this paper, we present the results of a project that seeks to transform low-level features to a higher level of meaning. This project concerns a technique, latent semantic indexing (LSI), in conjunction with normalization and term weighting, which have been used for full-text retrieval for many years. In this environment, LSI determines clusters of co-occurring keywords, sometimes, called concepts, so that a query which uses a particular keyword can then retrieve documents perhaps not containing this keyword, but containing other keywords from the same cluster. In this paper, we examine the use of this technique for content-based image retrieval, using two different approaches to image feature representation. We also study the integration of visual features and textual keywords and the results show that it can help improve the retrieval performance significantly. {\textcopyright} 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Zhao, Rong and Grosky, W. I.},
doi = {10.1016/S0031-3203(01)00062-0},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZhaoGrosskyPatternRecognition.pdf:pdf},
isbn = {9783540429128},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Anglogram,Content-based,Feature maps,Image retrieval,Latent semantic indexing,Semantic gap},
number = {3},
pages = {593--600},
title = {{Negotiating the semantic gap: From feature maps to semantic landscapes}},
volume = {35},
year = {2001}
}
@article{Kodirov2017,
abstract = {Existing zero-shot learning (ZSL) models typically learn a projection function from a feature space to a semantic embedding space (e.g.{\~{}}attribute space). However, such a projection function is only concerned with predicting the training seen class semantic representation (e.g.{\~{}}attribute prediction) or classification. When applied to test data, which in the context of ZSL contains different (unseen) classes without training data, a ZSL model typically suffers from the project domain shift problem. In this work, we present a novel solution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing ZSL models. However, the decoder exerts an additional constraint, that is, the projection/code must be able to reconstruct the original visual feature. We show that with this additional reconstruction constraint, the learned projection function from the seen classes is able to generalise better to the new unseen classes. Importantly, the encoder and decoder are linear and symmetric which enable us to develop an extremely efficient learning algorithm. Extensive experiments on six benchmark datasets demonstrate that the proposed SAE outperforms significantly the existing ZSL models with the additional benefit of lower computational cost. Furthermore, when the SAE is applied to supervised clustering problem, it also beats the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1704.08345},
author = {Kodirov, Elyor and Xiang, Tao and Gong, Shaogang},
doi = {10.1109/CVPR.2017.473},
eprint = {1704.08345},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/KodirovSAE.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {4447--4456},
title = {{Semantic autoencoder for zero-shot learning}},
volume = {2017-Janua},
year = {2017}
}
@article{Patel2015,
abstract = {In pattern recognition and computer vision, one is often faced with scenarios where the training data used to learn a model have different distribution from the data on which the model is applied. Regardless of the cause, any distributional change that occurs after learning a classifier can degrade its performance at test time. Domain adaptation tries to mitigate this degradation. In this article, we provide a survey of domain adaptation methods for visual recognition. We discuss the merits and drawbacks of existing domain adaptation approaches and identify promising avenues for research in this rapidly evolving field.},
author = {Patel, Vishal M. and Gopalan, Raghuraman and Li, Ruonan and Chellappa, Rama},
doi = {10.1109/MSP.2014.2347059},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SurveyDomainAdapt.pdf:pdf},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {3},
pages = {53--69},
title = {{Visual Domain Adaptation: A survey of recent advances}},
volume = {32},
year = {2015}
}
@book{Ritter1999,
author = {Ritter, Helge},
booktitle = {Kohonen maps},
publisher = {Elsevier},
title = {{Self-Organizing Maps on non-euclidean Spaces}},
year = {1999}
}
@article{Weston2010,
abstract = {Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced sibling precision metric, where our method also obtains excellent results.},
author = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
doi = {10.1007/s10994-010-5198-3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/WestonWordimg.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Embedding,Image annotation,Large scale,Learning to rank},
number = {1},
pages = {21--35},
pmid = {2895801},
title = {{Large scale image annotation: Learning to rank with joint word-image embeddings}},
volume = {81},
year = {2010}
}
