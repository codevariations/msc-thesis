Automatically generated by Mendeley Desktop 1.19
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Kobatake1994,
author = {Kobatake, E. and Tanaka, K.},
journal = {Journal of Neurophysiology},
pages = {856--867},
title = {{Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex}},
volume = {71},
year = {1994}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Chamberlain2017,
abstract = {Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.},
archivePrefix = {arXiv},
arxivId = {1705.10359},
author = {Chamberlain, Benjamin Paul and Clough, James and Deisenroth, Marc Peter},
eprint = {1705.10359},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Neural Embeddings.pdf:pdf},
keywords = {complex networks,geometry,graph embeddings,neural networks},
title = {{Neural Embeddings of Graphs in Hyperbolic Space}},
url = {http://arxiv.org/abs/1705.10359},
year = {2017}
}
@article{Griffin2013,
author = {Griffin, Greg},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GriffinTaxonomies.pdf:pdf},
isbn = {9781424422432},
title = {{Learning and Using Taxonomies for Visual and Olfactory Classification Thesis by}},
volume = {2013},
year = {2013}
}
@article{Wang2016,
abstract = {While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model},
archivePrefix = {arXiv},
arxivId = {1604.04573},
author = {Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
doi = {10.1109/CVPR.2016.251},
eprint = {1604.04573},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/cnnrnn.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2285--2294},
pmid = {10463930},
title = {{CNN-RNN: A Unified Framework for Multi-label Image Classification}},
url = {http://ieeexplore.ieee.org/document/7780620/},
year = {2016}
}
@article{Peterson2018,
abstract = {Modern convolutional neural networks (CNNs) are able to achieve human-level object classification accuracy on specific tasks, and currently outperform competing models in explaining complex human visual representations. However, the categorization problem is posed differently for these networks than for humans: the accuracy of these networks is evaluated by their ability to identify single labels assigned to each image. These labels often cut arbitrarily across natural psychological taxonomies (e.g., dogs are separated into breeds, but never jointly categorized as "dogs"), and bias the resulting representations. By contrast, it is common for children to hear both "dog" and "Dalmatian" to describe the same stimulus, helping to group perceptually disparate objects (e.g., breeds) into a common mental class. In this work, we train CNN classifiers with multiple labels for each image that correspond to different levels of abstraction, and use this framework to reproduce classic patterns that appear in human generalization behavior.},
archivePrefix = {arXiv},
arxivId = {1805.07647},
author = {Peterson, Joshua C. and Soulos, Paul and Nematzadeh, Aida and Griffiths, Thomas L.},
eprint = {1805.07647},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Peterson.pdf:pdf},
title = {{Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels}},
url = {http://arxiv.org/abs/1805.07647},
year = {2018}
}
@article{Read2014,
abstract = {In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature},
archivePrefix = {arXiv},
arxivId = {1502.05988},
author = {Read, Jesse and Perez-Cruz, Fernando},
eprint = {1502.05988},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ReadDLmultilabel.pdf:pdf},
pages = {1--8},
title = {{Deep Learning for Multi-label Classification}},
url = {http://arxiv.org/abs/1502.05988},
year = {2014}
}
@article{Socher,
abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training im-ages and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what ob-jects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or un-seen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.},
archivePrefix = {arXiv},
arxivId = {1301.3666},
author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
eprint = {1301.3666},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SocherXmodal.pdf:pdf},
issn = {10495258},
title = {{Zero-Shot Learning Through Cross-Modal Transfer}}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/inceptionV3.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@inproceedings{Paszke2017,
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
booktitle = {NIPS},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@article{Hodgkin1990,
abstract = {This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkinet al., 1952,J. Physiol.116, 424-448; Hodgkin and Huxley 1952,J. Physiol.116, 449-566). Its general object is to discuss the results of the preceding papers (Section 1), to put them into mathematical form (Section 2) and to whow that they will account for conduction and excitation in quantitative terms (Sections 3-6).},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hodgkin, A. L. and Huxley, A. F.},
doi = {10.1007/BF02459568},
eprint = {NIHMS150003},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HodgingHuxley.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$n1097-6256 (Linking)},
issn = {00928240},
journal = {Bulletin of Mathematical Biology},
number = {1-2},
pages = {25--71},
pmid = {2185861},
title = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
volume = {52},
year = {1990}
}
@article{Zweig2007,
abstract = {We investigated the computational properties of natural object hierarchy in the context of constellation object class models, and its utility for object class recognition. We first observed an interesting computational property of the object hierarchy: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels (e.g., closed-frame vehicles or vehicles) exhibit higher recall but lower precision when compared with the class specific level (e.g., bus). These inherent differences suggest that combining object classifiers from different hierarchical levels into a single classifier may improve classification, as it appears like these models capture different aspects of the object. We describe a method to combine these classifiers, and analyze the conditions under which improvement can be guaranteed. When given a small sample of a new object class, we describe a method to transfer knowledge across the tree hierarchy, between related objects. Finally, we describe extensive experiments using object hierarchies obtained from publicly available datasets, and show that the combined classifiers significantly improve recognition results.},
author = {Zweig, Alon and Weinshall, Daphna},
doi = {10.1109/ICCV.2007.4409064},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZweigHierarchies.pdf:pdf},
isbn = {978-1-4244-1630-1},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {October},
title = {{Exploiting object hierarchy: Combining models from different category levels}},
year = {2007}
}
@article{Xu2000,
abstract = {The authors present a Bayesian framework for understanding how adults and children learn the meanings of words. The theory explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents, by making rational inductive inferences that integrate prior knowledge about plausible word meanings with the statistical structure of the observed examples. The theory addresses shortcomings of the two best known approaches to modeling word learning, based on deductive hypothesis elimination and associative learning. Three experiments with adults and children test the Bayesian account's predictions in the context of learning words for object categories at multiple levels of a taxonomic hierarchy. Results provide strong support for the Bayesian account over competing accounts, in terms of both quantitative model fits and the ability to explain important qualitative phenomena. Several extensions of the basic theory are discussed, illustrating the broader potential for Bayesian models of word learning.},
author = {Xu, Fei and Tenenbaum, Joshua B},
doi = {10.1037/0033-295X.114.2.245},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/XuTennebaum.pdf:pdf},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Proceedings of the 22nd Annual Meeting of the Cognitive Science Society},
keywords = {Bayes Theorem,Child,Humans,Models,Preschool,Psychological,Verbal Learning,Vocabulary},
pages = {517--522},
pmid = {17500627},
title = {{Word learning as Bayesian inference.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17500627},
year = {2000}
}
@article{Sharma2015,
abstract = {This paper proposes a learning-based approach to scene parsing inspired by the deep Recursive Context Propagation Network (RCPN). RCPN is a deep feed-forward neural network that utilizes the contextual information from the entire image, through bottom-up followed by top-down context propagation via random binary parse trees. This improves the feature representation of every super-pixel in the image for better classification into semantic categories. We analyze RCPN and propose two novel contributions to further improve the model. We first analyze the learning of RCPN parameters and discover the presence of bypass error paths in the computation graph of RCPN that can hinder contextual propagation. We propose to tackle this problem by including the classification loss of the internal nodes of the random parse trees in the original RCPN loss function. Secondly, we use an MRF on the parse tree nodes to model the hierarchical dependency present in the output. Both modifications provide performance boosts over the original RCPN and the new system achieves state-of-the-art performance on Stanford Background, SIFT-Flow and Daimler urban datasets.},
archivePrefix = {arXiv},
arxivId = {1503.02725},
author = {Sharma, Abhishek and Tuzel, Oncel and Jacobs, David W.},
doi = {10.1109/CVPR.2015.7298651},
eprint = {1503.02725},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SharmaDeephierarchies.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {530--538},
title = {{Deep hierarchical parsing for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Linnainmaa1976,
abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Linnainmaa1976.pdf:pdf},
issn = {00063835},
journal = {Bit},
number = {2},
pages = {146--160},
title = {{Taylor expansion of the accumulated rounding error}},
volume = {16},
year = {1976}
}
@article{Zhou2005,
abstract = {Page 1. - with Training Examples Zhi-Hua Zhou 1 De-Chuan Zhan 1 Qiang Yang 2 1 National Key},
author = {Zhou, Zhi-Hua and Zhan, De-Chuan and Yang, Qiang},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/ZhouSemisuperv.pdf:pdf},
isbn = {978-1-57735-323-2},
journal = {Artificial Intelligence},
keywords = {Machine Learning,Technical Papers},
number = {1},
pages = {675--680},
title = {{Semi-Supervised Learning with Very Few Labeled Training Examples}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-107.pdf},
volume = {22},
year = {2005}
}
@article{Hinton1986,
abstract = {(From the chapter) first section of this chapter stresses some of the virtues of distributed representations / second section considers the efficiency of distributed representations, and shows clearly why distributed representations can be better than local ones for certain classes of problems / final section discusses some difficult issues which are often avoided by advocates of distributed representations, such as the representation of constituent structure and the sequential focusing of processing effort on different aspects of a structure object (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Hinton, G E and McClelland, J L and Rumelhart, D E},
doi = {10.1146/annurev-psych-120710-100344},
eprint = {15334406},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HintonDistributed.pdf:pdf},
isbn = {0-262-68053-X},
issn = {1534-7362},
journal = {Parallel Distributed Processing},
pages = {77--109},
pmid = {21943171},
title = {{Distributed representations}},
year = {1986}
}
@article{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their pre- dictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18{\%} across thou- sands of novel labels never seen by the visual model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5650v3},
author = {Frome, Andrea and Corrado, Gs and Shlens, Jonathon},
eprint = {arXiv:1312.5650v3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FromeDevise.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural {\ldots}},
pages = {1--11},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@article{Yang2018,
abstract = {Multi-label classification (MLC) is an important learning problem that expects the learning algorithm to take the hidden correlation of the labels into account. Extracting the hidden correlation is generally a challenging task. In this work, we propose a novel deep learning framework to better extract the hidden correlation with the help of the memory structure within recurrent neural networks. The memory stores the temporary guesses on the labels and effectively allows the framework to rethink about the goodness and correlation of the guesses before making the final prediction. Furthermore, the rethinking process makes it easy to adapt to different evaluation criterion to match real-world application needs. Experimental results across many real-world data sets justify that the rethinking process indeed improves MLC performance across different evaluation criteria and leads to superior performance over state-of-the-art MLC algorithms.},
archivePrefix = {arXiv},
arxivId = {1802.01697},
author = {Yang, Yao-Yuan and Lin, Yi-An and Chu, Hong-Min and Lin, Hsuan-Tien},
eprint = {1802.01697},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YangRethinking.pdf:pdf},
title = {{Deep Learning with a Rethinking Structure for Multi-label Classification}},
url = {http://arxiv.org/abs/1802.01697},
year = {2018}
}
@inproceedings{Zeiler2014,
author = {Zeiler, Matthew and Fergus, Rob},
booktitle = {European Conference on Computer Vision},
pages = {818--833},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2014}
}
@article{Robbins1951,
author = {Robbins, Herbert and Monro, Suttton},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {400--407},
title = {{A Stochastic Approximation Method}},
volume = {22},
year = {1951}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PenningtonGlove.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1532--1543},
pmid = {1710995},
title = {{Glove: Global Vectors for Word Representation}},
url = {http://aclweb.org/anthology/D14-1162},
year = {2014}
}
@article{Redmon,
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RedmonYOLO9000.pdf:pdf},
pages = {7263--7271},
title = {{Better , Faster , Stronger}}
}
@article{Gong2013,
abstract = {Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with approximate top-{\$}k{\$} ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10{\%}, obtaining the best reported performance in the literature.},
archivePrefix = {arXiv},
arxivId = {1312.4894},
author = {Gong, Yunchao and Jia, Yangqing and Leung, Thomas and Toshev, Alexander and Ioffe, Sergey},
eprint = {1312.4894},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GongDeepmultirank.pdf:pdf},
pages = {1--9},
pmid = {2094121},
title = {{Deep Convolutional Ranking for Multilabel Image Annotation}},
url = {http://arxiv.org/abs/1312.4894},
year = {2013}
}
@article{Lecun2015,
author = {Lecun, Yann and Hinton, Geoffrey E. and Bengio, Yoshua},
journal = {Nature},
pages = {436--444},
title = {{Deep Learning}},
volume = {521},
year = {2015}
}
@article{Shelhamer2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor and Shelhamer, Evan and Darrell, Trevor and Long, Jonathan and Darrell, Trevor and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/LongSegmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2015}
}
@article{Harris1954,
abstract = {Harris maintains that it is possible to define a linguistic structure solely in terms of the "distributions" (= patterns of co-occurrences) of its elements. There is no parallel meaning-structure which can aid in describing formal structure. Meaning is partly a function of distribution.},
author = {Harris, Zellig S.},
doi = {10.1080/00437956.1954.11659520},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HarrisLanguage.pdf:pdf},
isbn = {978-90-277-1267-7},
issn = {0043-7956},
journal = {{\textless}i{\textgreater}WORD{\textless}/i{\textgreater}},
number = {2-3},
pages = {146--162},
title = {{Distributional Structure}},
url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
volume = {10},
year = {1954}
}
@article{Biederman1989,
abstract = {Reports that an article by I. Biederman published in Psychological Review, 1987(Apr), Vol 94(2), 115-117) was inadvertently a duplicate publication. A large portion of this article had previously appeared as unedited conference proceedings in another journal and in an edited book. (The following abstract of this article originally appeared in PA, Vol 74:20898.) The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons, can be derived from contrasts of five readily detectable properties of edges in a two-dimensional image. The detection of these properties is generally invariant over viewing position and image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. (PsycLIT Database Copyright 1993 American Psychological Assn, all rights reserved)},
author = {Biederman, Irving},
doi = {10.1037/0033-295X.96.1.2},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/biederman1987.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471},
journal = {Psychological Review},
number = {1},
pages = {2--2},
title = {{"Recognition-by-components: A theory of human image understanding": Clarification.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.96.1.2},
volume = {96},
year = {1989}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1112.6209},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
eprint = {arXiv:1112.6209},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Rosenblatt1958.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471(Electronic);0033-295X(Print)},
journal = {Psychological Review},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in {\ldots}}},
url = {http://psycnet.apa.org/journals/rev/65/6/386.pdf{\%}5Cnpapers://c53d1644-cd41-40df-912d-ee195b4a4c2b/Paper/p15420},
volume = {65},
year = {1958}
}
@article{Pathak2014,
abstract = {Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.},
archivePrefix = {arXiv},
arxivId = {1412.7144},
author = {Pathak, Deepak and Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1103/PhysRevA.92.013626},
eprint = {1412.7144},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/PathakMIL.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {1050-2947},
number = {1},
pages = {1--4},
pmid = {823878},
title = {{Fully Convolutional Multi-Class Multiple Instance Learning}},
url = {http://arxiv.org/abs/1412.7144},
year = {2014}
}
@article{JurgenSchmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in
pattern recognition and machine learning. This historical survey compactly summarizes relevant work,
much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth
of their credit assignment paths, which are chains of possibly learnable, causal links between actions
and effects. I review deep supervised learning (also recapitulating the history of backpropagation),
unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short
programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828},
author = {{J{\"{u}}rgen Schmidhuber}},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/SchmidhuberHistory.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
pages = {85--117},
title = {{Deep learning in neural networks: An overview}},
url = {https://ac.els-cdn.com/S0893608014002135/1-s2.0-S0893608014002135-main.pdf?{\_}tid=d25460aa-c556-11e7-a97c-00000aacb35f{\&}acdnat=1510236415{\_}e1693fdedfe07935906af07f4120d5af},
volume = {61},
year = {2015}
}
@article{You2016,
abstract = {Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.},
archivePrefix = {arXiv},
arxivId = {1603.03925},
author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
doi = {10.1109/CVPR.2016.503},
eprint = {1603.03925},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YouCaptionatt.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
title = {{Image Captioning with Semantic Attention}},
url = {http://arxiv.org/abs/1603.03925},
year = {2016}
}
@article{Joliceur1984,
author = {Joliceur, P and Gluck, M A and Kosslyn, S M},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/JolicoeurPictures.pdf:pdf},
journal = {Cognitive Psychology},
pages = {243--275},
title = {{Pictures and Naming: Making the Connection}},
volume = {16},
year = {1984}
}
@misc{Hinton1986a,
abstract = {Concepts can be represented by distributed patterns of activity in networks of neuron-like units. One advantage of this kind of representation is that it leads to automatic generalization. When the weighjts in the network are changed to incorporate new knowledgwe about one concept, the changes affect the knowledge associated with other concepts that are represented by similar activity patterns. There have been numerous demonstrations of sensible generalization which have depended on the experimenter choosing appropriately similar patterns for diferent concepts. This paper shows how the network can be made to choose the patterns itself when shown a set of propositions that use the concepts. It chooses patterns which make explicit the underlying features that are only implicit in the propositions it is shown.},
archivePrefix = {arXiv},
arxivId = {1601.01280},
author = {Hinton, Geoffrey},
booktitle = {Css},
doi = {10.1109/69.917563},
eprint = {1601.01280},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Hinton1986.pdf:pdf},
isbn = {0-262-68053-X},
issn = {10414347},
pages = {1--12},
pmid = {21943171},
title = {{Learning distributed representations of concepts}},
url = {http://www.cogsci.ucsd.edu/{~}ajyu/Teaching/Cogs202{\_}sp13/Readings/hinton86.pdf},
year = {1986}
}
@article{Guillaumin2010,
author = {Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Schmid, Cordelia and Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Discrim-, Cordelia Schmid Tagprop and Guillaumin, Matthieu and Mensink, Thomas and Verbeek, Jakob and Schmid, Cordelia and Kuntzmann, Laboratoire Jean},
doi = {10.1109/ICCV.2009.5459266>},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/tagprop.pdf:pdf},
title = {{TagProp : Discriminative metric learning in nearest neighbor models for image auto-annotation To cite this version : TagProp : Discriminative Metric Learning in Nearest Neighbor Models for Image Auto-Annotation}},
year = {2010}
}
@article{Rosch1976,
abstract = {Categorizations which humans make of the concrete world are not arbitrary but highly determined. In taxonomies of concrete objects, there is one level of abstraction at which the most basic category cuts are made. Basic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. The four experiments of Part I define basic objects by demonstrating that in taxonomies of common concrete nouns in English based on class inclusion, basic objects are the most inclusive categories whose members: (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. The eight experiments of Part II explore implications of the structure of categories. Basic objects are shown to be the most inclusive categories for which a concrete image of the category as a whole can be formed, to be the first categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language. {\textcopyright} 1976.},
author = {Rosch, Eleanor and Mervis, Carolyn B. and Gray, Wayne D. and Johnson, David M. and Boyes-Braem, Penny},
doi = {10.1016/0010-0285(76)90013-X},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/RoschCategories.pdf:pdf},
isbn = {0010-0285},
issn = {00100285},
journal = {Cognitive Psychology},
number = {3},
pages = {382--439},
title = {{Basic objects in natural categories}},
volume = {8},
year = {1976}
}
@article{Guo2016,
abstract = {Deep learning algorithms are a subset of the machine learning algorithms, which aim at discovering multiple levels of distributed representations. Recently, numerous deep learning algorithms have been proposed to solve traditional artificial intelligence problems. This work aims to review the state-of-the-art in deep learning algorithms in computer vision by highlighting the contributions and challenges from over 210 recent research papers. It first gives an overview of various deep learning approaches and their recent developments, and then briefly describes their applications in diverse vision tasks, such as image classification, object detection, image retrieval, semantic segmentation and human pose estimation. Finally, the paper summarizes the future trends and challenges in designing and training deep neural networks.},
author = {Guo, Yanming and Liu, Yu and Oerlemans, Ard and Lao, Songyang and Wu, Song and Lew, Michael S.},
doi = {10.1016/j.neucom.2015.09.116},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GuoVisualunderstand.pdf:pdf},
isbn = {0925-2312},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Applications,Challenges,Computer vision,Deep learning,Developments,Trends},
pages = {27--48},
title = {{Deep learning for visual understanding: A review}},
volume = {187},
year = {2016}
}
@article{DiCarlo2012,
author = {DiCarlo, James J. and Yoccolan, Davide and Rust, Nicole C.},
doi = {10.1016/j.neuron.2012.01.010.How},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/DiCarlo2013.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
number = {3},
pages = {415--434},
pmid = {22325196},
title = {{How does the brain solve visual object recognition?}},
volume = {73},
year = {2012}
}
@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E},
doi = {10.1145/584091.584093},
eprint = {9411012},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/Shannon.pdf:pdf},
isbn = {0252725484},
issn = {07246811},
journal = {The Bell System Technical Journal},
number = {July 1928},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A mathematical theory of communication}},
url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
volume = {27},
year = {1948}
}
@misc{Le,
author = {Lecun, Y. and Boser, B. and Denker, J.S. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/lecun89.pdf:pdf},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
year = {1989}
}
@article{Monay2004,
abstract = {We address the problem of unsupervised image auto-annotation with probabilistic latent space models. Unlike most previous works, which build latent space representations assuming equal relevance for the text and visual modalities, we propose a new way of modeling multi-modal co-occurrences, constraining the definition of the latent space to ensure its consistency in semantic terms (words), while retaining the ability to jointly model visual information. The concept is implemented by a linked pair of Probabilistic Latent Semantic Analysis (PLSA) models. On a 16000-image collection, we show with extensive experiments that our approach significantly outperforms previous joint models.},
author = {Monay, Florent and Gatica-perez, Daniel},
doi = {10.1145/1027527.1027608},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MonayPLSA.pdf:pdf},
isbn = {1581138938},
journal = {Proceedings of the 12th annual ACM international conference on Multimedia},
keywords = {automatic annotation of images,plsa,semantic indexing},
pages = {348--351},
title = {{PLSA-based Image Auto-Annotation : Constraining the Latent Space}},
year = {2004}
}
@article{Wu2014,
abstract = {We introduce Deep Semantic Embedding (DSE), a super- vised learning algorithm which computes semantic repre- sentation for text documents by respecting their similarity to a given query. Unlike other methods that use single- layer learning machines, DSE maps word inputs into a low- dimensional semantic space with deep neural network, and achieves a highly nonlinear embedding to model the human perception of text semantics. Through discriminative fine- tuning of the deep neural network, DSE is able to encode the relative similarity between relevant/irrelevant document pairs in training data, and hence learn a reliable ranking score for a query-document pair. We present test results on datasets including scientific publications and user-generated knowledge base.},
author = {Wu, Hao and Min, Martin Renqiang and Bai, Bing},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BaiDeepsemantic.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Deep Learning,Nonlinear Embedding,Ranking,Semantic Indexing},
pages = {46--52},
title = {{Deep semantic embedding}},
volume = {1204},
year = {2014}
}
@article{Couprie2013,
abstract = {Scene labeling consists in labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape and contextual information.We report results using multiple post-processing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, e.g. they can be taken from a segmentation tree, or from any family of over-segmentations. The system yields record accuracies on the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) and near-record accuracy on Stanford Background Dataset (8 classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Couprie, Camille and Najman, Laurent and Lecun, Yann},
doi = {10.1109/TPAMI.2012.231},
eprint = {arXiv:1011.1669v3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/FarabetScenelabel.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {Convolutional networks,deep learning,image classification,image segmentation,scene parsing},
number = {8},
pages = {1915--1929},
pmid = {23787344},
title = {{Learning Hierarchical Features for scence labeling}},
volume = {35},
year = {2013}
}
@article{Hubeld1962,
author = {Hubeld, D. H. and Wiesel, T.},
journal = {Journal of Physiology},
number = {160},
pages = {106 -- 154},
title = {{Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex}},
year = {1962}
}
@misc{Abadi2015,
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
title = {{TensorFlow: Large-scale machine learning on heterogeneous systems}},
url = {tensorflow.org},
year = {2015}
}
@article{Mikolov,
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MikoloW2V.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
pages = {1--9},
pmid = {903},
title = {{5021-Distributed-Representations-of-Words-and-Phrases-and-Their-Compositionality}}
}
@article{Karpathy2015,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Li, Fei Fei},
doi = {10.1109/CVPR.2015.7298932},
eprint = {1412.2306},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/KarpathyDeepvisual.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {4},
pages = {3128--3137},
pmid = {16873662},
title = {{Deep visual-semantic alignments for generating image descriptions}},
volume = {39},
year = {2015}
}
@article{Makadia2008,
abstract = {Automatically assigning keywords to images is of great interest as it allows one to retrieve, index, organize and understand large collections of image data. Many techniques have been proposed for image annotation in the last decade that give reasonable performance on standard datasets. However, most of these works fail to compare their methods with simple baseline techniques to justify the need for complex models and subsequent training. In this work, we introduce a new and simple baseline technique for image annotation that treats annotation as a retrieval problem. The proposed technique utilizes global low-level image features and a simple combination of basic distance measures to find nearest neighbors of a given image. The keywords are then assigned using a greedy label transfer mechanism. The proposed baseline method outperforms the current state-of-the-art methods on two standard and one large Web dataset. We believe that such a baseline measure will provide a strong platform to compare and better understand future annotation techniques.},
author = {Makadia, Ameesh and Pavlovic, Vladimir and Kumar, Sanjiv},
doi = {10.1007/978-3-540-88690-7-24},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MakadiaBaseline.pdf:pdf},
isbn = {3540886893},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {a baseline measure will,a greedy label transfer,a strong platform to,and one large web,are then assigned using,compare and better understand,current state-of-the-art methods on,dataset,future annotation techniques,mechanism,posed baseline outperforms the,provide,the pro-,two standard,we believe that such},
number = {PART 3},
pages = {316--329},
pmid = {16546397},
title = {{A new baseline for image annotation}},
volume = {5304 LNCS},
year = {2008}
}
@article{Yeh2017,
abstract = {Multi-label classification is a practical yet challenging task in machine learning related fields, since it requires the prediction of more than one label category for each input instance. We propose a novel deep neural networks (DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this task. Aiming at better relating feature and label domain data for improved classification, we uniquely perform joint feature and label embedding by deriving a deep latent space, followed by the introduction of label-correlation sensitive loss function for recovering the predicted label outputs. Our C2AE is achieved by integrating the DNN architectures of canonical correlation analysis and autoencoder, which allows end-to-end learning and prediction with the ability to exploit label dependency. Moreover, our C2AE can be easily extended to address the learning problem with missing labels. Our experiments on multiple datasets with different scales confirm the effectiveness and robustness of our proposed method, which is shown to perform favorably against state-of-the-art methods for multi-label classification.},
archivePrefix = {arXiv},
arxivId = {1707.00418},
author = {Yeh, Chih-Kuan and Wu, Wei-Chieh and Ko, Wei-Jen and Wang, Yu-Chiang Frank},
eprint = {1707.00418},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/YehLatentmulti.pdf:pdf},
keywords = {Machine Learning Methods},
pages = {2838--2844},
title = {{Learning Deep Latent Spaces for Multi-Label Classification}},
url = {http://arxiv.org/abs/1707.00418},
year = {2017}
}
@article{Girshick2012,
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/GirschickSemseg.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
pages = {2--9},
pmid = {26656583},
title = {{Girshick{\_}Rich{\_}Feature{\_}Hierarchies{\_}2014{\_}CVPR{\_}paper}},
year = {2012}
}
@article{Marszaek2007,
abstract = {In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection. We evaluate how our approach influences the classification accuracy and speed on the PASCAL VOC challenge 2006 dataset, a set of challenging real-world images. We also demonstrate additional features that become available to object recognition due to the extension with semantic inference tools - we can classify high-level categories, such as animals, and we can train part detectors, for example a window detector, by pure inference in the semantic network.},
author = {Marsza{\l}ek, Marcin and Schmid, Cordelia},
doi = {10.1109/CVPR.2007.383272},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/MarszalekHierarchies.pdf:pdf},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
title = {{Semantic hierarchies for visual object Recognition}},
year = {2007}
}
@article{Rumelhart1985,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
journal = {Institute for Cognitive Science Report},
number = {8506},
title = {{Learning Internal Representations by Error Propagation}},
year = {1985}
}
@article{Author,
author = {Author, Anonymous and Author, Anonymous},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BaiSemantic.pdf:pdf},
number = {1},
pages = {1--9},
title = {{Polynomial Semantic Indexing}}
}
@article{Weston2010,
abstract = {Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced sibling precision metric, where our method also obtains excellent results.},
author = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
doi = {10.1007/s10994-010-5198-3},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/WestonWordimg.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Embedding,Image annotation,Large scale,Learning to rank},
number = {1},
pages = {21--35},
pmid = {2895801},
title = {{Large scale image annotation: Learning to rank with joint word-image embeddings}},
volume = {81},
year = {2010}
}
@article{Cadieu2014,
abstract = {The primate visual system achieves remarkable visual object recognition performance even in brief presentations and under changes to object exemplar, geometric transformations, and background variation (a.k.a. core visual object recognition). This remarkable performance is mediated by the representation formed in inferior temporal (IT) cortex. In parallel, recent advances in machine learning have led to ever higher performing models of object recognition using artificial deep neural networks (DNNs). It remains unclear, however, whether the representational performance of DNNs rivals that of the brain. To accurately produce such a comparison, a major difficulty has been a unifying metric that accounts for experimental limitations such as the amount of noise, the number of neural recording sites, and the number trials, and computational limitations such as the complexity of the decoding classifier and the number of classifier training examples. In this work we perform a direct comparison that corrects for these experimental limitations and computational considerations. As part of our methodology, we propose an extension of "kernel analysis" that measures the generalization accuracy as a function of representational complexity. Our evaluations show that, unlike previous bio-inspired models, the latest DNNs rival the representational performance of IT cortex on this visual object recognition task. Furthermore, we show that models that perform well on measures of representational performance also perform well on measures of representational similarity to IT and on measures of predicting individual IT multi-unit responses. Whether these DNNs rely on computational mechanisms similar to the primate visual system is yet to be determined, but, unlike all previous bio-inspired models, that possibility cannot be ruled out merely on representational performance grounds.},
archivePrefix = {arXiv},
arxivId = {1406.3284},
author = {Cadieu, Charles F. and Hong, Ha and Yamins, Daniel L K and Pinto, Nicolas and Ardila, Diego and Solomon, Ethan A. and Majaj, Najib J. and DiCarlo, James J.},
doi = {10.1371/journal.pcbi.1003963},
eprint = {1406.3284},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/cadieu2014.pdf:pdf},
isbn = {1553-7358},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {12},
pmid = {25521294},
title = {{Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition}},
volume = {10},
year = {2014}
}
@article{Huang2012,
abstract = {Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one represen- tation per word. This is problematic because words are often polysemous and global con- text can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the se- mantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning mul- tiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper- forms competitive baselines and other neural language models.},
archivePrefix = {arXiv},
arxivId = {1602.07019},
author = {Huang, Eric H and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
doi = {10.1002/9781118510001.ch8},
eprint = {1602.07019},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/HuangWordemb.pdf:pdf},
isbn = {9781937284244},
issn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {873--882},
pmid = {1710995},
title = {{ImprovingWord Representations via Global Context and MultipleWord Prototypes}},
year = {2012}
}
@article{Baltruschat2018,
abstract = {The increased availability of X-ray image archives (e.g. the ChestX-ray14 dataset from the NIH Clinical Center) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolutions of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we evaluate the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. We observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-50, integrating non-image data yields the best overall results.},
archivePrefix = {arXiv},
arxivId = {1803.02315},
author = {Baltruschat, Ivo M. and Nickisch, Hannes and Grass, Michael and Knopp, Tobias and Saalbach, Axel},
eprint = {1803.02315},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/XRAY.pdf:pdf},
keywords = {chest x-ray,chestx-ray14,convolutional neural net-,deep learning,transfer learning,works},
number = {c},
title = {{Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification}},
url = {http://arxiv.org/abs/1803.02315},
volume = {14},
year = {2018}
}
@book{Trindade2013,
abstract = {Sentiment analysis is an area of research that has gained considerable attention in recent years due to the increasing availability of opinionated information online. The majority of the work in sentiment analysis considers the polarity of word terms rather than the polarity of specific senses of the word but different senses of a word can have different opinion-related properties. In order to address this issue we consider novel semantic features of words in the context of a sentence. We take a sentence as a sequence of words augmented with features based on word sense disambiguation and sentiment lexicons with sense specific opinion-related properties. We then use a factored version of the sequence kernel in a support vector machine, and apply it to sentiment classification of sentences. We evaluate this sentiment analysis methodology on three publicly available corpuses. We also evaluate the effectiveness of several publicly available sense specific polarity lexicons and combinations. Experiments show that our factored approach offers improvements over the surface words baseline and other state-of-the-art kernels. {\textcopyright} 2013 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0445v2},
author = {Trindade, Luis and Wang, Hui and Blackburn, William and Rooney, Niall},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39593-2},
eprint = {arXiv:1305.0445v2},
file = {:home/hege/Documents/Thesis/msc-thesis/Papers/BengioRepresentations.pdf:pdf},
isbn = {978-3-642-39592-5},
issn = {03029743},
keywords = {Information Retrieval,Kernel Methods,Opinion Mining,Polarity Classification,Sentiment Analysis,Social Media,Word Sense Disambiguation},
number = {July},
pages = {284--296},
title = {{Statistical Language and Speech Processing}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84883190443{\&}partnerID=tZOtx3y1},
volume = {7978},
year = {2013}
}
