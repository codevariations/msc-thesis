\documentclass[12pt]{report}
\usepackage{setspace}
%\usepackage{subfigure}

\pagestyle{plain}
\usepackage{amssymb,graphicx,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\newtheorem{theorem}{THEOREM}
\newtheorem{lemma}[theorem]{LEMMA}
\newtheorem{corollary}[theorem]{COROLLARY}
\newtheorem{proposition}[theorem]{PROPOSITION}
\newtheorem{remark}[theorem]{REMARK}
\newtheorem{definition}[theorem]{DEFINITION}
\newtheorem{fact}[theorem]{FACT}

\newtheorem{problem}[theorem]{PROBLEM}
\newtheorem{exercise}[theorem]{EXERCISE}
\def \set#1{\{#1\} }

\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}


\graphicspath{
  {/home/hege/Documents/Thesis/msc-thesis/}
  {/home/hege/Documents/Thesis/msc-thesis/figures/}
}

\newcommand{\nats}{\mbox{\( \mathbb N \)}}
\newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{  	{ \includegraphics[scale=.5]{ucl_logo.png}}\\
{{\Huge Improving the Generalization Performance of Deep Learning with Hyperbolic Semantic Embeddings}}\\
{\large with Application to Zero-shot Learning}\\
		}
\date{Submission date: Day Month Year}
\author{Hermanni H{\"a}lv{\"a} \thanks{
{\bf Disclaimer:}
This report is submitted as part requirement for the MSc CSML degree at UCL. It is
substantially the result of my own work except where explicitly indicated in the text.
The report may be freely copied and distributed provided the source is explicitly acknowledged
\newline  %% \\ screws it up
}
\\ \\
MSc. Computational Statistics and Machine Learning\\ \\
Supervisor: Prof. Bradley Love}

\begin{document}
 
\onehalfspacing
\maketitle
\begin{abstract}
Summarise your report concisely.
\end{abstract}
\chapter*{Acknowledgements}
\thispagestyle{empty}
First, I would like to thank my supervisor Prof. Bradley Love, for all of his guidance, support and loads of great ideas. His genuine interest in the project and desire to help out when needed the most really made the entire process very enjoyable. Massive thanks also to Olivia Guest, and Sebastian Bobadilla-Suarez for sharing their ideas and for pointing me to loads of relevant research I likely would have otherwise missed. Thank you to Adam Hornsby for helping me get my head around the different word embedding methods and for how I should think about presenting the project. Special thank you to Brett Roads for being incredibly helpful with setting up and helping to maintain the compute resources etc. that were required for this project; I was so impressed with the whole set up and how well it all functioned. For sure, nothing would have come of this project had I not received all this help, so for that - thank you!

\clearpage

\tableofcontents
\listoffigures
\listoftables
\setcounter{page}{1}

\chapter{Notes}
\begin{itemize}
  \item essentially our model proposes that semantic taxonomy is the best guide to visual taxonomy -- better than wikipedia 
    \item discuss how first trained on small data set
    \item does current updates necessarily follow geodesics. perhaps small learning rate is very important -- need to explore in more detail lr setting
    \item poincare model distorts distances \cite{Greenberg1994} -- I wonder if this is a problem.
    \item check that final dense layer mentioned in CNN intro
    \item one practical limitation is how to compute nearest neighbours. We do exhaustive search but could do riemannian spectral hashing.
    \item in Data set section write about how difficult the task is by giving examples of 20k data set
    \item mention knowledge transfer
    \item one problem is that we are training only transformation to the leafs of the nodes. I wonder if we could includ basic level labels since those are the ones humans learn first and also some suggest on average capture most distinction (see the two papers)
    \item look at yolo 9000 for wordnet explanation
    \item Do I clearly state that CNNs are the state-of-the-art on image classification, give some example of current results / super human performance or when they wont competitions first time
    \item applications of zero-shot learning and better representation. e.g. autonomous vehicles need to make deductions. 
    \item benthos example on wikipedia
    \item because of benthos problem -- it would be worth testing our model on the animals with attributes data set 
    \item Is this really zero-shot learning since we are using knowledge
    \item see DEvise for motivating ZSL
    \item one motivatin is that it's expensive to get labelled data
    \item a potential problem is dimensionality is it too low? especially we wish some local smoothness in therms of visual samples in embedding space.perhaps more dimensions are needed for this. Though hard to really say since hyperbolic space..
    \item doman adaptation to help with transfer  from leaf to centre?
    \item problem if overall visual things arent near to each other in the hierarchy. Maybe more true for animals but less true for things that are 'used' for something.
    \item maybe one contribution is that visual similarity is not entire from taxonomy e.g. zebra maybe difficult to guess. Our benefit is really probabily to predict basic level classes. 
    \item zero shot image retrieval
    \item final layer of CNN doesnt have necessarily enough detailed information contained. Perhaps need to make links to higher layers \cite{Ba2015}
    \item is projection domain shift problem for this instance where output space is structured
    \item if need information on applications of ZSL see Fu and Sigal semi-supervised vocabulary-informed learning
    \item use maybe some other data set as well
    \item some very complex architectures - we want to produce simple easily justifiable solution
    \item we probably need to consider domain adaptation in more detail e.g. some literature \cite{Patel2015}, \cite{Wang2018}
    \item a problem with our domanin adaptation might be from only images to leaves, but actually we could re-group data and predict into higher nodes, this may improve domain adaptation
    \item Zebra may be difficult if the semantic space doesnt fully reflect visual space smoothly -- maybe need some model 
    \item suggest all\_embs model but note that it may not be practical as not dynamic
    \item maybe another issue is that labels on different levels may not be organised wrt each other
    \item other problem is that the poincare embedding that we have learnt is not perfect for example it appears golden retriever is closer to some non-retriever breeds than labrador retriever which shouldnt be the case. This in general might not be a problem because as demonstrate by Douwe and Kiela the overall strucutre is very good but perhaps there is more imprecision at the leaf nodes but problematically that is where the imagenet labels are. need better embedding maybe lorenz, maybe higher dimensional and also maybe need ot learn mapping with not-just leaf nodes. In fact, if there is imprecision more at the leaf nodes, then by leanrig a mapping on them we may overfit. This maybe also why softmax works better than distanc
    \item also problem since 10\% are dog breeds.
    \item one idea is to combine wordnet and wikipedia info and just do a voting system
    \item maybe could also compare attributes -- these attributes could be learnt from the different layers of the CNN, perhaps together they could used with hierarchy to predict new semantic embedding, could even combine with wikiedia. This would help. Combining different layers may be helpful for prediction of leaf nodes because at these nodes the differences between classes are determined by small details in parts.
    \item can we somehow get attributes into the thing by using the internal layers of a DNN -- avoid problem of expensive to manually draw them.
    \item maybe we dont have enough levels of hierarchy still to really get the benefit of the poincare models
\end{itemize}

\chapter[Introduction]{Introduction\raisebox{.3\baselineskip}{\normalsize\footnotemark}}
\footnotetext{All the code developed for this project can be found at https://github.com/HHalva/msc-thesis}

\section{Project motivation}
Over the past decade deep learning has achieved state-of-the-art results on virtually all computer vision tasks \cite{Goodfellow2016}. The impressive performance of these models has, in turn, propelled paramount improvements in a wide range of practical applications areas such as autonomous vehicles\cite{Bojarski2016}, face recognition \cite{Taigman2014} and robotics \cite{Finn2015}\cite{Ganegedara2017}. Despite the many successes of deep learning, most of the achievements rely on well defined training and test envionments; the generalization performance of these models is stil far from that of the human visual system. The broad aim of this thesis is to improve the generalization performance deep learning models in image classification. We approach this by attempting to teach the models a more human-like understanding of object categorization, and illustrate this within the context of zero-shot learning.

Some of the biggest advances in deep learning have come in the area of image classification, that is, the task of being able to a assign a correct label to an object in an image. Inspired by the breakthrough performance of Krizhevsky \textit{et al.} \cite{Krizhevsky2012} on the ImageNet 2012 classification challenge, researchers have produced increasingly novel deep learning models and as a result have exceeded human performance on this task \cite{Dodge2017}. Yet, the classification these models perform is simplistic in the way that each object is predicted to be one of a pre-defined subset of object classes. This is very different from the rich semantic understanding that humans display upon viewing an image. For instance, if we are presented an image of a labrador retriever, we could also put the image under a higher level category of dogs, or even animals and mammals. We therefore posses a complex semantic hierarchy of categories in our memory which is intertwined with our visual processing system \cite{Joliceur1984}. Deep learning models, on the other hand, are usually trained against individual object labels without any broader semantic context about the relationship between the object classes. We propose to resolve this shortcoming by instead training deep learning models against semantic embedding labels that capture the hierarchical semantic relationships between different classes of objects. For instance, in our model the label for labrador retriever will encode information about it belonging to the aforementioned higher-level categories of dog and animal, as well as information about how close or far away it is semantically from other categories such as a 'cat' or a 'football'. We do this by projecting all the class labels into a hyperbolic semantic embedding space that is optimal for representing hierarchical concepts and their relations, and train a deep learning model against these labels.

We chose to evaluate the generalization performance of our deep hyperbolic semantic embedding model on the problem of zero-shot learning \cite{Palatucci2009}. This is a challenging task in which the model is presented with images from classes of objects it has never seen before and the model tries to predict them correctly. In fact, this may seem impossible at first, but is something humans can easily do, under certain conditions. To see this, imagine for a moment that you have never seen a Zebra, but your zoologist friend aptly tells you that ``a Zebra is like a horse with black and white stripes''. The next time you see such an animal, you could recognize the Zebra, even though you have never seen one before! This demonstrates the potential power of semantic information in zero-shot learning. In particular, in zero-shot learning the zoologist friend is replaced by a semantic embedding space that contains the full range of possible concepts. The trick is to train a deep learning model to map a novel image into the embedding space and then find the nearest concept in this space. This idea is illustrated in Figure \ref{fig:intro}.

\begin{figure}
  \centering
	\includegraphics[width=\textwidth]{introzsl}
	\caption{Hypothetical example of zero-shot classfication. Assume that we have used training images and their embedding vectors (blue dots) to learn a mapping from image features into the hyperbolic embedding space. Next a novel class of image is encountered, since the entity in the image looks a bit like a mix of golden retriever and Irish red setter, the predicted semantic embedding falls in between those two, which indeed happens to be close to the ground-truth embedding: Nova-scotia Duck Tolling Retriever.}
	\label{fig:intro}
\end{figure}


We believe that hyperbolic semantic embeddings are particularly well suited for this task, but have not, our knowledge, previously been explore. This conjecture is based on the following two arguments. First, the hyperbolic embedding is able to reflect semantic hierarchical relationships between objects explicitly. Second, semantic hierarchies and similarities are closely linked, for instance, animals of the same biological family will share more visual similarities than those of different. Visually similar concepts should thus be close to each other in this space, and consequently searching for nearest embedding labels should provide better zero-shot predictions. Our central research question can thus be stated as: 
\begin{itemize}
\item Does semantic hierarchical knowledge in the form of hyperbolic embeddings help improve the zero-shot prediction capability of deep learning models.
\end{itemize}

Our research into zero-shot learning is also motivated by the fundamental scarcity of readily available labelled data. This is a problem for many machine vision systems as most are trained in supervised fashion, and subsequently limits many real world applications. As an example, for autonomous vehicles to operate safely, they need to be able to perform inference on unexpected visual views. Being able to identify an uknown object as a living entity can be incredibly valueable, even if more precise classification is not possible.

Overall our resuls\dots\dots\dots

In the remainder of this chapter we provide a brief overview of our methodology and how it fits in the existing literature on zero-shot learning. We also summarize our key results more explicitly and the main take-aways for future research. In Chapter \ref{ch:backg} we cover the necessary technical background for this project and review literature in more detail. Chapter \ref{} presents our approach in detail, and the full results from our experiments are given in Chapter \ref{}. Chapter \ref{} concludes our work with a discussion of the results and presents ideas for future research.

\section{Summary of our approach in relation to existing literature}
Modern deep neural networks take the form of Convolutional Neural Networks \cite{Lecun2015}, which learn a hierarchical visual representation of objects over multiple layers of convolution operations. Typically these models are trained against one-hot-encoded vectors of class labels: if there are N classes, the label vector of each image will be N long and have zeros at each dimension except a one in that of the correct class. Crucially, these vectors are orthogonal to each other. This is why standard deep learning models do not learn to capture any semantic relations between the visual objects, eventhough the hierarchical visual features that the model learn would be well suited for this. A handful works have attempted to improve up this by training against two different levels of hierarchy and found that this improves the generalization of these models \cite{Wang2015}, \cite{Peterson2018}. More research is needed however to determine whether accounting for more complex multi-level hierarchies would lead to further benefits in standard classification tasks.

Despite its limited use in the most popular deep learning applications, the idea of using semantic embeddings is well established in the area of zero--shot learning \cite{Palatucci2009, Socher, Huang2012, Frome2013, Norouzi2013}. Common to all of these approaches is to utilize a small subset of data to learn a mapping from image features into a semantic embedding vector-space where similar words are close to each other. By using this learnt mapping, any image can in fact be mapped into the embedding space and by finding the nearest label-vector predictions can be made for novel classes i.e. zero-shot classes of objects. Note that this is possible because the embedding space contains all the labels, including zero-shot ones; this akin to a human who knows the names and relations of all physical entities but has only seen some of them.

The dominant approach for constructing semantic embedding spaces has been to use the word2vec family of models \cite{Mikolov, Mikolov2013}. These take in a large corpus of text, such the 5.2 billions words long Wikipedia data, and place words that occur in a similar context near to each other in the resulting embedding space. For example, the model that is most similar to ours, DeViSE \cite{Frome2013}, uses a deep neural network to map into such an embedding. We posit, however, that these embeddings suffer from the fundamental limitation that many words which occur in a similar context may not share visual resemblence, and subsequently the semantic embedding built on them may not be as useful for visual reasoning. Moreover, similarity embeddings fail to capture the semantic hierarchy of objects, that is so natural to how humans categorize the world around us \cite{Rosch1976}. As a consequence it is not possible to say, based on the embedding alone, whether a cat is a domestic animal or a domestic animal is a cat, and thereon. 

Our idea of basing zero-shot recognition around an embedding that accurately captures semantic hierarchies is based on research in psychology that has found this to be the way humans perform object recogntion \cite{Rosch1976, Joliceur1984}. We have also been inspired by the recent work of Nickel and Kiela \cite{Nickel2017} who have illustrated how a hypebolic embedding, namely a Poincare embedding, can accurately preserve various complex semantic relations such as graph structures. We have used their model, in conjunction with the WordNet database\cite{Miller1995}, a hierarchical taxonomy of words, to create the hyperbolic embedding used in this project.

The main property than distinguishes hyperbolic spaces from Euclidean ones, and make them particularly suitable for embedding hierarchical data, is that they have a constant negative curvature \cite{Greenberg1994}. A consequence of this is that if we are in the middle of a hyperbolic ball, for instance, and start walking towards the the outer boundary of the ball, the space ahead of us expands exponentially rather than linearly; in fact, we will never reach the boundary. This exponential growth is also the rate at which hierarchical trees, with a constant branching factor, grow; hence the suitability of hyperbolic spaces to embed them \cite{Nickel2017, Chamberlain2017}.

After calculting the Poincare embedding, we propose two separate models to map into this spaces. The first one, called Deep Hyperbolic Semantic Embedding model (Deep-HSEM) employes a standard convolutional neural network in a similar vein as the DeViSE model of Frome \textit{et al.} \cite{Frome2013}. The second one, called Convex Hyperbolic Semantic Embedding model (Convex-HSEM) follows the approach of Norouzi \textit{et al.} \cite{Norouzi2013} and computes the predicted embeddings for zero-shot classes as convex combinations of known classes' embeddings. Both of these models were trained on the 1000 classes of ImageNet 2012 dataset \cite{JiaDeng2009, Russakovsky2015}, which contains a total of 1.2 million images. 21,000 previously unseen classes, total of around 13 million images, from the full ImageNet 2011 release \cite{JiaDeng2009} were used to evaluate the zero-shot capabilitis of the model. The next section presents our key results and conclusions.

\section{Summary of our results and discussion}






\newpage

\chapter{Background} \label{ch:backg}

\section{Artificial Neural Networks \\ \& Deep Learning}

Even though deep learning is often viewed as a new technique, in reality the recent breakthroughs are underpinned by decades, if not centuries of related research. For instance, earliest artificial neural networks, such as Rosenblatt's Perceptron \cite{Rosenblatt1958} from the 1950s, are closely related to linear regressions dating back to Gauss \cite{JurgenSchmidhuber2015}; these models are all of the form $f(\mathbf{x}, \mathbf{w})=\mathbf{x}^t \mathbf{w}$ where $\mathbf{x}$ is a vector describing some input covariate and $\mathbf{w}$ model weights. Much like in linear regression, the aim is to learn a mapping $f(\mathbf{w}, \mathbf{x}) = y$ that defines the relationship between the input $\mathbf{x}$ and some category $y$ it belongs to. In a simple problem,  $y$ would be binary and the weights $\mathbf{w}$ would be learnt such that the resulting hyperplane could linearly separate the data into the two classes based on the input features. These early models were largely restricted by their assuming linear separability and there was also no computationally feasible way of training the models at the time \cite{Goodfellow2016}, \cite{JurgenSchmidhuber2015} but they form the basis for nearly all Artificial Neural Networks (ANNs). 

Several steps were taken to increase to complexity of these models and to allow for non-linearities, many of them initially inspired to some degree by biological neurons \cite{Goodfellow16}. For instance, real neurons typically only fire once action potential exceeds a specific threshold \cite{Hodgkin1990}. In ANNs, a reminiscent behaviour is attained via activation functions on top of a neuron outputs, most typically in the form of Rectified Linear Units (ReLU), which apply the following non-linearity: $f(\mathbf{x}, \mathbf{w})=max\{0, \mathbf{x}^t \mathbf{w}\}$. ReLUs also improve the representational capacity of a network by imposing sparsity which can make it easier to disentangle the data \cite{Glorot2011} and hence lead to faster convergence of training \cite{Krizhevsky2012}.

Another insights borrowed from neuroscience was that intelligence stems from groups of neurons acting together rather than from the behaviour of individual neurons \cite{Goodfellow2016}; this idea is behind deep ANNs where several layers of neurons are connected to each other and numerous neurons are present in each layer. Below equations and Figure \ref{fig:mlp} give a simplified example of an ANN with two hidden layers:

\begin{align} \label{mlp_eq}
  \mathbf{H_1} &= max\{0, \mathbf{W}\mathbf{X} + \mathbf{B}\} \\
  \mathbf{H_2} &= max\{0, \mathbf{V}\mathbf{H_1} + \mathbf{C}\} \\ 
  \mathbf{F} &= \mathbf{Z} \mathbf{H_{2}} + \mathbf{D} 
\end{align}
where $\mathbf{X}$ is an $D\times N$ input data matrix that holds the $N$ different observations in columns and $D$ is the dimension of a single observation, which for image data is often a flattened array of the image pixels. This matrix representation of input allows several datapoints to be fed through the network simultaneously, which is what is done in practice. $\mathbf{W}$, $\mathbf{V}$ and $\mathbf{Z}$ are the weights matrices of the two hidden layers and output layer respectively. The number of rows in each of these matrices is the number of neurons in that layer, whilst the number of columns corresponds to dimensionality of output from the previous layer. Notice also that constant bias matrices $\mathbf{B}$, $\mathbf{C}$, and $\mathbf{D}$ are added to the neuron outputs. These bias matrices are akin to intercepts in regression analysis and increase the representation ability of the model. Usually all the columns of the matrices are identical such that the same bias values are added to each input observation. We can see that mathematically these matrix operations are just affine transformations on the input, followed by ReLUs, which are applied elementwise. The resulting output of each neuron is thus a matrix, here $\mathbf{H1}$ and $\mathbf{H2}$, where each row corresponds to an output from a specific neuron, calculated separately in the columns for each input vector. Notice also that ReLUs are not added on the output, rather the output layer transforms the representations of the hidden layer into a desired shape of output. For instance, for binary classification $\mathbf{F}$ would be of $2 \times N$, and the two output values for each input would reflect the relative likelihood of the two labels.

\begin{figure}[h!]
  \centering
	\includegraphics[width=0.8\textwidth]{mlp}
	\caption{A simple Feed Forward ANN with two hidden layers. This figure illustrates a graphical model for Equation \ref{mlp_eq} - 3.3 as well as explicitly showing the matrix operations performed by the different neurons on a single vector input. The output dimension is set arbitraily to be a $2\times1$ vector, which could for instance be used as class scores in binary classification; in practice, the dimension will be problem dependent.}
	\label{fig:mlp}
\end{figure}

The ANN model we have described thus far is known as Feedforward Network or a Fully Connected Network, which refers to all neurons being connected to all other neurons in the preceding and succeeding layers. This is an important point as it fascilitates a hierarchical structure in which neurons in the later layers may combine features from earlier layers to create more complex features in turn. Relatedly, this architecture enables distributed representation in which several types features can be combined in different ways to represent an exponential number of different inputs \cite{Hinton1985}. As an example, if we had squares, rectangles and circles and each of them could be either red, green or blue, then we have 9 possible visual objects, yet all the possibilities could efficiently be represented by combining three color neurons with three shape neurons \cite{Goodfellow2016}. \textbf{see goodfellow ch 15}.

\subsection{Training ANNs}

In a typical classification problem we have $n$ possible labels for each input and wish to predict a probability distribution over them. In these applications the outputs of ANN are transformed into 0 to 1 range. More formally, consider that the output from above ANN for a single input $\mathbf{x}$ is the $n \times 1$ vector $\mathbf{f}$; we then require, that each element of $\mathbf{f}$ is between 0 and 1 and that $\sum_1^n \text{f}_i = 1$. The most common way of doing this is to assume that the output of the ANN are unnormalized (predicted) class log-probabilities, that is $f_i = \log \hat{P}(y = i | \mathbf{x})$ \cite{Goodfellow2016}. By taking exponents and normalizing across possible labels predicted class probabilities are calculated as per below - this is known as the softmax function:

\begin{align} \label{softmax_eq}
  \text{softmax}(\mathbf{f})_i = \frac{\exp (f_i)}{\sum_j \text{exp}(f_j)}=q_i
\end{align}

where $q_i \in [0,1]$ captures the predicted probability that the input into the ANN belongs to class $i$. One reason for the softmax layer's popularity is that it is easily compatabile with the cross entropy loss function defined as $L_i=-\sum_i p_i \log q_i$ \cite{Shannon1948}. In simple image classification tasks where the classes are mutually exclusive we have $p_i=0 \forall i\ne C$ and $p_i=1$ for $i=C$ denoting the correct class. Plugging Equation \ref{softmax_eq} into this equation gives:

\begin{align} \label{XEloss}
  L_i  = -\log \left(\frac{\exp (f_C)}{\sum_j \text{exp}(f_j)}\right) = -f_C + \log\left(\sum_j \text{exp}(f_j)\right)
\end{align}
which is the loss incurred from one observation, and is clearly continuous and differentiable. The sequence of computation leading from model inputs all the way to the output of scalar loss is known as the forward pass. Usually forward pass is computed simultaneously for several inputs, known as a mini-batch, in which case the average loss across the mini-batch is typically used. 
The aim of training an ANN is based on learnng model weights that minimize some appropriate loss function, such as the cross-entropy loss above. Most supervised deep learning models, including the basic feedforward-network described above, are nowadays trained using the back-propagation algorithm \cite{Linnainmaa1976} \cite{Rumelhart1985}. The main idea of this algorithm is to use the chain rule to decompose the gradient of a loss function so that it can be efficiently passed back through the network. More formally, assume the loss of an ANN is produced by a sequence of $m$ nested operation:

\begin{align} \label{bp_eq1}
  L(y_i, x_i) = f^{(m)}(y_i, f^{(m-1)}(\dots f^{(2)}(f^{(1)}(x_i))))
\end{align}
where $y_i$ is the real label of the observation, $x_i$ the input data, the different $f^{(i)}$ may for example represent different types of layers. Employing the chain rule recursively, a simple decomposition gives:

\begin{align} \label{bp_crule}
  \frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial f^{(m)}}\frac{\partial f^{(m)}}{\partial f^{(m-1)}} \dots \frac{\partial f^{(1)}}{\partial x_i}
\end{align}

These computations are done in the opposite order of the forward-pass operations; hence back-propagation. Above example is simplistic since usually in addition to inputs from preceding layer, each layer has also its own paramets which can be also be thought as inputs to that layer. The general idea of the chain rule still works in that situation as well, now however the gradient flow bifurcates at each layer; part of gradients flow into tha earlier layer while the others flow back into parameters; this is explained in more depth in Figure \ref{fig:backprop}. By representing ANNs as computational graphs, and using the chain rule similar to above, it becomes easy to see how backpropagation can be used to send appropriate gradients to right places even in complex network architectures. Importantly, backpropagation does this efficiently since at each operation all upstream gradients are collated and then passed on to one layer down, which is a lot more efficient than considering every single path through an ANN individually. To see this, consider Figure \ref{fig:mlp}: using back-propagation we can start from the output and, for instance, calculate the derivative of the output layer with respect to $H5$ neuron only once, and then pass this derivative to all neurons $H1$ - $H4$ simultaneously, which requires a lot less computation than considering all the paths that involve $H5$ separately. 

\begin{figure}
  \centering
	\includegraphics[width=\textwidth]{backprop}
	\caption{A simple example of forward (top) and backward passes (bottom) for a simple two-layer ANN, which illustrates how gradients flowing to early layers are calculated by multiplying the local gradient of a given layer with the gradient that flows back from the next layer}
	\label{fig:backprop}
\end{figure}

After gradients are calculated using back-propagation, they are used by an optimization algorithm to change the model's parameters with the aim of minimizing the loss function. Here we consider stochastic gradient descent (SGD) \cite{Robbins1951} which is likely the most widely used optimization algorithm in deep learning. This algorithm is called stochastic because at each learning iteration only a subset, known as mini-batch, of the data is use to calculate gradients and to perform parameter updates; it has been shown that SGD converges much faster than calculating gradients always on all available data; the time per update for the algorithm is independet of size of data as long as batch size is held constant \cite{Goodfellow2016}. The parameter updates are based on moving 'downhill' i.e. in the direction of negative gradients:

\begin{align} \label{sgd_eq}
  \pmb{\theta}_{(t+1)}=\pmb{\theta}_{(t)} - \eta \nabla_{\theta_{(t)}}L(\mathbf{X}, \mathbf{y})
\end{align}

Most deep learning models are very sensitive to the choice of learning rate $\eta$; if it is too high, we are likely to miss minima and conversly there is a risk of local minima and slow training time when the parameter is set too small. Usually learning rate is reduced linearly with training, or in bigger steps at regular intervals, to reduce the impact of noise when we are near a minimum \cite{Goodfellow2016}.

Another common addition to the vanilla SGD is momentum \cite{Rumelhart1985}, which can accelerate learning when there is a lot of noise from SGD or when the Hessian of the loss (matrix of 2nd order derivatives) is ill-conditioned as shown in Figure \ref{fig:momentum}. SGD with momentum ammends the original SGD by introducing a velocity term that accumulates gradients from previous iterations with an exponential decay. Rumelhart and Hinton \cite{Rumelhart1985} described momentum as if dropping a ball-bearing on loss surface and letting momentum drive the ball. Further, the loss landscape can be imagined to be immersed in a liquid with a specified level of viscosity that defines how quickly the momentum fades. Algorithm \ref{alg:sgd_mom} gives an example of full SGD momentum algorithm for learning model parameters. In practice, back-propagation and optimization can nowadays be done automatically by modern deep learning libraries such as PyTorch \cite{Paszke2017} and Tensorflow \cite{Abadi2015}. 

\begin{figure}
  \centering
	\includegraphics[width=0.5\textwidth]{momentum}
	\caption{Example of ill conditioned Hessian and how momentum speeds up learning by accumulating the gradients of previous iterations such that velocity towards centre (lower loss) is established (shown in yellow). SGD without momentum keeps jumpin across the loss surface as if in a downward sloping canyon, but failing to utilize the slope (blue trace). The contours depict different levels of loss that decreases inwards}
	\label{fig:momentum}
\end{figure}

\begin{algorithm}
  \caption{SGD with momentum (following \cite{Goodfellow2016})} \label{alg:sgd_mom}
\begin{algorithmic}
  \Require Learning rate $\eta$, Momentum decay parameter $\alpha$
  \Require Initial parameters $\pmb{\theta}$, initial velocity $\nu$
  \While{Convergence not met}
  \State Sample a minibatch of m observations from training data $\{\mathbf{x}_1, \dots, \mathbf{x}_m \}$ and their
  \State Get corresponding targets from training data $\{\mathbf{y}_1, \dots, \mathbf{y}_m \}$ 
  \State Compute gradient for the minibatch: $\mathbf{g} \leftarrow \frac{1}{m} \nabla_{\theta}\sum_i L(f(\mathbf{x}_i| \pmb{\theta}), \mathbf{y}_i)$
  \State Update velocity: $\pmb{\nu} \leftarrow \alpha \pmb{\nu} - \eta\mathbf{g}$
  \State Updata parameters: $\pmb{\theta} \leftarrow \pmb{\theta} + \pmb{\nu}$
  \EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Convolutional Neural Networks (CNNs)}

So far we have considered only simple feedforward ANNs with fully connected layers. Most of the ground-breaking accomplishments over the past decade in deep learning have however been achieved with convolutional neural networks (CNNs) \cite{Le} of some sort. This is particularly true for computer vision tasks such as image classifiction \cite{JurgenSchmidhuber2015}. In fact, the term deep learning is often used synonymously with CNNs that have a large number of layers.

Unlike fully connected neurons in feedforward networks, convolutional layer has neurons, usually called kernels, which are connected only to some parts of the input it receives from its preceding layer - together many such neurons span the entire input data. Furthermore, there is weight sharing between the kernels to allow for the recognition of a particular feature anywhere in the input space \cite{Lecun2015}; convolution layers are thus particularly suited for data with locally correlated structures such as images. Consider an input image of 225x225, a typical convolution filter may have size 3x3 and, after having been trained on image data, could have learnt feature mapping that represents particular visual feature such as a vertical edge. This filter is then replicated over the entire 225x225 input range so that the model can detect that particular feature anywhere in the image. Usually each layer has a multitude of such kernels to detect different features in the input data. Mathematically, this feature detection corresponds to the convolution operation between input data $X$ and convolution kernels $K$, which for discrete 2D data is given as $S(i, j) = (X \ast K)=(i, j) \sum_m \sum_n X(i-m, j-n)K(m, n)$ \cite{Goodfellow2016} where $i$ and $j$ denotes a particular image pixel location and $m$ and $n$ those of the kernel. Figure \ref{fig:conv} gives a brief toy example to illustrate this process. 
  
\begin{figure}
  \centering
	\includegraphics[width=0.7\textwidth]{conv}
	\caption{A simple example of the computations performed by a single 2x2 convolution kernel on a 4x4 input. Assuming stride length of two squares, the kernel will see each of the four corners and performs a convolution operation on each of them. This is shown explicitly for the bottom left green square. The computation is hence essentially a dot product similarity metric between the kernel and the local image areas.}
	\label{fig:conv}
\end{figure}

The reason why convolutions have proved so effective for image data is that natural visual scenes present strong local correlations - the world we view is not just a collection of randomly ordered pixels. Additionally, these visual features can appear at essentially anywhere in our visual scenes; thus the need for convolution layers to scan across the whole image \cite{Lecun2015}. Further, CNNs usually have several layers of convolutions on top of each other. When such deep CNNs are trained on image data, the kernels across the different layers usually learn a hierarchy of visual features, such that the early layers detect edges and corners, which are then used by middle layers to create contours and parts of objects, and finally later layers build up more complex representations, possibly of complete objects \cite{Zeiler2014}. This shares some similarities with mammals' visual cortex \cite{Cadieu2014} in which earlier (V1) cells respond to edges and bars of different orientations \cite{Hubeld1962} and later ones like V4 and the IT cortext to more complex shapes \cite{Kobatake1994}. This idea of distributed hierarchical representations is crucial to our research. First, the abiliy to learn fundamental visual features, such as edges, should help to generalize models to novel visual scenes. Second, generalization is also improved by hierarchy of features: for example, if a CNN only trained on images of cats and apples, is shown a picture of a dog, it would be more likely to classify it as a cat than apple which is arguably the closer of the two. In the next chapter we show how an extension of this idea can be explotited to perform more accurate predictions for previously unseen classes of objects.

Convolution layers in CNNs are normally followed up by max-pooling functions. Usually this is just the $max()$ function applied to a grid output of its preceding convolution output. For example, in Figure \ref{fig:conv} the max-pooling would only pass through the value of 25. The benefit of such down-samplign is local invariance: visual objects are not completely rigid and by passing on only the largest value, we are likely to produce the same output even if the values of the square changed around a bit \cite{Lecun2015} (imagine a letter that's slightly rotated between different views).

One of the most important features of CNNs is that they can be trained end-to-end using back-propagation and SGD. A consequence of paramount importance is that the type of visual features which convolution kernels learn to represent is fully determined by what best fits the data, and they can be learned directly from raw data. This is in sharp contrast to earlier computer vision image classification that usually involved feature extraction techniques \cite{Prince2012} such as SIFT \cite{Lowe1999} in which manually designed features are used. These features would then be passed to a separate classifier, for example Support Vector Machine \cite{Weston2010}. CNNs with their automatic feature learning capability produced very large accuracy gains on this apprach \cite{Razavian2014} and following the breakthrough performance of the seminal AlexNet CNN \cite{Krizhevsky2012} at the ImageNet 2012 competition, deep CNNs of various designs have become state-of-the-art in virtually all machine vision classification and detection tasks \cite{JurgenSchmidhuber2015}.

In addition to convolution and pooling layers, CNNs involve several other important architectural designs. For example, deep networks with a large number of convolution and pooling layers have been found to perform better \cite{Srivastava2015} than shallow ones. This is exploited by several CNN models such as the ResNet architectures which can have more than 100 layers \cite{He2015}. Theoretically, depth increases the representational capacity of the network \cite{Sun2015}. Representational capacity is also increased by the use of ReLUs on top of convolution layers. All in all, modern deep CNNs can easily have hundres of millions of parameters and the ability to learn super-human performance on many image classification tasks \cite{Lecun2015}. Due to their large representational capactiy, these models can also easily overfit training data and the development of appropriate regularization techniques has played a large role in CNNs recent achievements. Perhaps the most popular regularization technique, and the one applied in this work, is DropOut \cite{Srivastava2014}. During model training, dropout deactivates a each neuron in each iteration with a probability $P$. The consequence of this is that over the entire training period we essentially end up training an ensemble of different models, and the final model is an average of exponentially many sub-models \cite{Goodfellow2016}. This method regularizes the network as individual neurons can not rely too much on any other neurons and the output expected to get out of them. Durng test-time, dropout is turned off.

Many of the theoretical concepts of modern deep CNNs have been around for several decades \cite{JurgenSchmidhuber2015}. The reason for their recent surge in popularity is to a large extent that they have become a lot easier and faster to train as our computers have gotten more powerful and our datasets much larger. In particular, the ability to train deep models on Graphical Processing Units (GPUs) has cut training times by at least an order of magnitude \cite{Lecun2015}. However, we have only been able reap the benefits of faster compute and better models because of 'big data'. It has been suggested that CNNs with around 5000 labels per training category can on average start to match human performance \cite{Goodfellow2016} - over the past decade we have seen the advent of datasets that have up to tens of millions of training examples \cite{Russakovsky2015i}, \cite{JiaDeng2009}, \cite{Netzer2011}.

\section{Exploiting Semantic Information in \\ Computer Vision}

\subsection{Semantic Hierarchies and \\ the Human Visual System}
Upon observing almost any visual scene, humans are able to effortlessly cut it up into segments of distinct objects \cite{Rosch1976}, even if we have never seen the scene before. This is a remarkable ability considering that the world we live in can present us with an infinite amount of visual variation to our retinas and yet we are able to easily recognize tens of thousands of distinct object categories \cite{Biederman1989} with invariance to various factors such as movement, lighting, shade, orientation and partial occlusion \cite{DiCarlo2012}. Rosch \textit{et al.} \cite{Rosch1976} argue that this is because of the non-random, and hierarchical, structure of visual objects, for instance no breed of dogs will have wings but they will often share many features such as four legs with certain type of paws. In general entities that are closer to each other in a hierarchical semantic taxonomy will also typically share more visual features; dogs and birds are still more alike than dogs and vehicles since both belong to a higher level category of animals which all share certain visual attributes. Deselaers and Ferrari \cite{Deselaers2011} veried these observations in their study on the visual separability of different semantic categories using the ImageNet dataset.

Standard machine learning and deep learning models used for image classification do not exploit this semantic taxonomy since they are typically trained against one-hot encoded target vectors. For example, the 1K ImageNet data set \cite{JiaDeng2009} will have labels as 1000-dimensional vectors where there is 1 for the dimension indicating the correct class and zeros elsewhere. It follows that label vectors for different classes are thus orthogonal and normally a model trained on this data will therefore not account for any classes belonging to a common category at a higher level of the taxonomy. For example, in reality two different breeds of dog do fall under the same higher level categories of dogs, animals, mammals and so forth - all of these share certain amount of visual features. The chosen level of hierarchy of a one-hot encoding is also usually arbitrary; should an image be labelled a dog or a labrador retriever? Optimally both would be taken into account, we argue. The number of machine learning algorithms that do take this into account is limited, however; below we give an overview of them.

\subsection{Semantic Hierarchies for Image Classification}
The use of semantic hierarchies in computer vision dates back to early work on image retrieval \cite{Aslandogan1997}, \cite{Zhao2001}, \cite{Grosky2002} \cite{Barnard2001}, where it has been used to expand potential query terms and to impose a more general association between image features and corresonding labels. These ideas were subsequently utilized to improve image annotation and classification \cite{Srikanth2005}, \cite{Marszaek2007} \cite{Griffin2013}. For example, Marszalek and Schmid \cite{Marszaek2007} use the WordNet database to create a lexical hierarchy of their image labels and train an SVM classifier \cite{Scholkopf2002} that acts at each node of the hierarchy. The resulting model proves very flexible and performs well under uncertainty - if the model is unsure about what dog breed is in an image, it can then move up one level and just predict a 'dog'. It should be noted that the dataset used by the authors is very simple by today's standards, however. More recently, Redmon and Farhadi \cite{Redmon} employed a similar idea but in the context of deep learning. In particular, for each image, its label was taken to be the full path from the root node of the WordNet tree to the originl singular label. The model was then trained with multiple softmax functions, one for each level of hierarchy in order produce a chain of conditional probabilities from the root node to leaves. The authors' purpose for building this model was to learn simultaneously from two datasets of unequal hierarchies and to jointly optimize classification and detection (locating an object) on the two datasets. However, other possible benefits of such a flexible model, like generalization to new classes, were not explored in detail. 

In their research on human categorization, Rosch \textit{et al.} \cite{Rosch1976} defined the concept of 'basic level'. This is described as the most fundamental level of a specific category in the semantic taxonomy and is the one first learnt usually by children and also objects are identified the quickest at this level. The level below basic level is called 'subordinate level'. An example of this structure would be the basic level label of 'fox' and its subordinate 'arctic fox'. The importance of the basic level seems to stem from this level having the highest ratio of visual variation between categories to variation within categories \cite{Rosch1976}, \cite{Joliceur1984}. It results that it has been especially practical for humans to identify and label objects at this level. Hillel and Weinshall \cite{Hillel2007} implemented the idea of basic and subordinate levels by building a two-stage classifier where the first stage generates a vector representing the different parts of an objects and the second stage classifies instances based on this vector. On average, this strategy outperformed a traditional one-step algorithm. Wang and Cottrell \cite{Wang2015} took these ideas to the deep learning-era an trained a CNN on both basic and subordinate labels of the ImageNet 2012 data \cite{Russakovsky2015}. They found that this leads to an improved performance on the standard top-5 classification accuracy. Another simiar study \cite{Lei2018} explored a dataset which mostly had coarse labeled images (similar to the basic level) and only some fine grained examples. The authors showed that their custom CNN, which used both hierarchies, could predit fine-level labels better than a model that's only trained on fine level. This suggests that the fine-grained model is able to borrow strength from coarse labels and consequently generalize better. Peterson \textit{et al.} \cite{Peterson2018} extended this line of research by exploring the type of representations learnt by a CNN trained on both 'basic' and 'sub-ordinate' labels. First, the authors found that including the basic-level labels in pre-training or fine-tuning led to a much more clustered representation of the feature spaces extracted from the final layer of the CNN (e.g. the features vectors of different breeds of dogs were now bundeled up together whilst if trained on just subordinate labels, then there was no clustering of similar categories). The features also had learn separate hierarchies for natural and man made objects. Finally, they illustrated the generalization power of these representations by running a zero-shot learning experiment in which the model was shown only a few examples of either sub- or basic-level objects and then tries to find all other images from the data set belonging to the given label. Interestingly, the results show that the supplementary basic-level training had led to a bias to label objects at this level which is congruent with corresponding studies in humans \cite{Xu2000}.

Despite the important contribution of above works, they are limited in that they only consider two levels of a semantic taxonomy. It has been shown that, despite the importance of the basic level, humans posses and utilize a much more complex multi-level semantic hierarchy \cite{Joliceur1984}. We are not aware of any deep learning papers that explore the type of representations and the consequent generalization properties that would be learnt by considering a multi-level semantic taxonomy. We attempt to fill this void by exploiting a full set of hierarchical taxonomy via semantic embeddings built on a large-scale lexical database WordNet \cite{Miller1995}.

\subsection{Semantic Embeddings and Zero-shot Learning} \label{sec:semlit}
Above studies illustrate the implicit connection between text and visual data processing in humans. Joliceur \textit{et al.} \cite{Joliceur1984} explored the dependence of our visual system on semantic understanding in more depth and illustrated this with several experiments. For instance, upon viewing a picture of a chair we can use our semantic knowledge base to generalize it to the category of furniture even though the image itself doesn't reveal that such an abstraction even exists. Similary, we can seamlessly access our visual memory to imagine semantic concepts, even ones we may never have seen such as 'a cat wearing ice skates'.

Zero-shot learning (ZSL) \cite{Palatucci2009} is concerned with making predictions about previously unseen classes of images. Formally if a classifier $f$ is trained on features $X_{TR}$ and training labels $Y_{TR}$ such that $f:X_{TR} \rightarrow Y_{TR}$, then in ZSL for the possible class labels we have that $Y_{TR} \cap Y_{ZSL} = \emptyset$ where $Y_{ZSL}$ represents the test labels of any ZSL dataset. A fundamental idea in this area was to exploit the semantic relatedness of visually linked categories \cite{Monay2004}, \cite{Weston2010}, \cite{Palatucci2009} by projecting input data into a lower dimensional semantic embedding space. This can be represnted by the following equations \cite{Palatucci2009} (see Figure \ref{fig:zsl} for a graphical explanation):
\begin{align} \label{zsl_eq}
  \mathcal{S} &: X^d \rightarrow F^P \\
  \mathcal{L} &: F^P \rightarrow Y
\end{align}
Here we assume that we have a full collection of all pairs $\{f, y\}_{1:M}$ of both seen and unseen classes where $f$ is the point in the semantic embedding space $F^P$ that corresponds to the label $y$. In the training phase we can take the data for the seen classes $\{X, y\}_{1:N}$, where $N<<M$, and replace each $y$ with its embedding vector $f$ and thus learn the mapping $\mathcal{S}$. Labels for objects of unseen classes can then be predicted by first mapping them into $F^P$ through $\mathcal{S}$ and then employing $\mathcal{L}$ to find the most appropriate $y$ based on the predicted semantic embedding; this could for instance be a nearest neighbor classifier. The trick is thus having the semantic embedding space $F^P$ to contain vectors for all seen and unseen classes. To be clear, the term 'zero-shot' thus refers to the trained model never having seen $M-N$ of the classes but in our semantic knowledge base we do know about the existence of those zero-shot classes and their names. The challenge is thus in how to construct the semantic embedding space and what type of model to use to learn $\mathcal{S}$; the following paragraphs will cover the existing approaches.

\begin{figure}
  \centering
	\includegraphics[width=\textwidth]{zsl}
	\caption{A graphical illustration of zero-shot learning where the left square contains all input images. Red ones are the training classes whcih are used to learn a mapping $\mathcal{S}:X^d \rightarrow F^P$. The circle represents the semantic embedding space. Note how we can access embedding vector for also classes we dont have visual training data for. Upon seeing a novel image (yellow square) we predict its embedding vector (yellow dot) and then find nearest plausible classe (here fox) using some function $\mathcal{L}$. Novel image is hence classified as a fox even though the model has never seen one before}
	\label{fig:zsl}
\end{figure}

In an early work, Socher \textit{et al.} \cite{Socher} created word embedding vectors for all seen and unseen class labels from Wikipedia text data using a methodology \cite{Huang2012} that placed similar words next to each other in the embedding space. A two-layer feedforward network was used to learn a mapping from image features into the embedding space, and outlier detection was used to determine if the predicted embedding was one of the known or unknown classes. Only 8 known classes and 2 unknown classes were considered however. To address this and other limitations, Frome \textit{et al.} \cite{Frome2013} developed the Deep Visual-Semantic Embedding Model (DeViSE), which was trained on 1000 classes of the ImageNet dataset and its zero-shot recognition abilities were tested on the remaining 20,000 ImageNet classes. The architecture of this model forms the foundation for the model we build in this paper: it takes a pre-trained deep learning model, AlexNet \cite{Krizhevsky2012}, removes the softmax layer and replaces it with a projection layer that attempts to predict the word vector embedding of each image label. The word embeddings, in turn, were extracted from a skip-gram model trained on 5.4billion words of text from Wikipedia \cite{Mikolov2013}, \cite{Mikolov} by place similar words near to each other. This model succesfully used the semantic information to make correct guesses on thousands of unseen classes, and it still provides a good baseline for new methods as we will show in our results section. 

The authors of DeViSE worried, however, that the model was overfitting the transformation from the image representations into the semantic feature space. To overcome this they introduced another model, ConSE \cite{Norouzi2013}. In this model, an image of unknown class is first passed into a pretrained CNN. The CNN will try to classify the image as one of the known classes and outputs corresponding clas probabilities, $p_0 (y | \mathbf{x})$ such that $\sum_{y=1}^{n_{seen}} p_0 (y | \mathbf{x}) = 1$. The predicted semantic embedding for the unknown class image is then taken as the convex combination of the known classes weighed by the predicted probabilities from the previous step. More formally \cite{Norouzi2013}:
\begin{align}
  f(x) = \frac{1}{Z}\sum_{t=1}^T p(\hat{y}(\mathbf{x}, t)| \mathbf{x}) \cdot \mathcal{L}(\hat{y}(\mathbf{x}, t))
\end{align}
where $\hat{y}(\mathbf{x}, t)$ is the $t^{th}$ most likely class-label for image $\mathbf{x}$ out of the known labels, with $p()$ giving the respective probability. $\mathcal{L}$ maps each of these labels deterministically to their embedding. $Z$ is a normalizing consant, and $T$ is user-defined. This model was shown to generalize slightly better to novel classes than DeViSE when both used the same image features and embedding vectors. One contribution of this paper is to show how this model can be adapted into hyperbolic space.

A potential problem with above models, which use Wikipedia text data to create semantic embeddings, is that vectors for low occurence labels may be imprecise. Li \textit{et al.} \cite{} resolve this by trying exploit the full WordNet hierarchy. To do this, they reconstract the semantic embedding for each word by tracing its path to the root of the WordNet tree and calculating an inverse distance-weighted combination of the semantic embeddings along the path. For example, the embedding vector for 'arctic fox' is computed as a combination of the semantic embeddings of the set ('arctic fox', 'fox', 'canine', 'carnivore' \dots 'living being'). Whilst attempting to capture the semantic structure of the WordNet, the embeddings at each node are still based on the Wikipedia data, which does not account for this taxonomical structure. Further, each label only considers its own parent nodes. Consequently the resulting embedding manifold as a whole is unlikely to accurately reflect the entire WordNet graph and its complex relations. In the next section we show how this can be achieved with Poincare embeddings.

Fu \textit{et al.} \cite{Fu2015} identified identified a central issue which maligns most ZSL approaches, namely the projection domain shift: the mapping that is learned from the seen image space into the embedding space may not transfer well to the disjoint set of unseen classes. Transductive multi-view embedding was proposed as a solution by the authors: in addition to the semantic embeddings of the seen classes, the embeddings of unseen classes are also available and can hence be used at training time. In fact, multiple embeddings for each label were created and they were all jointly projected to common embedding space, which alleviated the domain shift problem. Kodirov \textit{et al.} \cite{Kodirov2015} proposed a similar approach: sparse coding is used to map from the unseen labels to the semantic space and this in turn helps to guide the domain projection from seen to unseen visual features. See also Rohrbach \textit{et al.} for similar ideas \cite{Rohrbach2013}. The use of test labels during training may not be practical, however, and as an alternative Kodirov \textit{et al.} \cite{Kodirov2017} proposed a semantic autoencoder. The general idea is to learn one mapping (decoder) from input image features to a latent semantic embedding space and another mapping (decoder) from there back to reconstruct the input data. The addition of the decoder constraints the model to solve the domain shift. Tsai \textit{et al.} \cite{Tsai2016} use two autoencoders, one on just the image data and another on text data, extract the representation layer from each and minimize their mismatch. The advantage of this model is that the unsupervised nature of autoencoders allows both labeled and unlabeled data to be used and thus more robust embedding representations to be learnt.

Several other works have found ways to learn better embeddings. While most approaches learn semantic embedding and the mapping function separately, Ba \textit{et al.} \cite{Ba2015} trained a feedforward network from text features to predict the visual features in the different layers of a CNN and thus learns an embedding that captures smaller visual features more accurately. Ji \textit{et al.} \cite{Ji2017} show how performance can be improved by ensuring that the embedding manifold preserves the local structure of the visual input space, thus encouraging visually similar classes to be closer to each other in the embedding space. For local similarity aware deep embeddings see Huang \textit{et al.} \cite{Huang2016}. Zhang and Saligrama \cite{Zhang2015} build semantic embedding labels for unseen classes as mixture distributions of the embedding vectors of the seen classes. These are employed during test time to find the closest match for the mixture distribution of the test image. In a follow up work the authors \cite{Zhang2015a} provide a latent probabilistic framework in which zero-shot recognition is performed by estimating the posterior probability that the test image matches one of the label vectors. Fu and Sigal \cite{Fu2016} introducedd the idea of using embeddings built on full semantic vocabularies, rather than just training labels, this led to a better mapping between image and semantic embeddings. Shigeto \textit{et al.} \cite{Shigeto2015} show that instead of learning a mapping from images to label space, it can be beneficial to learn this mapping in the opposite direction since nearest neighbor search is often easier in the lower dimensional image space. Zhang \textit{et al.} utilize a similar idea with CNNs. Changpinyo \textit{et al.} \cite{Changpinyo2017} also use this direction; they fist cluster different training class images and for each class take the cluster centre as an exemplar visual vector of that class. Next they utilize a kernel-based regressor to learn mapping from semantic vectors into these exemplars. Zero shot recognition can then be performed by first predicting new visual exemplars for possible unseen class labels and then performing nearest neighbor search in this space to map all unseen images to one of the new exemplar classes. Finding nearest neighbours in high dimensional spaces is a common challenge with Euclidean spaces, which we manage to avoid by working in hyperbolic geometry instead.

Graph based approaches have also been popular in tackling ZSL recently. Fu \textit{et al.} \cite{Fu2015a} note that the vectors embedded in a semantic space often form class manifolds. Motivated by this observation they build a semantic manifold graph out of the labels in the embedding space and create a special form of a markov chain process that predicts the unseen class labels. Changpinyo \textit{et al.} \cite{Changpinyo2016}, on the other hand, consider a weighted graph of semantic embedding for all classes in one manifold and align this with a weighted graph of possible model weights for the visual features in another manifold by projecting graph vertices. So called phantom classes are learned to optimally connect seen and unseen classes in the graphs on the basis of the semantic space, and classifiers for unseen classes are constructed as the convex combination of phantom classes in the model space. The current state-of-the-art \cite{Wang2018a} exploits both explicit knowledge-based relationship between categories from the wordnet graph as well as similarity based semantic embeddings constructed from the Wikipedia text data \cite{Pennington2014}. More specifically, each node in the wordnet graph is made to contain its respective semantic embedding vector. Graph Convolution Network, which builds a multi-layer version of the graph and performs convolution between the nodes, is then used to learn logistic regression classifier weights for each given embedding label. After training, the model can be used to predict logistic regressors for also unseen classes based on their embedding vectors, and thus perform ZSL. The results of the this current state-of-the-art approach highlight the difficulty of the ZSL task: they achieve around 2 \% top-1 accuracy and 12 \% top-20 accuracy on predicting labels for 20,000 previously unseen classes. The approach taken by the authors is similar to ours in that they also try explicitly represent semantic hierarchies, as well as similarities, however in our work both are captured by the embedding space. Further, our approach has the advantage that it can be combined with standard deep learning models, which is important given their popularity.

In addition to above, there exists a vast array of approaches that focus on zero-shot learning via class attributes \cite{Farhadi2010}, \cite{Rohrbach2010}, \cite{Rohrbach2011}, \cite{Hwang2014}, \cite{Akata2016}, \cite{Xian2016}, \cite{Romera-Paredes2017}, \cite{Bucher2016}. A limitation of these approaches is that when the number of classes grow very big, it becomes harder to manage the space of attributes manually. The set of attributes is also hard to transfer to new context \cite{Ba2014} with different attributes. This is why our work will focus on text and language based semantic embeddings. More importantly, attribute based ZSL often aims to predict fine-grained classes that are visually very similar \cite{Xian2016} \cite{Reed2016}. We on the other hand focus on building a framework that is robust in predicting the higher level classes and structures of novel visual objects, rather than their fine details.

Finally, there exist various studies that combine language and attribute based semantic embeddings \cite{Akata2015}, \cite{Fu2015} \cite{Zhou2005}. Akata \textit{et al.} \cite{Akata2015}, for instance, combine three embeddings, one learnt from the Wikipedia dataset, one for physical attributes of animals and a hierarchical embedding, which attempts to capture the taxonomical structure of the WordNet using various distance measures such as the shortest path between nodes. The model performs well on zero-shot prediction of fine-grained classes on a relatively small data set; it's generalization properties on a large data set such as the 21,000 label ImageNet are not explore however. A more comprehensive review of different ZSL approaches and trends is found in \cite{Xian2017}.


\section{Semantic Embeddings in Hyperbolic Space}

\textbf{Section overview:} As alluded to in the previous chapter, hyperbolic spaces provide an efficient way of embedding hierarchical graph structres. In this chapter we provide a brief overview of the required theoretical background and discuss the recent literature on building word embeddings in this space. Hyperbolic geometry and hyperbolic spaces are a vast fields containing thousands of years of work and formal statements and proofs are thus out of the scope of this work; we merely aim to provide the background necessary for the present work. For more thorough treatment of the topic, we refer the reader to Cannon \textit{et al.} \cite{} and Greenberg \cite{Greenberg1994}; unless where otherwise stated, all the theories and intuitions provided below are based on these two books.

\subsection{Basics of Hyperbolic Geometry}
Hyperbolic geometry is a type of non-Euclidean geometry in that it is derived by altering one of the five postulates of the classic Euclidean geometry, namely the fifth one:
\begin{enumerate}
    \item Any pair of points can be joined up by stricly one staight line segment
    \item All lines can be extended indefinitely
    \item Exactly one circle of a specific circle and specific centre exists
    \item All righ angles are identical
    \item Parallel postulate: Consider line L and a point P which does \textit{not} lie on L. Then there exists only one other line M that passes through P and is parallel to L, no matter how much we extend the two lines.
\end{enumerate}

In hyperbolic geometry the fifth postulate becomes its negation: 

\begin{enumerate}
   \setcounter{enumi}{4}
   \item * Consider line L and a point P which does \textit{not} lie on L. For any L, ther are infinitely many lines that are both parallel to L and pass through P.
 \end{enumerate}

\begin{figure}
  \centering
	\includegraphics[width=0.5\textwidth]{pdisklines}
	\caption{Parallel lines in Hyperbolic space (Poincare disk model). Thick blue lines is parallel to all the other lines since it never intersects any of them in the hyperbolic space that is defined as interior of the circle. All the lines are arcs of circles of different size that fall in the interior - these are known as geodesics. Image used with permission under Creative Commons license, released into public domain by Trevorgoodchild of English Wikipedia}
	\label{fig:disklines}
\end{figure}

\begin{figure}
  \centering
	\includegraphics[width=0.5\textwidth]{hsaddle}
	\caption{Saddle point in Euclidean space allows a representation of a hyperbolic triangle, for which the sum of its angles is less than 180 degrees. Image used under Creatie Commons license from Wikimedia Commons.}
	\label{fig:hsaddle}
\end{figure}

For us who are accustomed to thinking in Euclidean terms, this may seem impossible, at first. This becomes possible when we consider spaces of constant negative Gaussian curvature, which indeed is the fundamental property of hyperbolic spaces (Figure \ref{fig:disklines}). For instance a hyperbolic plane is a 2-dimensional manifold with constant negative curvature; similar logic holds in higher dimensions too \cite{Stillwell1991}. As long as lines in hyperbolic space never intersect each other, they are parallel. Straight lines in hyperbolic space are more precisely known as geodesics - arcs of circle, and locally they define the shortest path between two points in a curved space \cite{Stillwell1991}. It is illustrative to consider the shortest path between two locations on the globe, say new york and london; this too is an arc of the great circle of the earth circumference, rather than a straight line (though in this instance it is a result of the positive curvature of the earth). For an improved, intuition of hyperbolic geometry, it should be noted that all points in a hyperbolic space can be viewed as saddle points of euclidean geomtry (Figure \ref{fig:hsaddle}). There are several ways of constructing hyperbolic spaces from Euclidean geometry; the only requirement is that aforementioned necessary axioms are satisfied. Below we define the most important models.

\subsubsection{The Beltrami-Klein Model}
Consider a circle $\gamma$ of the Euclidean plane and its centre $O$, radius $OR$ (see Figure \ref{fig:klein}). The hypebolic space is then defined as the interior of all points $X$ for which $OX < OR$. Lines in this model of hyperbolic space are open chords of $\gamma$, with open referring to a chord without its end points. This model satisfies the requirement of hyperbolic space since if we have line $l$ and a point not on it $P$, then there are infinite number of parallel lines of $l$ that go through $P$. Example is given in the RHS of Figure \ref{fig:klein}. The reason why $m$ and $n$ in this figure are parallel to $l$ is that they are open chords in $\gamma$ and at no point of this space do $m$ and $n$ interect $l$, which is the working definition of parallelism \cite{Greenberg1994}. It is crucial to notice that only the interior of the circle forms the hyperbolic space and the boundary can never be reached, hence the \textit{open} chords as lines. This combined with the requirement that lines can be extended indefinitely means that we can get arbitrarily close to the boundary of the circle but never reache it. As a consequences the distance between two points on the line can not be computed as euclidean distance as it grows exponentially the further away from the centre we get, which is not visibly evident from the figures \cite{Greenberg1994}. Notice also that the appearance of lines as straight is just a matter of mapping them from geodesics, such as those in Figure \ref{fig:disklines}. 

\begin{figure}
  \centering
	\includegraphics[width=0.6\textwidth]{klein}
	\caption{Depiction of the Klein model of hyperbolic geomtry.}
	\label{fig:klein}
\end{figure}

We can extend this model to any higher dimension $n$ by re-defining the disk as $\mathcal{K}^n = \{\mathbf{x} \in \mathbb{R}^n | \lVert \mathbf{x} \rVert <1 \}$, and taking all other axioms from above. Distance between two points $\mathbf{p}$ and $\mathbf{q}$ is defined as \cite{Iversen1992}:
\begin{align}
  \mathcal{D_K}(\mathbf{p}, \mathbf{q}) = \text{arcosh} \left( \frac{1 - \mathbf{p} \cdot \mathbf{q}}{\sqrt{1-\mathbf{p}\cdot \mathbf{p}}\sqrt{1-\mathbf{q}\cdot \mathbf{q}}}   \right) 
\end{align}

\subsubsection{The Poincare Model}
In the Klein model arcs were mapped to straight lines. An alternative representation of a hyperbolic plane is the Poincare model. Here two types of lines exist: open chords that pass through the origin of the circle, which are like those in the Klein model, and arcs of all the circles orthogonal to $\gamma$ i.e. \textit{open} arcs (Figure \ref{fig:pdisk}) \cite{Greenberg1994}. The other statements about Klein model transfer here; the boundary can never be reached and distances grow exponentially as we approach it. Again an infinite number of parallel lines exist; this was visualized in Figure \ref{fig:disklines}.

\begin{figure}
  \centering
	\includegraphics[width=0.4\textwidth]{poincdisk}
	\caption{Depiction of the Poincare Disk model of hyperbolic geomtry. Line $l$ represent an open chord passing through the centre of the circle, whilst line $l$ is an open arc.
}
	\label{fig:pdisk}
\end{figure}

To extend this formally into $n$ higher dimensions we can define the hyperbolic space as an open unit ball $\mathcal{B}^n$ as we did for the Klein model $\mathcal{B}^n = \{\mathbf{x} \in \mathbb{R}^n | \lVert \mathbf{x} \rVert <1 \}$, but as straight lines are arcs (due to negative curvature), we have different distance function \cite{Iversen1992}:
\begin{align}
  \mathcal{D_P}(\mathbf{p}, \mathbf{q}) = \text{arcosh} \left(1 + 2 \frac{\lVert \mathbf{p} - \mathbf{q} \rVert^2}{(1-\lVert \mathbf{p}\rVert^2)(1-\lVert \mathbf{q}\rVert^2)}   \right) 
  \label{eq:pdist}
\end{align}

Whilst the Klein and Poincare model appear different, they both obey all the same axioms of hyperbolic spaces, in fact, the models are \textit{isomorphic}: there is a one-to-one corresondence between points between all the points $P$ and $P'$ of the two models as well as one-to-one correspondence between $l$ and $l'$ of the models such that $P$ lies on $l$ if and only if $P'$ lies on $l'$ \cite{Greenberg1994}. It can be proven that all models of hyperbolic geometry are isomorphic \cite{Greenberg1994}. Therefore it is possible map from one model to another. We will employ the mapping from Poincare ball into the Klein model at a later stage and thus present the required equation here \cite{Greenberg1994}:

\begin{align}
  \mathbf{s} = \frac{2\mathbf{u}}{1 + \lVert \mathbf{u} \rVert^2}
  \label{eq:p2k}
\end{align}
where $\mathbf{s}$ is the point in the coordinates of the Klein model and $\mathbf{u}$ in the Poincare model.



\subsubsection{The Hyperboloid Model}
This model stems from the theory of special relativity. If we denote $x$, $y$ the 2-dimensional coordinates and $t$ as time, then distance in is measured by the Minkowski metric \cite{Greenberg1994}:
\begin{align*}
  ds^2 = dx^2 + dy^2 - dt^2
\end{align*}
The surface equation that corresponds to this metric is given by below equation. See \cite{Greenberg1994} for details.
\begin{align*}
  x^2 + y^2 - t^2 = -1 
\end{align*}

In Euclidean space, this forms a hyperboloid object with two sheets, with the upper sheet shown in Figure \ref{fig:lorenz} \cite{Greenberg1994}. In that figure we also show how the model relates to the Poincare disk we saw above. The 'bowl' of the hyperboloid is infinite so the 'brim' can never be reached, thus satisfyin the indefinite extensibility of lines which are geodesics on the hyperboloid surface.

\begin{figure}
  \centering
	\includegraphics[width=0.6\textwidth]{lorenz}
	\caption{Upper sheet of the hyperboloid model in $\mathbb{R}^3$ and a projection of one of its geodesics on to the Poincare disk. While not possible to visually illustrate, the brim of the 'bowl' is at infinity and can never be reached. Image used under Creative Commons license from Wikimedia Commons.}
	\label{fig:lorenz}
\end{figure}

In higher dimensions, the $n$ dimensional hyperbolic space is defined as $\mathcal{H}^n = \{\mathbf{x} \in \mathbb{R}^{n+1} | \mathbf{x} * \mathbf{x}=-1 \}$ where $\mathbf{x} * \mathbf{x} = \sum_{i=1}^n x_i x_i - x_0 x_0$ is the Lorenzian inner product \cite{Nickel2018} \cite{Cannon}. Distance function is:
\begin{align}
  \mathcal{D_H(\mathbf{p}, \mathbf{q})} = \text{arcosh}(-\mathbf{p} * \mathbf{q})
\end{align}

\begin{figure}
  \centering
	\includegraphics[width=0.6\textwidth]{tiling}
	\caption{Illustrations of Klein and Poincare Discs with hyperbolic tiling. All the tiles have equal area so this visually depicts how the area of the circles grows exponentially towards the boundary. Images used under Creative Commons license from Wikimedia Commons.}
	\label{fig:hyspace}
\end{figure}

\subsection{Semantic Embeddings in Hyperbolic Spaces} \label{sec:poincare}
Most popular word embedding methods such as the skip-gram and word2vec \cite{Mikolov2013}, \cite{Mikolov} taken into account the local context of words, or global context like Glove \cite{Pennington2014}, and produce vector representations for the words so that semantically similar words are embedded near each other in the resulting vector space. The ability of these embeddings to capture complex relationships is constrained, however, as the required dimensionality becomes easily exceedingly large \cite{Nickel2017} \cite{Chamberlain2017} \cite{Nickel2018}. This limit on representational capacity is particularly relevant to our project as we aim to build an embedding space that accurately captures the hierarchical graph structure of word semantics. The idea of using hyperbolic spaces for representing semantic hierarchies more accurately was explored early on by \cite{Ritter1999}, \cite{Ontrup2002} and \cite{Walter2006}, though this was in the context of Self-Organizing Maps rather than word embeddings. More recently, Nickel and Kiela \cite{Nickel2017}, Chamberlain \textit{et al.} \cite{Chamberlain2017} have succesfully demonstrated the advantages of hyperbolic word embeddings over Euclidean ones in terms generalization performance and computational cost. Additionally, Gulcehre \textit{et al.} \cite{Gulcehre2018} have achieved success on various tasks, such as neural machine translation, by imposing hyperbolic spaces on the activation functions of their novel neural network model.

To understand the benefits of hyperbolic word embeddings, consider the Poincare and Klein disk models and contrast this to an Euclidean disk. The area of an Euclidean disc increases quadratically, since $A_E=\pi r^2$, whereas the negative curvature of hypebolic space allows the hyperbolic disc area to grow exponentially with $r$ as given by $A_H=2 \pi \left(\text{cosh}(r) - 1 \right)$ (assuming constant curvature of 1) where cosh is the hyperbolic cosine function $\text{cosh}(x)=\frac{e^{2x} + 1}{2e^x}$ \cite{Greenberg1994}. Intuitively the benefit of this is that in hyperbolic spaces there is much more room for embedding different concepts as we move outwards from the centre (see Figure \ref{fig:hyspace}). Further, Hyperbolic spaces can be interpreted as continuous space extensions of trees and are thus particularly efficient for embedding hierarchical structure in a way that preserves the associated graph structure, even for infinite trees \cite{Chamberlain2017}; like hyperbolic space, the number of leaves of a tree also grows exponentially as we increase hierarchical levels (assuming the number of splits per level remains constant) \cite{Nickel2017} (See Figure \ref{fig:htree} for a naive example). 

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{htree}
	\caption{Naive example of embedding a hierarchical semantic taxonomy in a hyperbolic disc. All the concepts at each level of hierarchy are equidistant from the centre of the disk as portrayed by the dotted circles.}
  \label{fig:htree}
\end{figure}

The hyperbolic embedding that we utilized is the Poincare Embedding model of Nickel and Kiela \cite{Nickel2017}, mainly because they show its effectiveness on the Wordnet lexical hierarchy which is the basis for the labels in the ImageNet dataset that we used. Here we give a brief overview of the way in which the poincare embeddings are actually computed in this model; rest of this section is thus entirely based on \cite{Nickel2017}.  To use the original notation of the paper, let $\mathcal{S} = \{x_i\}_{i=1}^n$ denote the set of words, and $\Theta = \{\boldsymbol{\theta}_i\}_{i=1}^n$ be their repsective locations in the $d$ dimensional Poincare embedding space $\mathcal{P}^d$ i.e. $\boldsymbol{\theta}_i \in \mathcal{P}^d$. To find embeddings, the authors set to solve the following optimization problem:
\begin{align*}
  &\Theta' \leftarrow \text{arg}\,max_{\Theta} \mathcal{L}\left(\Theta\right)\text{  s.t.} \forall \boldsymbol{\theta}_i \in \Theta : \lVert \boldsymbol{\theta}_i \rVert < 1
\end{align*}
where the final inequality is needed to ensure the vectors fall inside the open unit ball. And their loss function is a ranking loss:
\begin{align*}
  &\mathcal{L} \left(\Theta\right) = \sum_{(u,v) \in \mathcal{D}} \log \frac{\exp^{-d( \mathbf{u},\mathbf{v})}}{\sum_{\mathbf{v'}\in N(u)}\exp^{-d(\mathbf{u},\mathbf{v'})}}
\end{align*}
where the tuple $(u,v)$ represents the nouns that are connected to each other in the wordnet graph, $\mathbf{u}$ and $\mathbf{v}$ their estimated embedding vectors, and $N(u)$ those nouns $v$ not actually connected to a given $u$. Finally, $d(\mathbf{u}, \mathbf{v})$ is the hyperbolic distance betwen the nouns. Since this embedding is based on the Poincare model, the relevant distance function is the one in Equation \ref{eq:pdist}. A point of emphasis is that the model is not given any explicit hierarchical information but it learns it from the connection between the nouns in the wordnet, thus enabling the ability to learn latent hierarchies. 

Above optimization problem could be solved using Riemannian optimization, but the authors show that the Riemanian gradients can also be obtained in terms of Euclidean gradinets $\nabla_E$, resulting in the following SGD updates:  
\begin{align*}
  \boldsymbol{\theta}_{t+1} = \text{proj} \left( \boldsymbol{\theta}_t -\eta_t \frac{(1-\lVert\boldsymbol{\theta}_t\rVert^2)^2}{4}\nabla_E \right)
\end{align*}
where $\eta_t$ is the learning rate, and $\text{proj}(\boldsymbol{\theta}) = \boldsymbol{\theta}/\lVert \boldsymbol{\theta} \rVert - 10^{-5}$ if $ \boldsymbol{\theta} \geq 1; \text{ otherwise } \boldsymbol{\theta}$ ensures the updates remain with the open unit ball Poincare space. $\nabla_E$ is easily derived using the backpropagation algorithm introduced in Section. The trained model on WordNet achieved superior performance to an Euclidean baseline in various experiments that tested for the models' ability to preserve the original taxonomical structure \cite{Nickel2017}.

\chapter{Our Approach}

\section{Approach Overview}
We propose two model architectures Deep Hyperbolic Semantic Embedding (Deep-HSEM) model  and Convex Hyperbolic Semantic Embedding (Convex-HSEM) model . The general intuition is to replace the standard one-hot-encoded targets used in deep learning with poincare embedding vectors that should better reflect the location of each label in a hierarchical semantic taxonomy; we use the lexical database WordNet as the hierarchy \cite{Miller1995}. By training against these hierarchical embedding labels, we exploit the fact that many objects of initially different categories share common visual attributes through a shared super-catgory. For images, we use the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 1000-class dataset \cite{JiaDeng2009}. After we have learned a mapping from images into the hyperbolic embedding space, we then perform zero-shot recognition by feeding previously unseen classes of images through the trained model and then search for the nearest label in the embedding space. In total we use 20,000 classe of unseen images for the zero-shot evaluation. These images come from the broader 21K full ImageNet 2011 dataset. Crucially, we hypothesized that hyperbolic embedding space which is based on semantic hierarchy will reflect visual similarities better than pure similarity based word embeddings which just captures the context the words occur in; words that occur in a similar context could easily have no visual resemblence. To explore this, we also performed the zero-shot experiment with a version of our model that uses the Glove embedding in place of the hyperbolic embedding.

\section{Model Details}

\subsection{Poincare Embedding Construction}
In order to learn a mapping between images and their location in the hyperbolic embedding space, we first had to acquired the ground truth hyperbolic embedding vector for each image label. Since all of the ImageNet class-labels correspond to specifc words in the WordNet lexical database, WordNet was the obvious choise for based on which to construct the hyperbolic embeddings. Moreover, WordNet provides exactly the type of lexical semantic hierarchy that we were looking to use. As a whole, the WordNet consists of 117,000 synsets (synonym sets; words in a single synset share a meaning), which are connected to each other in a graph based on their super- and sub-ordinate relations. These relations are transitive so that if A is an subordinate of B and B is subordinate of C, then A is also subordinate of C. Several different types of words are encoded in WordNet, such as verbs and nouns. In this work only nouns were considered. One potentail issue was that each synset may contain several nouns, but fortunately the ImageNet database provides an exact mapping from each image class to a specific noun. Overall, our final list consisted of 82,000 of commonly used nouns, including all the 21,000 classes of the full ImageNet. Note that we also used nouns not included in the ImageNet labels in order to learn a more accurate structure for the hyperbolic embedding.

The specific hyperbolic embedding model we used is Poincare embedding model of Nickel and Kiela \cite{Nickel2017}, which was described in detail in Section \ref{sec:poincare}. These authors also provided an open-source implementation of the model \footnote{\label{fb_fn} Available at https://github.com/facebookresearch/poincare-embeddings} , which we used to construct Poincare embeddings for all the 82,000 nouns. The dimensionality of embedding vectors was chosen to be 10 for this project. This is much lower than the number of dimensions usually used by Euclidean word embeddings (50 to 500) and thus allowed us to demonstrate the relative efficacy of hyperbolic embeddings. Further, \cite{Nickel2017} illustrated very good predictive performance for 10 dimensions specifically on WordNet reconstruction. To get our final label embeddings, we trained the Poincare embedding model with default hyperparameters for 1500 epochs (4 days run-time). Important detail of the model is that we only provide it with information about which nounds are connected to each other, that is, no explicit hierarchical relation need to be given to the model, rather the embedding self-organizes to learn the latent hierarchy.

\subsection{Deep Hyperbolic Semantic Embedding Model \\ (Deep-HSEM)}
\subsubsection{Model Architecture}
Typically convolutional neural network takes in an image and outputs predicted probabilities for the different possible class labels. Our model is very similar with the crucial exception that, for each image, Deep-HSEM predicts a single vector that corresponds to a point in the Poincare embedding space. More precisely, we have taken a well known CNN architecture, VGG-16 \cite{Simonyan2014}, and re-adjusted its final output layer by taking out the softmax layer and replace it with a fully connected layer that corresponds to the dimensionality of the Poincare embedding space. We chose the VGG-16
model since it provides near state-of-the-art performance on the ImageNet 2012 1K data (top-1 accuracy 76.3\%, top-5 accuracy 93.8\%) but is simpler and less costly to train than many of the more recent CNNs. The architecture of the VGG-16 consists of 16 layers: 13 3x3 convolution layers followed by three fully-connected layers. Additionally, the network uses 2x2 max-pooling, ReLU non-linearities and dropout for regularization. We illustrate the Deep-HSEM architecture in Figure \ref{fig:dhsem}. The resulting architecture is very similar to the DeViSE model of Frome \textit{et al.} \cite{Frome2013} (reviewed in Section \ref{sec:semlit}); the most significant difference is that our model outputs predictions within the Poincare embedding hyperbolic space. 

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{VGG16}
  \caption{a.) Original VGG-16 structure except that we have taken out the final classification layer. The resulting model extracts 4096 long feature vector from the image b.) The VGG feature extractor is then attached to at fully-connected layer that predicts 10-dimensional embedding vector. This can then be compared to the ground-truth embedding vector of that image class to compute a loss and backpropage gradients through the network.}
  \label{fig:dhsem}
\end{figure}

Formally, our aim is to learn a mapping $S$ from 224x224 RGB image space $X^{224x224x3}$ into the 10-dimensional Poincare embedding $F_{\mathcal{P}}^{10}$.
\begin{align*}
\mathcal{S}: X^{224x224x3} \rightarrow F_{\mathcal{P}}^{10}
\end{align*}
In order to learn this mapping, the Deep-HSEM, denote it $h_{deep}(X, \pmb{\theta})$, where $\pmb{\theta}$ denotes the model weights, is provided with training data pairs of input images and corresponding embeddings $\{X, f\}$. The model takes the input image $X$ and predicts an embedding $\hat{f}$:
\begin{align*}
  h_{deep}(X, \pmb{\theta}) = \hat{f}
\end{align*}
The Poincare hyperbolic space in this instance is defined as the 10-dimensional open unit ball:
\begin{align*}
  F_{\mathcal{P}}^{10}=\{\mathbf{f} \in \mathbb{R}^{10} | \lVert \mathbf{f} \rVert <1 \}
\end{align*}
Note that we need to therefore ensure that the Deep-HSEM outputs are constrained into this space. This is ensured by the below operation performed at the end of the network:

\begin{equation}
\mathbf{\hat{f}} =
\begin{cases}
   \frac{\mathbf{\hat{f}}}{\lVert\mathbf{\hat{f}}\rVert}, & \text{if}\ \lVert\mathbf{\hat{f}}\rVert \geq 1 \\
   \mathbf{\hat{f}}, & \text{otherwise} 
\end{cases}
\end{equation}
In order to train the model parameters $\pmb{\theta}$, a loss function $L(\mathbf{\hat{f}}, \mathbf{f})$ was minimized. We considered various alternatives: squared Poincare distance, ranking hinge-loss on Poincare distance, and softmax-cross entropy loss on Poincare distances. The softmax-cross entopy loss was found to perform the best and was used for all the models presented here. In order to adapt the standard cross-entropy softmax (Equation \ref{softmax_eq}) we calculated the Poincare distance $\mathcal{D_{\mathcal{P}}}(\mathbf{\hat{f}}, \mathbf{f}_i)$ (Equation \ref{eq:pdist}) between the predicted embedding and all the possible ground-truth class embeddings. Since we want to minimize the distance to the correct ground-truth, inverses of these distances were used as class-scores i.e. logits. The final loss for a single prediction is hence:
\begin{align} \label{softmax_eq_poinc}
& \text{softmax}(\mathbf{\hat{f}})_C = \frac{\exp (-\mathcal{D_{\mathcal{P}}}(\mathbf{\hat{f}}, \mathbf{f}_C))}{\sum_{j=1}^M \text{exp}(-\mathcal{D_{\mathcal{P}}}(\mathbf{\hat{f}}, \mathbf{f}_j))} := q_C \\
& L_i  = -\log \left(q_C\right) = \mathcal{D_{\mathcal{P}}}(\mathbf{\hat{f}}, \mathbf{f}_C) + \log\left(\sum_j \text{exp}(-\mathcal{D_{\mathcal{P}}}(\mathbf{\hat{f}}, \mathbf{f}_j))\right)
\end{align}
where $j \in \{1,\dots,C,\dots,M\}$ and $M$ is the total number of training classes, and $C$ is the indicator for the correct class of the image. 

During test time, the Deep-HSEM predicts $\mathbf{\hat{f}}$ after which a nearest neighbor search is performed in the Poincare embedding using $\mathcal{D_{\mathcal{P}}}$ to get a class prediction.

\subsubsection{Model Training}
To implement the Deep-HSEM model, we started from a pre-trained open-source implementation of the VGG-16 network \footnote{Available at https://github.com/pytorch/examples/tree/master/imagenet} and then adjusted the code to get the architecture described above. All of the implementations were done with PyTorch \cite{Paszke2017} which provides auto differentation, and in general is able to compute the gradients needed for backpropagation. However, since our loss function calculates hyperbolic distances rather than Euclidean, we required the gradients output of the loss function to be Riemannian rather than Euclidean, and we could not thus rely fully on the standard autodiff. Instead we utilized the Poincare Distance function given in the open-source code of Nickel and Kiela \cite{Nickel2017} (see Footnote \ref{fb_fn}) which correctly calculates the Riemannian gradients, and ammended it to fit rest of the Deep-HSEM architecture.

We initially tested the model on small subset of the full ImageNet dataset to ensure it was functioning as expected. We then proceeded, to train the model on the 1000 classes of the ImageNet 2012 dataset which contains around 1.3 million images. Rather than training the model from scratch, we initialized the VGG portion of the Deep-HSEM with pre-trained VGG-16 weights available on PyTorch. The pre-trained model was trained on the same ImageNet dataset so should give better starting point for the model than random weights. Further, this reduced the necessary training time; training the model from scratch on such a big data set would have taken nearly two weeks. With this set-up, two versions of the Deep-HSEM were trained. In the first, call it Deep-HSEM-FC, we only trained the final fully-connected layer that maps the 4096-long image feature vector into the embedding space. In the second, call it just Deep-HSEM, we trained all the layers.

Both Deep-HSEM-FC and Deep-HSEM were trained for a total of 90 epochs using SGD with momentum. The initial learning rate was set to 0.001 and it was reduced by factor of 10 every 30 epochs. We set momentum to 0.9. Batch-size of 32 images was used, with loss for each iteration averaged over the batch images. The above hyperparameters were based on initial tests of 20 epochs of various settings. During and after training, the models were evaluated on a separate test data of 50,000 images. The overall training time for Deep-HSEM-FC was 4 days and 6 days for Deep-HSEM.

\subsection{Convex Hyperbolic Semantic Embedding Model (Convex-HSEM)}
The Convex-HSEM model is a hyperbolic embedding version of the ConSE model of Norouzi \textit{et al.} \cite{Norouzi2013}. To reiterate our discussion from Section \ref{sec:semlit}, in the first step this model utilizes a standard pre-trained CNN to get predicted softmax class probabilities for an input image. In our case, we use the original VGG-16 CNN trained on the 1000 ImageNet 2012 classes. Therefore, for any input image, the CNN will output probabilities $p(1|\mathbf{x}) \dots p(y|\mathbf{x}) \dots p(1000|\mathbf{x})$ where $\mathbf{x}$ is the input image and $\sum_1^{1000}p(y|\mathbf{x})=1$.

In the second stage of the model, the predictions from the previous step are sorted according to probabilities and the corresonding $T$ most likely class labels are taken: $\hat{y}(\mathbf{x}, 1),\hat{y}(\mathbf{x}, 2) \dots \hat{y}(\mathbf{x}, T)$. Next, the corresponding embedding vectors are taken $\mathbf{f}_1 = \mathcal{L}(\hat{y}(\mathbf{x}, 1)) \dots \mathbf{f}_T = \mathcal{L}(\hat{y}(\mathbf{x}, T))$ where $\mathcal{L}$ is just a look-up table for the embeddings. Finally, the predicted embedding vector is calculated as the weighted convex combination of the $T$ most likely embeddings, where the weights are the T most likely softmax probabilities from the first stage of the model:
\begin{align}
  \mathbf{\hat{f}(\mathbf{x})} = \frac{1}{Z}\sum_{t=1}^T p(\hat{y}(\mathbf{x}, t)| \mathbf{x}) \cdot \mathbf{f}_t
\end{align}

In our Convex-HSEM, the embedding vectors $\mathbf{f}$ come from the Poincare embeddings we calculated for the 1000 ImageNet 2012 class labels. Calculating convex combinations in hyperbolic space proves tricky, however. To see this, recall that due to constant negative curvature, straight lines in the Poincare model are arcs. Therefore if we take an Eucliden convex combination of two points on the arc, the resulting point will fall off the arc. Further, distances cannot be treated as in Euclidean case as they grow exponentially away from the centre of the embedding space. These challenges are exacerbated in higher dimension and our wanting to calculate a weighted convex combination of more than just two point. Fortunately, Gulcehre \textit{et al.} \cite{Gulcehre2018} solved a similar predicament using the Einstein midpoint, and we adapted their approach to our model. The Einstein midpoint in our setting is given as:
\begin{align*}
  \mathbf{\hat{f}(\mathbf{x})}_{EM} = \sum_{t=1}^T \left[\frac{p(\hat{y}(\mathbf{x}, t)|\mathbf{x})\gamma(\mathbf{v}_t)}{\sum_{l=1}^T p(\hat{y}(\mathbf{x}, l)|\mathbf{x})\gamma(\mathbf{v}_l)}\right]\mathbf{v}_t
\end{align*}
where $\mathbf{v}_t$ are the embedding vectors expressed in Klein coordinates, and  $\gamma(\mathbf{v}_t)=\frac{1}{\sqrt{1 - \lVert\mathbf{v_t}\rVert^2}}$, are known as Lorentz factors. Since we used the Poincare model to construct our embeddings, we had to project our vectors into Klein coordinates, $\mathbf{f}_t \rightarrow \mathbf{v}_t$, using the Equation \ref{eq:p2k} introduced in the previous section. After calculating the Einstein midpoint it is projected back to Poincare coordinates. In above expositon we have followed \cite{Gulcehre2018}, who, in turn, based it on Ungar \cite{Ungar2005} where the full derivation can be found. 

The Convex-HSEM therefore consists of two step. First, the pre-trained VGG-16 model is used to produce predicted class probabilities, out of which $T$ most likely ones, and their corresponding embedding vectors, are chosen. The Einstein midpoint of these embeddings gives the predicted Poincare embedding for the input image, which allows it to be classified using nearest neighbour search over the possible ground-truth embeddings. Figure \ref{fig:chsem} illustrates these steps. 

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{chsem}
  \caption{Illustration of the Convex-HSEM model. The orange circle represents the einstein midpoint of the semantic embeddings (yellow circles) of the $T$ most likely classes. The einstein midoint is mapped to the nearest ground-truth embedding in order to classify the input image}
  \label{fig:chsem}
\end{figure}

Since we used the pre-trained VGG-16 model, Convex-HSEM did not require any additional training and was therefore much faster to implement than the Deep-HSEM. The only hyperparameter to set was the number of data points over which to calculate the convex combination, $T$. We ran our experiments for 
$T \in \{1, 2, 5, 10, 100, 1000\}$. The motivation for this model architecture is to represent any novel image as combination of known image classes. This model is hence predominantly aimed at zero-shot learning; if we were to classify a new image, and we knew it to be one of the 1000 training classes, it would make more sense just to use the standard VGG-16 model.



\chapter{Discussion \& Future Research}

Nickel showed that the Poincare embedding much better represent relationships of explicit graph. Also graph convolution showed graph importance.
The failure of hierarchical semantic embedding to overperform w2v has hence two possible reason: alone semantic relations dont really show visual resemblence as well (this is hard to understand, consider the Bethos example, and GCN). It it however the possible that GCN performance would have been worse without similarit (check paper). Nevertheless it is unlikely that this space is smooth. Second possbility is that mapping into poincare embedding space is difficult. On the otherhand, our ConSE model failed but this may be because Einstein midpoint is not great..



\bibliographystyle{ieeetr}
\bibliography{Thesis}

\end{document}
